"""
ë¬¸ì„œ ì²˜ë¦¬ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³ , 
í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°í™”ì— ì í•©í•œ ì²­í¬ë¡œ ë¶„í• í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import logging
import re
import bisect
import asyncio
import os
import base64
from pathlib import Path
from typing import List, Dict, Optional, Any
import aiofiles
import PyPDF2
from docx import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter, Language
from service.llm.llm_service import LLMService

import os
import csv
try:
    from openpyxl import load_workbook
    OPENPYXL_AVAILABLE = True
except ImportError:
    OPENPYXL_AVAILABLE = False

try:
    import xlrd
    XLRD_AVAILABLE = True
except ImportError:
    XLRD_AVAILABLE = False

try:
    from pdfminer.high_level import extract_text
    PDFMINER_AVAILABLE = True
except ImportError:
    PDFMINER_AVAILABLE = False

try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
    print("âœ… pdfplumber available")
except Exception:
    PDFPLUMBER_AVAILABLE = False

try:
    from pdf2image import convert_from_path
    PDF2IMAGE_AVAILABLE = True
    print("âœ… pdf2image available")
except ImportError:
    PDF2IMAGE_AVAILABLE = False

try:
    import fitz  # PyMuPDF
    PYMUPDF_AVAILABLE = True
    print("âœ… PyMuPDF available")
except Exception:
    PYMUPDF_AVAILABLE = False

try:
    from langchain_openai import ChatOpenAI
    from langchain_core.messages import HumanMessage
    LANGCHAIN_OPENAI_AVAILABLE = True
except ImportError:
    LANGCHAIN_OPENAI_AVAILABLE = False

# DOCX to Image ë³€í™˜ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì²´í¬
try:
    from docx2pdf import convert as docx_to_pdf_convert
    DOCX2PDF_AVAILABLE = True
    print("âœ… docx2pdf available")
except ImportError:
    DOCX2PDF_AVAILABLE = False

# PPT ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì²´í¬
try:
    from pptx import Presentation
    PYTHON_PPTX_AVAILABLE = True
    print("âœ… python-pptx available")
except ImportError:
    PYTHON_PPTX_AVAILABLE = False

# ëŒ€ì•ˆ: python-docx + PILì„ ì´ìš©í•œ ë°©ë²•
try:
    from PIL import Image, ImageDraw, ImageFont
    from io import BytesIO
    PIL_AVAILABLE = True
    print("âœ… PIL available")
except ImportError:
    PIL_AVAILABLE = False

logger = logging.getLogger("document-processor")

LANGCHAIN_CODE_LANGUAGE_MAP = {
    'py': Language.PYTHON, 'js': Language.JS, 'ts': Language.TS,
    'java': Language.JAVA, 'cpp': Language.CPP, 'c': Language.CPP,
    'cs': Language.CSHARP, 'go': Language.GO, 'rs': Language.RUST,
    'php': Language.PHP, 'rb': Language.RUBY, 'swift': Language.SWIFT,
    'kt': Language.KOTLIN, 'scala': Language.SCALA,
    'html': Language.HTML, 'jsx': Language.JS, 'tsx': Language.TS,
}

class DocumentProcessor:
    """ë¬¸ì„œ ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self, collection_config=None):
        # ğŸ”¥ PPT í˜•ì‹ ì¶”ê°€
        self.document_types = ['pdf', 'docx', 'doc', 'pptx', 'ppt']
        self.text_types = ['txt', 'md', 'markdown', 'rtf']
        self.code_types = ['py','js','ts','java','cpp','c','h','cs','go','rs',
                          'php','rb','swift','kt','scala','dart','r','sql',
                          'html','css','jsx','tsx','vue','svelte']
        self.config_types = ['json','yaml','yml','xml','toml','ini','cfg','conf','properties','env']
        self.data_types   = ['csv','tsv','xlsx','xls']
        self.script_types = ['sh','bat','ps1','zsh','fish']
        self.log_types    = ['log']
        self.web_types    = ['htm','xhtml']
        # ğŸ”¥ JPG í˜•ì‹ ì¶”ê°€
        self.image_types  = ['jpg','jpeg','png','gif','bmp','webp']
        self.supported_types = (
            self.document_types + self.text_types + self.code_types +
            self.config_types + self.data_types + self.script_types +
            self.log_types + self.web_types + self.image_types
        )
        self.collection_config = collection_config
        
        # ë³€ê²½
        if not OPENPYXL_AVAILABLE and not XLRD_AVAILABLE:
            # openpyxlë„ ì—†ê³  xlrdë„ ì—†ìœ¼ë©´ Excel ì§€ì› ì œê±°
            self.supported_types = [t for t in self.supported_types if t not in ['xlsx', 'xls']]
            logger.warning("openpyxl and xlrd not available. Excel processing disabled.")
        if not LANGCHAIN_OPENAI_AVAILABLE:
            self.supported_types = [t for t in self.supported_types if t not in self.image_types]
            logger.warning("langchain_openai not available. Image processing disabled.")
        
        # ğŸ”¥ PPTëŠ” LibreOfficeë¡œë„ ì²˜ë¦¬ ê°€ëŠ¥í•˜ë¯€ë¡œ ì§€ì› ëª©ë¡ì—ì„œ ì œê±°í•˜ì§€ ì•ŠìŒ
        # if not PYTHON_PPTX_AVAILABLE:
        #     self.supported_types = [t for t in self.supported_types if t not in ['pptx', 'ppt']]
        #     logger.warning("python-pptx not available. PPT processing disabled.")
            
        self.encodings = ['utf-8','utf-8-sig','cp949','euc-kr','latin-1','ascii']
        if not PDFMINER_AVAILABLE:
            logger.warning("pdfminer not available. Using PyPDF2 fallback.")
        if not PDF2IMAGE_AVAILABLE:
            logger.warning("pdf2image not available. OCR disabled.")
        if not DOCX2PDF_AVAILABLE and not PIL_AVAILABLE:
            logger.warning("docx2pdf and PIL not available. DOCX/PPT OCR disabled.")

    def _get_current_image_text_config(self) -> Dict[str, Any]:
        """ì‹¤ì‹œê°„ìœ¼ë¡œ í˜„ì¬ IMAGE_TEXT ì„¤ì • ê°€ì ¸ì˜¤ê¸°"""
        try:
            from main import app
            if hasattr(app.state, 'config_composer'):
                collection_config = app.state.config_composer.get_config_by_category_name("collection")
                
                # ğŸ”¥ get_env_value() ëŒ€ì‹  ì§ì ‘ .value ì ‘ê·¼
                if hasattr(collection_config, 'IMAGE_TEXT_MODEL_PROVIDER'):
                    provider_obj = getattr(collection_config, 'IMAGE_TEXT_MODEL_PROVIDER')
                    base_url_obj = getattr(collection_config, 'IMAGE_TEXT_BASE_URL')
                    api_key_obj = getattr(collection_config, 'IMAGE_TEXT_API_KEY')
                    model_obj = getattr(collection_config, 'IMAGE_TEXT_MODEL_NAME')
                    temp_obj = getattr(collection_config, 'IMAGE_TEXT_TEMPERATURE')
                    # ğŸ”¥ ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì • ì¶”ê°€
                    batch_size_obj = getattr(collection_config, 'IMAGE_TEXT_BATCH_SIZE', None)
                    
                    # PersistentConfig ê°ì²´ì—ì„œ ì‹¤ì œ ê°’ ì¶”ì¶œ
                    config = {
                        'provider': str(provider_obj.value if hasattr(provider_obj, 'value') else provider_obj).lower(),
                        'base_url': str(base_url_obj.value if hasattr(base_url_obj, 'value') else base_url_obj),
                        'api_key': str(api_key_obj.value if hasattr(api_key_obj, 'value') else api_key_obj),
                        'model': str(model_obj.value if hasattr(model_obj, 'value') else model_obj),
                        'temperature': float(temp_obj.value if hasattr(temp_obj, 'value') else temp_obj),
                        # ğŸ”¥ ë°°ì¹˜ í¬ê¸° ì„¤ì • (ê¸°ë³¸ê°’: 1, ìµœëŒ€: 5)
                        'batch_size': int(batch_size_obj.value if batch_size_obj and hasattr(batch_size_obj, 'value') else 1)
                    }
                    
                    logger.info(f"ğŸ”„ Direct value access config: {config}")
                    return config
            
        except Exception as e:
            logger.error(f"ğŸ” Error in _get_current_image_text_config: {e}")
            import traceback
            logger.error(f"ğŸ” Traceback: {traceback.format_exc()}")
        
        # fallback
        logger.warning("ğŸ” Using fallback config")
        return {'provider': 'no_model', 'batch_size': 1}

    def _is_image_text_enabled(self, config: Dict[str, Any]) -> bool:
        """ì„¤ì •ì— ë”°ë¼ OCRì´ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        provider = config.get('provider', 'no_model')
        if provider in ('openai', 'vllm'):
            # OCR ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡œë°”ì´ë”ì¸ì§€ í™•ì¸
            if not LANGCHAIN_OPENAI_AVAILABLE:
                logger.warning("langchain_openai not available for OCR")
                return False
            return True
        return False

    def get_supported_types(self) -> List[str]:
        """ì§€ì›í•˜ëŠ” íŒŒì¼ í˜•ì‹ ëª©ë¡ ë°˜í™˜"""
        return self.supported_types.copy()
    
    def get_file_category(self, file_type: str) -> str:
        """íŒŒì¼ íƒ€ì…ì˜ ì¹´í…Œê³ ë¦¬ ë°˜í™˜"""
        file_type = file_type.lower()
        if file_type in self.document_types:
            return 'document'
        elif file_type in self.text_types:
            return 'text'
        elif file_type in self.code_types:
            return 'code'
        elif file_type in self.config_types:
            return 'config'
        elif file_type in self.data_types:
            return 'data'
        elif file_type in self.script_types:
            return 'script'
        elif file_type in self.log_types:
            return 'log'
        elif file_type in self.web_types:
            return 'web'
        elif file_type in self.image_types:
            return 'image'
        else:
            return 'unknown'
    
    def clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if not text:
            return ""
        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ í†µí•©
        text = re.sub(r'\s+', ' ', text)
        # ì—°ì†ëœ ì¤„ë°”ê¿ˆì„ ë‘ ê°œë¡œ ì œí•œ
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
        return text.strip()

    def _is_text_quality_sufficient(self, text: Optional[str], min_chars: int = 500, min_word_ratio: float = 0.6) -> bool:
        """ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ í…ìŠ¤íŠ¸ í’ˆì§ˆ íŒë‹¨

        - min_chars ë¯¸ë§Œì´ë©´ ë‚®ìŒ
        - í…ìŠ¤íŠ¸ ë‚´ ì•ŒíŒŒë²³/í•œê¸€ ë“± ë‹¨ì–´ ë¬¸ìì˜ ë¹„ìœ¨ì´ ë‚®ìœ¼ë©´(ì´ë¯¸ì§€ OCR ì¡ìŒ ê°€ëŠ¥) ë‚®ìŒ
        """
        try:
            if not text:
                return False
            if len(text) < min_chars:
                return False
            # ë‹¨ì–´ ë¬¸ì ë¹„ìœ¨ (í•œê¸€/ë¼í‹´/ìˆ«ì ë“±)
            import re
            word_chars = re.findall(r"[\w\u1100-\u11FF\u3130-\u318F\uAC00-\uD7AF]", text)
            ratio = len(word_chars) / max(1, len(text))
            return ratio >= min_word_ratio
        except Exception:
            return False
    
    def clean_code_text(self, text: str, file_type: str) -> str:
        """ì½”ë“œ í…ìŠ¤íŠ¸ ì •ë¦¬ (ì½”ë“œì˜ êµ¬ì¡°ë¥¼ ë³´ì¡´)"""
        if not text:
            return ""
        
        # ì½”ë“œ íŒŒì¼ì˜ ê²½ìš° ë“¤ì—¬ì“°ê¸°ì™€ ì¤„ë°”ê¿ˆì„ ë³´ì¡´
        # ë‹¤ë§Œ íŒŒì¼ ëì˜ ê³¼ë„í•œ ê³µë°±ì€ ì œê±°
        text = text.rstrip()
        
        # íƒ­ì„ 4ê°œì˜ ìŠ¤í˜ì´ìŠ¤ë¡œ ë³€í™˜ (ì¼ê´€ì„±ì„ ìœ„í•´)
        text = text.replace('\t', '    ')
        
        return text
    
    # ğŸ”¥ ë°°ì¹˜ OCR ì²˜ë¦¬ ë©”ì„œë“œë“¤
    async def _convert_images_to_text_batch(self, image_paths: List[str], batch_size: int = 1) -> List[str]:
        """ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ ë°°ì¹˜ë¡œ í…ìŠ¤íŠ¸ ë³€í™˜ (ì‹¤ì‹œê°„ ì„¤ì • ì‚¬ìš©)"""
        current_config = self._get_current_image_text_config()
        
        if not self._is_image_text_enabled(current_config):
            return ["[ì´ë¯¸ì§€ íŒŒì¼: ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë³€í™˜ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤]" for _ in image_paths]
        
        # ë°°ì¹˜ í¬ê¸° ì œí•œ (1-5)
        batch_size = max(1, min(batch_size, 10))
        
        results = []
        total_batches = (len(image_paths) + batch_size - 1) // batch_size
        
        for i in range(0, len(image_paths), batch_size):
            batch_paths = image_paths[i:i+batch_size]
            batch_num = (i // batch_size) + 1
            
            logger.info(f"Processing batch {batch_num}/{total_batches} with {len(batch_paths)} images")
            
            if len(batch_paths) == 1:
                # ë‹¨ì¼ ì´ë¯¸ì§€ëŠ” ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                result = await self._convert_image_to_text(batch_paths[0])
                results.append(result)
            else:
                # ì—¬ëŸ¬ ì´ë¯¸ì§€ëŠ” ë°°ì¹˜ ì²˜ë¦¬
                batch_results = await self._convert_multiple_images_to_text(batch_paths, current_config)
                results.extend(batch_results)
        
        return results

    async def _convert_multiple_images_to_text(self, image_paths: List[str], config: Dict[str, Any]) -> List[str]:
        """ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ í•œë²ˆì— OCR ì²˜ë¦¬"""
        try:
            # ëª¨ë“  ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
            image_contents = []
            for image_path in image_paths:
                with open(image_path, "rb") as image_file:
                    base64_image = base64.b64encode(image_file.read()).decode('utf-8')
                    image_contents.append(base64_image)
            
            provider = config.get('provider', 'openai')
            api_key = config.get('api_key', '')
            base_url = config.get('base_url', 'https://api.openai.com/v1')
            model = config.get('model', 'gpt-4-vision-preview')
            temperature = config.get('temperature', 0.7)
            
            logger.info(f'ğŸ”„ Using batch OCR with {len(image_paths)} images, provider: {provider}')
            
            # í”„ë¡œë°”ì´ë”ë³„ LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            if provider == 'openai':
                llm = ChatOpenAI(
                    model=model,
                    openai_api_key=api_key,
                    base_url=base_url,
                    temperature=temperature
                )
            elif provider == 'vllm':
                llm = ChatOpenAI(
                    model=model,
                    openai_api_key=api_key or 'dummy',
                    base_url=base_url,
                    temperature=temperature
                )
            else:
                logger.error(f"Unsupported image-text provider: {provider}")
                return [f"[ì´ë¯¸ì§€ íŒŒì¼: ì§€ì›í•˜ì§€ ì•ŠëŠ” í”„ë¡œë°”ì´ë” - {provider}]" for _ in image_paths]
            
            # ë°°ì¹˜ OCR í”„ë¡¬í”„íŠ¸
            prompt = f"""ë‹¤ìŒ {len(image_paths)}ê°œì˜ ì´ë¯¸ì§€ë¥¼ ê°ê° ì •í™•í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”. 

            **ì¤‘ìš”í•œ ê·œì¹™:**
            1. ê° ì´ë¯¸ì§€ì˜ ê²°ê³¼ë¥¼ ëª…í™•íˆ êµ¬ë¶„í•´ì£¼ì„¸ìš”
            2. ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

            === ì´ë¯¸ì§€ 1 ===
            [ì²« ë²ˆì§¸ ì´ë¯¸ì§€ì˜ í…ìŠ¤íŠ¸ ë‚´ìš©]

            === ì´ë¯¸ì§€ 2 ===
            [ë‘ ë²ˆì§¸ ì´ë¯¸ì§€ì˜ í…ìŠ¤íŠ¸ ë‚´ìš©]

            === ì´ë¯¸ì§€ 3 ===
            [ì„¸ ë²ˆì§¸ ì´ë¯¸ì§€ì˜ í…ìŠ¤íŠ¸ ë‚´ìš©]

            **ë³€í™˜ ê·œì¹™:**
            - í‘œê°€ ìˆë‹¤ë©´ ë§ˆí¬ë‹¤ìš´ í‘œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            - ì›ë³¸ì˜ ë ˆì´ì•„ì›ƒ, ë“¤ì—¬ì“°ê¸°, ì¤„ë°”ê¿ˆ ë³´ì¡´
            - ëª¨ë“  ë¬¸ì, ìˆ«ì, ê¸°í˜¸ë¥¼ ì •í™•íˆ ì¸ì‹
            - ì œëª©, ë¶€ì œëª©, ëª©ë¡, ë‹¨ë½ êµ¬ë¶„ì„ ëª…í™•íˆ í‘œí˜„
            - íŠ¹ìˆ˜ í˜•ì‹(ë‚ ì§œ, ê¸ˆì•¡ ë“±) ì •í™•íˆ ìœ ì§€
            - **ì„¹ì…˜ êµ¬ë¶„**: ê° ì´ë¯¸ì§€ ë‚´ì—ì„œ ë¬¸ë§¥ì ìœ¼ë¡œ ë‹¤ë¥¸ ë‚´ìš© ì„¹ì…˜ë“¤ì€ `\n\n\n` (ì„¸ ê°œì˜ ì¤„ë°”ê¿ˆ ë¬¸ì)ìœ¼ë¡œ ëª…í™•íˆ êµ¬ë¶„

            **ì„¹ì…˜ êµ¬ë¶„ ì˜ˆì‹œ:**
            - ì œëª©ê³¼ ë³¸ë¬¸ ì‚¬ì´
            - ì„œë¡œ ë‹¤ë¥¸ ì£¼ì œë‚˜ ë‹¨ë½ ì‚¬ì´  
            - í‘œì™€ ë‹¤ë¥¸ ë‚´ìš© ì‚¬ì´
            - ì°¨íŠ¸/ê·¸ë˜í”„ì™€ ì„¤ëª… í…ìŠ¤íŠ¸ ì‚¬ì´

            **ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ:**
            === ì´ë¯¸ì§€ 1 ===
            # ì œëª©
            \n\n\n
            ë³¸ë¬¸ ë‚´ìš©ì´ ì—¬ê¸°ì—...

            \n\n\n
            ## ì†Œì œëª©
            ë‹¤ë¥¸ ì„¹ì…˜ì˜ ë‚´ìš©...

            \n\n\n
            | í‘œ ë°ì´í„° |
            |----------|
            | ë‚´ìš©     |

            í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ê³ , ì¶”ê°€ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”."""
            # ë©€í‹° ì´ë¯¸ì§€ ë©”ì‹œì§€ ìƒì„±
            content = [{"type": "text", "text": prompt}]
            
            for i, base64_image in enumerate(image_contents):
                content.append({
                    "type": "image_url", 
                    "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}
                })
            
            message = HumanMessage(content=content)
            
            # ì‘ë‹µ ìƒì„±
            response = await llm.ainvoke([message])
            response_text = response.content
            
            # ì‘ë‹µì„ ì´ë¯¸ì§€ë³„ë¡œ ë¶„í• 
            results = self._parse_batch_ocr_response(response_text, len(image_paths))
            
            logger.info(f"Successfully processed {len(image_paths)} images in batch")
            return results
            
        except Exception as e:
            logger.error(f"Error in batch OCR processing: {e}")
            # ì‹¤íŒ¨ì‹œ ê°œë³„ ì²˜ë¦¬ë¡œ fallback
            logger.warning("Batch OCR failed, falling back to individual processing")
            results = []
            for image_path in image_paths:
                result = await self._convert_image_to_text(image_path)
                results.append(result)
            return results

    def _parse_batch_ocr_response(self, response_text: str, expected_count: int) -> List[str]:
        """ë°°ì¹˜ OCR ì‘ë‹µì„ ì´ë¯¸ì§€ë³„ë¡œ ë¶„í• """
        try:
            # "=== ì´ë¯¸ì§€ N ===" íŒ¨í„´ìœ¼ë¡œ ë¶„í• 
            import re
            
            # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ê° ì´ë¯¸ì§€ ì„¹ì…˜ ì°¾ê¸°
            pattern = r'=== ì´ë¯¸ì§€ (\d+) ===\s*(.*?)(?=\s*=== ì´ë¯¸ì§€ \d+ ===|\s*$)'
            matches = re.findall(pattern, response_text, re.DOTALL)
            
            results = []
            
            if matches and len(matches) >= expected_count:
                # ë§¤ì¹­ëœ ê²°ê³¼ ì‚¬ìš©
                for i in range(expected_count):
                    if i < len(matches):
                        _, content = matches[i]
                        results.append(content.strip())
                    else:
                        results.append("[ì´ë¯¸ì§€ ë¶„í•  ì‹¤íŒ¨]")
            else:
                # íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨ì‹œ ë‹¨ìˆœ ë¶„í• 
                logger.warning("Pattern matching failed, using simple split")
                parts = re.split(r'=== ì´ë¯¸ì§€ \d+ ===', response_text)
                
                for i in range(expected_count):
                    if i + 1 < len(parts):
                        results.append(parts[i + 1].strip())
                    else:
                        results.append("[ì´ë¯¸ì§€ ë¶„í•  ì‹¤íŒ¨]")
            
            # ê²°ê³¼ ê°œìˆ˜ ë§ì¶”ê¸°
            while len(results) < expected_count:
                results.append("[ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨]")
            
            return results[:expected_count]
            
        except Exception as e:
            logger.error(f"Error parsing batch OCR response: {e}")
            # ì‹¤íŒ¨ì‹œ ë™ì¼í•œ ì‘ë‹µì„ ëª¨ë“  ì´ë¯¸ì§€ì— ì ìš©
            return [response_text for _ in range(expected_count)]
    
    async def _convert_image_to_text(self, image_path: str) -> str:
        """ì´ë¯¸ì§€ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì‹¤ì‹œê°„ ì„¤ì • ì‚¬ìš©)"""
        # ğŸ”¥ ì‹¤ì‹œê°„ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        current_config = self._get_current_image_text_config()
        
        if not self._is_image_text_enabled(current_config):
            return "[ì´ë¯¸ì§€ íŒŒì¼: ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë³€í™˜ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤]"
        
        try:
            # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
            with open(image_path, "rb") as image_file:
                base64_image = base64.b64encode(image_file.read()).decode('utf-8')
            
            provider = current_config.get('provider', 'openai')
            api_key = current_config.get('api_key', '')
            base_url = current_config.get('base_url', 'https://api.openai.com/v1')
            model = current_config.get('model', 'gpt-4-vision-preview')
            temperature = current_config.get('temperature', 0.7)
            
            logger.info(f'ğŸ”„ Using real-time image-text provider: {provider}')
            logger.info(f'Model: {model}, Base URL: {base_url}')
            
            # í”„ë¡œë°”ì´ë”ë³„ LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            if provider == 'openai':
                llm = ChatOpenAI(
                    model=model,
                    openai_api_key=api_key,
                    base_url=base_url,
                    temperature=temperature
                )
            elif provider == 'vllm':
                # vLLMì˜ ê²½ìš° OpenAI í˜¸í™˜ API ì‚¬ìš©
                llm = ChatOpenAI(
                    model=model,
                    openai_api_key=api_key or 'dummy',  # vLLMì€ ë³´í†µ API í‚¤ê°€ í•„ìš”ì—†ìŒ
                    base_url=base_url,
                    temperature=temperature
                )
            else:
                logger.error(f"Unsupported image-text provider: {provider}")
                return f"[ì´ë¯¸ì§€ íŒŒì¼: ì§€ì›í•˜ì§€ ì•ŠëŠ” í”„ë¡œë°”ì´ë” - {provider}]"
            
            # OCR í”„ë¡¬í”„íŠ¸
            prompt = """ì´ ì´ë¯¸ì§€ë¥¼ ì •í™•í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”. ë‹¤ìŒ ê·œì¹™ì„ ì² ì €íˆ ì§€ì¼œì£¼ì„¸ìš”:

                        1. **í‘œ êµ¬ì¡° ë³´ì¡´**: í‘œê°€ ìˆë‹¤ë©´ ì •í™•í•œ í–‰ê³¼ ì—´ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ê³ , ë§ˆí¬ë‹¤ìš´ í‘œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”
                        2. **ë ˆì´ì•„ì›ƒ ìœ ì§€**: ì›ë³¸ì˜ ë ˆì´ì•„ì›ƒ, ë“¤ì—¬ì“°ê¸°, ì¤„ë°”ê¿ˆì„ ìµœëŒ€í•œ ë³´ì¡´í•´ì£¼ì„¸ìš”
                        3. **ì •í™•í•œ í…ìŠ¤íŠ¸**: ëª¨ë“  ë¬¸ì, ìˆ«ì, ê¸°í˜¸ë¥¼ ì •í™•íˆ ì¸ì‹í•´ì£¼ì„¸ìš”
                        4. **êµ¬ì¡° ì •ë³´**: ì œëª©, ë¶€ì œëª©, ëª©ë¡, ë‹¨ë½ êµ¬ë¶„ì„ ëª…í™•íˆ í‘œí˜„í•´ì£¼ì„¸ìš”
                        5. **íŠ¹ìˆ˜ í˜•ì‹**: ë‚ ì§œ, ê¸ˆì•¡, ì£¼ì†Œ, ì „í™”ë²ˆí˜¸ ë“±ì˜ í˜•ì‹ì„ ì •í™•íˆ ìœ ì§€í•´ì£¼ì„¸ìš”
                        6. **ìŠ¬ë¼ì´ë“œ êµ¬ì¡°**: ìŠ¬ë¼ì´ë“œ ì œëª©, ë‚´ìš©, ì°¨íŠ¸/ê·¸ë˜í”„ ì„¤ëª…ì„ êµ¬ë¶„í•´ì£¼ì„¸ìš”
                        7. **ì„¹ì…˜ êµ¬ë¶„**: ë¬¸ë§¥ì ìœ¼ë¡œ ë‹¤ë¥¸ ë‚´ìš© ì„¹ì…˜ë“¤ì€ `\\n\\n\\n` (ì„¸ ê°œì˜ ì¤„ë°”ê¿ˆ ë¬¸ì)ìœ¼ë¡œ ëª…í™•íˆ êµ¬ë¶„í•´ì£¼ì„¸ìš”

                        **ì„¹ì…˜ êµ¬ë¶„ ì˜ˆì‹œ:**
                        - ì œëª©ê³¼ ë³¸ë¬¸ ì‚¬ì´
                        - ì„œë¡œ ë‹¤ë¥¸ ì£¼ì œë‚˜ ë‹¨ë½ ì‚¬ì´  
                        - í‘œì™€ ë‹¤ë¥¸ ë‚´ìš© ì‚¬ì´
                        - ì°¨íŠ¸/ê·¸ë˜í”„ì™€ ì„¤ëª… í…ìŠ¤íŠ¸ ì‚¬ì´

                        ë§Œì•½ í‘œê°€ ìˆë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì€ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”:
                        | í•­ëª© | ë‚´ìš© |
                        |------|------|
                        | ë°ì´í„°1 | ê°’1 |
                        | ë°ì´í„°2 | ê°’2 |

                        **ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ:**
                        # ì œëª©
                        \n\n\n
                        ë³¸ë¬¸ ë‚´ìš©ì´ ì—¬ê¸°ì—...

                        \n\n\n
                        ## ì†Œì œëª©
                        ë‹¤ë¥¸ ì„¹ì…˜ì˜ ë‚´ìš©...

                        \n\n\n
                        | í‘œ ë°ì´í„° |
                        |----------|
                        | ë‚´ìš©     |

                        í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ê³ , ì¶”ê°€ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”."""


            # ì´ë¯¸ì§€ ë©”ì‹œì§€ ìƒì„±
            message = HumanMessage(
                content=[
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}
                ]
            )
            
            # ì‘ë‹µ ìƒì„±
            response = await llm.ainvoke([message])
            logger.info(f"Successfully converted image to text using {provider}: {Path(image_path).name}")
            return response.content
            
        except Exception as e:
            logger.error(f"Error converting image to text {image_path}: {e}")
            return f"[ì´ë¯¸ì§€ íŒŒì¼: í…ìŠ¤íŠ¸ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - {str(e)}]"
    
    # PDF ê´€ë ¨ ë©”ì„œë“œë“¤
    async def _extract_text_from_pdf(self, file_path: str) -> str:
        """PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì‹¤ì‹œê°„ ì„¤ì •ì— ë”°ë¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë˜ëŠ” OCR)"""
        try:
            # ğŸ”¥ ì‹¤ì‹œê°„ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            current_config = self._get_current_image_text_config()
            provider = current_config.get('provider', 'no_model')
            
            logger.info(f"ğŸ”„ Real-time PDF processing with provider: {provider}")
            
            # no_modelì¸ ê²½ìš°ì—ë§Œ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if provider == 'no_model':
                logger.info("Using text extraction mode (no_model)")

                # PyMuPDF ìš°ì„  ì‚¬ìš©: í˜ì´ì§€ì™€ ë¼ì¸ ì •ë³´ë¥¼ ë³´ë‹¤ ì •í™•í•˜ê²Œ ì¶”ì¶œ
                if PYMUPDF_AVAILABLE:
                    try:
                        fitz_text = self._extract_text_from_pdf_fitz(file_path)
                        if fitz_text and fitz_text.strip():
                            logger.info(f"Text extracted via PyMuPDF: {len(fitz_text)} chars")
                            # í’ˆì§ˆ ê²€ì‚¬: ë„ˆë¬´ ì§§ê±°ë‚˜ ë¬¸ì ë¹„ìœ¨ì´ ë‚®ìœ¼ë©´ pdfplumber/pdfminerë¡œ í´ë°±
                            if not self._is_text_quality_sufficient(fitz_text):
                                logger.info("PyMuPDF extraction seems low quality, attempting pdfplumber/pdfminer fallbacks")
                                # ì‹œë„ 1: pdfplumber (í…Œì´ë¸”/ë ˆì´ì•„ì›ƒ ë³´ì¡´ì— ê°•í•¨)
                                if PDFPLUMBER_AVAILABLE:
                                    try:
                                        import pdfplumber
                                        with pdfplumber.open(file_path) as pdf:
                                            pages = [p.extract_text() or "" for p in pdf.pages]
                                        pb_text = "\n".join(pages).strip()
                                        if pb_text and self._is_text_quality_sufficient(pb_text):
                                            logger.info(f"Text extracted via pdfplumber: {len(pb_text)} chars")
                                            return self.clean_text(pb_text)
                                        else:
                                            logger.info("pdfplumber result not sufficient, will try pdfminer")
                                    except Exception as e:
                                        logger.warning(f"pdfplumber extraction failed: {e}")

                                # ì‹œë„ 2: pdfminer layout extraction
                                if PDFMINER_AVAILABLE:
                                    try:
                                        layout_text = await asyncio.to_thread(self._extract_text_from_pdf_layout, file_path)
                                        if layout_text and layout_text.strip() and self._is_text_quality_sufficient(layout_text):
                                            logger.info(f"Text extracted via pdfminer layout (after PyMuPDF fallback): {len(layout_text)} chars")
                                            return self.clean_text(layout_text)
                                    except Exception as e:
                                        logger.debug(f"pdfminer fallback failed: {e}")

                            # ê¸°ë³¸ì ìœ¼ë¡œ PyMuPDF ê²°ê³¼ ë°˜í™˜ (í’ˆì§ˆì´ ì¶©ë¶„í•˜ë©´)
                            if self._is_text_quality_sufficient(fitz_text):
                                return fitz_text
                            else:
                                # í’ˆì§ˆ ë¶€ì¡±ì´ì§€ë§Œ ë‹¤ë¥¸ ë°©ë²•ë„ ì‹¤íŒ¨í•œ ê²½ìš° ì´í›„ í´ë°±ìœ¼ë¡œ ì§„í–‰
                                logger.info("PyMuPDF result kept as last-resort; continuing with other fallbacks")
                    except Exception as e:
                        logger.warning(f"PyMuPDF extraction failed: {e}")
                
                # 1ë‹¨ê³„: pdfminerë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©´ layout ê¸°ë°˜ìœ¼ë¡œ ë¼ì¸ ë‹¨ìœ„ ì¶”ì¶œ ì‹œë„
                if PDFMINER_AVAILABLE:
                    logger.info(f"Attempting pdfminer layout extraction for {file_path}")
                    try:
                        # layout ê¸°ë°˜ ë¼ì¸ ì¶”ì¶œì„ ìš°ì„  ì‹œë„
                        layout_text = await asyncio.to_thread(self._extract_text_from_pdf_layout, file_path)
                        if layout_text and layout_text.strip():
                            cleaned_text = self.clean_text(layout_text)
                            if cleaned_text.strip():
                                logger.info(f"Text extracted via pdfminer layout: {len(cleaned_text)} chars")
                                return cleaned_text

                        # layout ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ per-page ë³‘ë ¬ ì¶”ì¶œë¡œ í´ë°±
                        try:
                            pdf_reader = PyPDF2.PdfReader(file_path)
                            num_pages = len(pdf_reader.pages)
                        except Exception:
                            num_pages = None

                        all_text = ""
                        if num_pages and num_pages > 0:
                            max_workers = 2
                            page_texts = await self._extract_pages_pdfminer(file_path, num_pages, max_workers)
                            for i, page_text in enumerate(page_texts):
                                if page_text and page_text.strip():
                                    all_text += f"\n=== í˜ì´ì§€ {i+1} ===\n"
                                    all_text += page_text + "\n"

                            cleaned_text = self.clean_text(all_text)
                            if cleaned_text.strip():
                                logger.info(f"Text extracted per-page via pdfminer: {len(cleaned_text)} chars, pages: {num_pages}")
                                return cleaned_text

                        # ìµœí›„ í´ë°±: ì „ì²´ ì¶”ì¶œ
                        text = extract_text(file_path)
                        cleaned_text = self.clean_text(text)
                        if len(cleaned_text.strip()) > 100:
                            logger.info(f"Text extracted via pdfminer (full): {len(cleaned_text)} chars")
                            return cleaned_text
                    except Exception as e:
                        logger.warning(f"pdfminer extraction failed: {e}")
                        
                # 2ë‹¨ê³„: PyMuPDFë¥¼ ëŸ°íƒ€ì„ì—ì„œ ìš°ì„  ì‹œë„í•˜ê³ , ë¶ˆê°€ëŠ¥í•˜ë©´ PyPDF2ë¡œ í´ë°±
                try:
                    import fitz
                    logger.info("PyMuPDF detected at runtime, attempting extraction via PyMuPDF")
                    fitz_text = await asyncio.to_thread(self._extract_text_from_pdf_fitz, file_path)
                    if fitz_text and fitz_text.strip():
                        logger.info(f"Text extracted via PyMuPDF: {len(fitz_text)} chars")
                        return fitz_text
                except Exception as e:
                    logger.debug(f"PyMuPDF runtime attempt failed or not available: {e}")

                # PyMuPDFê°€ ì—†ê±°ë‚˜ ì‹¤íŒ¨í•˜ë©´ ê¸°ì¡´ PyPDF2 fallback ì‚¬ìš©
                logger.info(f"Using PyPDF2 fallback for {file_path}")
                text = await self._extract_text_from_pdf_fallback(file_path)
                logger.info(f"Text extracted via PyPDF2: {len(text)} chars")
                return text  # no_modelì—ì„œëŠ” OCRë¡œ ë„˜ì–´ê°€ì§€ ì•ŠìŒ
            
            else:
                # openai, vllm ë“± ë‹¤ë¥¸ í”„ë¡œë°”ì´ë”ì¸ ê²½ìš° ë¬´ì¡°ê±´ OCR
                logger.info(f"Using OCR mode with provider: {provider}")
                return await self._extract_text_from_pdf_via_ocr(file_path)
                
        except Exception as e:
            logger.error(f"PDF processing failed: {e}")
            
            # ì—ëŸ¬ ë°œìƒì‹œì—ë„ ì‹¤ì‹œê°„ ì„¤ì •ì— ë”°ë¼ ì²˜ë¦¬
            current_config = self._get_current_image_text_config()
            provider = current_config.get('provider', 'no_model')
            
            if provider == 'no_model':
                # no_modelì¸ ê²½ìš° ê¸°ë³¸ fallbackë§Œ ì‹œë„
                try:
                    return await self._extract_text_from_pdf_fallback(file_path)
                except:
                    return "[PDF íŒŒì¼: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ]"
            else:
                # ë‹¤ë¥¸ í”„ë¡œë°”ì´ë”ì¸ ê²½ìš° OCR ì‹œë„
                return await self._extract_text_from_pdf_via_ocr(file_path)

    def _extract_single_page_pdfminer(self, file_path: str, page_number: int) -> Optional[str]:
        """ë‹¨ì¼ í˜ì´ì§€ë¥¼ pdfminerë¡œ ì¶”ì¶œ (ë™ê¸° ì‹¤í–‰, to_threadë¡œ ë˜í•‘ë¨)"""
        try:
            # pdfminer.extract_textì€ page_numbersë¡œ ì§‘í•©ì„ ë°›ìŒ
            page_text = extract_text(file_path, page_numbers={page_number})
            return page_text
        except Exception as e:
            logger.debug(f"pdfminer single page extraction failed for page {page_number}: {e}")
            return None

    def _extract_text_from_pdf_fitz(self, file_path: str) -> str:
        """PyMuPDF(fitz)ë¥¼ ì‚¬ìš©í•´ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  í˜ì´ì§€ ë§ˆì»¤ë¥¼ í¬í•¨í•œ ë¬¸ìì—´ ë°˜í™˜"""
        try:
            import fitz
            doc = fitz.open(file_path)
            all_text = ""
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                # get_text("text")ëŠ” í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ ë¼ì¸ ë‹¨ìœ„ë¡œ ë°˜í™˜
                page_text = page.get_text("text")
                # ensure we preserve original newlines; add page marker
                all_text += f"\n=== í˜ì´ì§€ {page_num+1} ===\n"
                all_text += page_text
                if not page_text.endswith("\n"):
                    all_text += "\n"
            doc.close()
            return all_text
        except Exception as e:
            logger.error(f"PyMuPDF extraction error: {e}")
            raise

    async def _extract_pages_pdfminer(self, file_path: str, num_pages: int, max_workers: int = 4) -> List[Optional[str]]:
        """ì—¬ëŸ¬ í˜ì´ì§€ë¥¼ ë³‘ë ¬/ë°°ì¹˜ë¡œ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜

        Args:
            file_path: PDF ê²½ë¡œ
            num_pages: ì „ì²´ í˜ì´ì§€ ìˆ˜
            max_workers: ë³‘ë ¬ ì‘ì—… ìˆ˜ (ë°°ì¹˜ í¬ê¸°)
        Returns:
            í˜ì´ì§€ ì¸ë±ìŠ¤ ìˆœì„œì˜ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸(ì‹¤íŒ¨í•œ í˜ì´ì§€ëŠ” None)
        """
        results: List[Optional[str]] = [None] * num_pages
        try:
            batch_size = max_workers
            for start in range(0, num_pages, batch_size):
                batch = list(range(start, min(start + batch_size, num_pages)))
                tasks = [asyncio.to_thread(self._extract_single_page_pdfminer, file_path, p) for p in batch]
                try:
                    completed = await asyncio.gather(*tasks)
                except Exception as e:
                    logger.debug(f"Error during concurrent pdfminer page extraction batch {batch}: {e}")
                    completed = [None] * len(batch)

                for idx, page_text in zip(batch, completed):
                    results[idx] = page_text

            return results
        except Exception as e:
            logger.error(f"Failed to extract pages via pdfminer in batches: {e}")
            return results

    def _extract_text_from_pdf_layout(self, file_path: str) -> Optional[str]:
        """pdfminer ë ˆì´ì•„ì›ƒ ë¶„ì„ì„ ì‚¬ìš©í•´ í˜ì´ì§€ë³„ ë¼ì¸ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ

        ë°˜í™˜ê°’ì€ í˜ì´ì§€ ë§ˆì»¤(=== í˜ì´ì§€ N ===)ì™€ ê° ë¼ì¸ì„ ì¤„ë°”ê¿ˆìœ¼ë¡œ ê²°í•©í•œ ì „ì²´ ë¬¸ìì—´ì…ë‹ˆë‹¤.
        """
        try:
            # ì§€ì—­ ì„í¬íŠ¸ë¡œ pdfminer ì˜ì¡´ì„± ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
            from pdfminer.high_level import extract_pages
            from pdfminer.layout import LTTextContainer, LTTextLine, LAParams

            all_text = ""
            laparams = LAParams()
            for page_num, page_layout in enumerate(extract_pages(file_path, laparams=laparams)):
                lines = []
                for element in page_layout:
                    if isinstance(element, LTTextContainer):
                        for obj in element:
                            # LTTextLine ë˜ëŠ” í•˜ìœ„ ìš”ì†Œì—ì„œ í…ìŠ¤íŠ¸ íšë“
                            try:
                                text_line = obj.get_text()
                            except Exception:
                                continue
                            if text_line and text_line.strip():
                                lines.append(text_line.rstrip('\n'))

                if lines:
                    all_text += f"\n=== í˜ì´ì§€ {page_num+1} ===\n"
                    for ln in lines:
                        all_text += ln + "\n"

            return all_text if all_text.strip() else None
        except Exception as e:
            logger.debug(f"PDF layout extraction failed: {e}")
            return None

    async def _extract_text_from_pdf_fallback(self, file_path: str) -> str:
        """PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (PyPDF2 fallback)"""
        text = ""
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page_num, page in enumerate(pdf_reader.pages):
                    page_text = page.extract_text()
                    if page_text:
                        text += f"\n=== í˜ì´ì§€ {page_num + 1} ===\n"
                        text += page_text + "\n"
            return self.clean_text(text)
        except Exception as e:
            logger.error(f"Error extracting text from PDF {file_path}: {e}")
            raise
    
    async def _extract_text_from_pdf_via_ocr(self, file_path: str) -> str:
        """PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ í›„ ë°°ì¹˜ OCR ë©”ì„œë“œ ì‚¬ìš©"""
        try:
            if not PDF2IMAGE_AVAILABLE:
                logger.error("pdf2image not available for OCR processing")
                return "[PDF íŒŒì¼: pdf2image ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤]"
            
            current_config = self._get_current_image_text_config()
            if not self._is_image_text_enabled(current_config):
                logger.warning("OCR is disabled, falling back to text extraction")
                return await self._extract_text_from_pdf_fallback(file_path)
            
            import tempfile
            import os
            
            logger.info(f"Converting PDF to images for OCR: {file_path}")
            
            # PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
            images = convert_from_path(file_path, dpi=300)
            
            temp_files = []
            
            try:
                # ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                for i, image in enumerate(images):
                    with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as temp_file:
                        image.save(temp_file.name, 'PNG')
                        temp_files.append(temp_file.name)
                
                # ğŸ”¥ ë°°ì¹˜ ì²˜ë¦¬ë¡œ OCR ìˆ˜í–‰
                batch_size = current_config.get('batch_size', 1)
                logger.info(f"Processing {len(temp_files)} pages with batch size: {batch_size}")
                
                page_texts = await self._convert_images_to_text_batch(temp_files, batch_size)
                
                # ê²°ê³¼ ì¡°í•©
                all_text = ""
                for i, page_text in enumerate(page_texts):
                    if not page_text.startswith("[ì´ë¯¸ì§€ íŒŒì¼:"):  # ì˜¤ë¥˜ ë©”ì‹œì§€ê°€ ì•„ë‹Œ ê²½ìš°
                        all_text += f"\n=== í˜ì´ì§€ {i+1} (OCR) ===\n"
                        all_text += page_text + "\n"
                    else:
                        logger.warning(f"OCR failed for page {i+1}: {page_text}")
                       
            finally:
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                for temp_file in temp_files:
                    try:
                        os.unlink(temp_file)
                    except:
                        pass
            
            if all_text.strip():
                logger.info(f"Successfully extracted text via batch OCR: {len(all_text)} chars")
                return self.clean_text(all_text)
            else:
                logger.warning("OCR failed, falling back to text extraction")
                return await self._extract_text_from_pdf_fallback(file_path)
                
        except Exception as e:
            logger.error(f"OCR processing failed for {file_path}: {e}")
            logger.warning("OCR failed, falling back to text extraction")
            return await self._extract_text_from_pdf_fallback(file_path)

    # DOCX ê´€ë ¨ ë©”ì„œë“œë“¤
    async def _convert_docx_to_images(self, file_path: str) -> List[str]:
        """DOCXë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ì—¬ ì„ì‹œ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        import tempfile
        import os
        
        temp_files = []
        
        try:
            # ë°©ë²• 1: docx2pdf + pdf2image ì‚¬ìš© (ê°€ì¥ ê¶Œì¥)
            if DOCX2PDF_AVAILABLE and PDF2IMAGE_AVAILABLE:
                logger.info("Converting DOCX to PDF, then to images using docx2pdf + pdf2image")
                
                with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as temp_pdf:
                    # DOCXë¥¼ PDFë¡œ ë³€í™˜
                    docx_to_pdf_convert(file_path, temp_pdf.name)
                    
                    # PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
                    images = convert_from_path(temp_pdf.name, dpi=300)
                    
                    for i, image in enumerate(images):
                        with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as temp_img:
                            image.save(temp_img.name, 'PNG')
                            temp_files.append(temp_img.name)
                    
                    # ì„ì‹œ PDF íŒŒì¼ ì‚­ì œ
                    os.unlink(temp_pdf.name)
                    
                return temp_files
            
            # ë°©ë²• 2: LibreOffice ì»¤ë§¨ë“œë¼ì¸ ì‚¬ìš©
            elif PDF2IMAGE_AVAILABLE:
                logger.info("Trying LibreOffice command-line conversion")
                import subprocess
                
                with tempfile.TemporaryDirectory() as temp_dir:
                    try:
                        # LibreOfficeë¡œ DOCXë¥¼ PDFë¡œ ë³€í™˜
                        subprocess.run([
                            'libreoffice', '--headless', '--convert-to', 'pdf',
                            '--outdir', temp_dir, file_path
                        ], check=True, capture_output=True)
                        
                        # ë³€í™˜ëœ PDF íŒŒì¼ ì°¾ê¸°
                        pdf_name = Path(file_path).stem + '.pdf'
                        pdf_path = os.path.join(temp_dir, pdf_name)
                        
                        if os.path.exists(pdf_path):
                            # PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
                            images = convert_from_path(pdf_path, dpi=300)
                            
                            for i, image in enumerate(images):
                                with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as temp_img:
                                    image.save(temp_img.name, 'PNG')
                                    temp_files.append(temp_img.name)
                            
                            return temp_files
                        
                    except (subprocess.CalledProcessError, FileNotFoundError) as e:
                        logger.warning(f"LibreOffice conversion failed: {e}")
            
            # ëª¨ë“  ë°©ë²•ì´ ì‹¤íŒ¨í•œ ê²½ìš°
            logger.error("No available method to convert DOCX to images")
            return []
            
        except Exception as e:
            logger.error(f"Error converting DOCX to images: {e}")
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            for temp_file in temp_files:
                try:
                    os.unlink(temp_file)
                except:
                    pass
            return []

    async def _extract_text_from_docx_via_ocr(self, file_path: str) -> str:
        """DOCXë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ í›„ ë°°ì¹˜ OCR ì²˜ë¦¬"""
        try:
            current_config = self._get_current_image_text_config()
            if not self._is_image_text_enabled(current_config):
                logger.warning("OCR is disabled for DOCX, falling back to text extraction")
                return await self._extract_text_from_docx_fallback(file_path)
            
            logger.info(f"Converting DOCX to images for OCR: {file_path}")
            
            image_files = await self._convert_docx_to_images(file_path)
            
            if not image_files:
                logger.warning("Failed to convert DOCX to images, falling back to text extraction")
                return await self._extract_text_from_docx_fallback(file_path)
            
            try:
                # ğŸ”¥ ë°°ì¹˜ ì²˜ë¦¬ë¡œ OCR ìˆ˜í–‰
                batch_size = current_config.get('batch_size', 1)
                logger.info(f"Processing {len(image_files)} DOCX pages with batch size: {batch_size}")
                
                page_texts = await self._convert_images_to_text_batch(image_files, batch_size)
                
                # ê²°ê³¼ ì¡°í•©
                all_text = ""
                for i, page_text in enumerate(page_texts):
                    if not page_text.startswith("[ì´ë¯¸ì§€ íŒŒì¼:"):  # ì˜¤ë¥˜ ë©”ì‹œì§€ê°€ ì•„ë‹Œ ê²½ìš°
                        all_text += f"\n=== í˜ì´ì§€ {i+1} (OCR) ===\n"
                        all_text += page_text + "\n"
                    else:
                        logger.warning(f"OCR failed for DOCX page {i+1}: {page_text}")
                        
            finally:
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                for temp_file in image_files:
                    try:
                        import os
                        os.unlink(temp_file)
                    except:
                        pass
            
            if all_text.strip():
                logger.info(f"Successfully extracted DOCX text via batch OCR: {len(all_text)} chars")
                return self.clean_text(all_text)
            else:
                logger.warning("DOCX OCR failed, falling back to text extraction")
                return await self._extract_text_from_docx_fallback(file_path)
                
        except Exception as e:
            logger.error(f"DOCX OCR processing failed for {file_path}: {e}")
            logger.warning("DOCX OCR failed, falling back to text extraction")
            return await self._extract_text_from_docx_fallback(file_path)

    async def _extract_text_from_docx_fallback(self, file_path: str) -> str:
        """DOCX íŒŒì¼ì—ì„œ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê¸°ì¡´ ë°©ë²•)"""
        try:
            doc = Document(file_path)
            text = ""
            processed_tables = set()  # ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€
            current_page = 1
            
            # ë°©ë²• 1: ë¬¸ì„œì˜ ëª¨ë“  ìš”ì†Œë¥¼ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬ (ê³ ê¸‰ ë°©ë²•)
            try:
                for element in doc.element.body:
                    if element.tag.endswith('p'):  # ë‹¨ë½(paragraph)
                        # í˜ì´ì§€ ë¸Œë ˆì´í¬ ì²´í¬
                        if self._has_page_break(element):
                            current_page += 1
                            text += f"\n=== í˜ì´ì§€ {current_page} ===\n"
                        
                        para_text = self._extract_paragraph_text(element, doc)
                        if para_text.strip():
                            text += para_text + "\n"
                            
                    elif element.tag.endswith('tbl'):  # í‘œ(table)
                        table_text = self._extract_table_text(element)
                        if table_text.strip():
                            text += "\n=== í‘œ ===\n" + table_text + "\n=== í‘œ ë ===\n\n"
                            processed_tables.add(table_text)
                
                logger.info("Successfully used advanced DOCX parsing method")
            except Exception as e:
                logger.warning(f"Advanced parsing failed, falling back to simple method: {e}")
                # Fallback: ê°„ë‹¨í•œ ë°©ë²•ìœ¼ë¡œ ëª¨ë“  ë‹¨ë½ ì¶”ì¶œ
                text = ""
                current_page = 1
                for paragraph in doc.paragraphs:
                    # í˜ì´ì§€ ë¸Œë ˆì´í¬ ì²´í¬ (ê°„ë‹¨í•œ ë°©ë²•)
                    if self._paragraph_has_page_break(paragraph):
                        current_page += 1
                        text += f"\n=== í˜ì´ì§€ {current_page} ===\n"
                    
                    if paragraph.text.strip():
                        text += paragraph.text + "\n"
            
            # ë°©ë²• 2: ëª¨ë“  í‘œë¥¼ í™•ì‹¤íˆ ì¶”ì¶œ (ì¤‘ë³µ ì œê±°)
            for i, table in enumerate(doc.tables):
                table_text = self._extract_simple_table_text(table)
                if table_text.strip():
                    # ì´ë¯¸ ì²˜ë¦¬ëœ í‘œì¸ì§€ í™•ì¸ (ê°„ë‹¨í•œ ë¹„êµ)
                    is_duplicate = any(
                        self._is_similar_table_text(table_text, processed) 
                        for processed in processed_tables
                    )
                    
                    if not is_duplicate:
                        text += f"\n=== í‘œ {i+1} ===\n" + table_text + "\n=== í‘œ ë ===\n\n"
                        processed_tables.add(table_text)
            
            logger.info(f"Extracted {len(processed_tables)} tables from DOCX, detected {current_page} pages")
            
            # ì²« ë²ˆì§¸ í˜ì´ì§€ í—¤ë” ì¶”ê°€ (í˜ì´ì§€ê°€ ì—¬ëŸ¬ ê°œì¸ ê²½ìš°)
            if current_page > 1 and not text.startswith("=== í˜ì´ì§€ 1 ==="):
                text = f"=== í˜ì´ì§€ 1 ===\n{text}"
            
            return self.clean_text(text)
        except Exception as e:
            logger.error(f"Error extracting text from DOCX {file_path}: {e}")
            raise

    async def _extract_text_from_docx(self, file_path: str) -> str:
        """DOCX íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì‹¤ì‹œê°„ ì„¤ì •ì— ë”°ë¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë˜ëŠ” OCR)"""
        try:
            # ğŸ”¥ ì‹¤ì‹œê°„ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            current_config = self._get_current_image_text_config()
            provider = current_config.get('provider', 'no_model')
            
            logger.info(f"ğŸ”„ Real-time DOCX processing with provider: {provider}")
            
            # no_modelì¸ ê²½ìš°ì—ë§Œ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if provider == 'no_model':
                logger.info("Using DOCX text extraction mode (no_model)")
                return await self._extract_text_from_docx_fallback(file_path)
            
            else:
                # openai, vllm ë“± ë‹¤ë¥¸ í”„ë¡œë°”ì´ë”ì¸ ê²½ìš° OCR ì‹œë„
                logger.info(f"Using DOCX OCR mode with provider: {provider}")
                return await self._extract_text_from_docx_via_ocr(file_path)
                
        except Exception as e:
            logger.error(f"DOCX processing failed: {e}")
            
            # ì—ëŸ¬ ë°œìƒì‹œì—ë„ ì‹¤ì‹œê°„ ì„¤ì •ì— ë”°ë¼ ì²˜ë¦¬
            current_config = self._get_current_image_text_config()
            provider = current_config.get('provider', 'no_model')
            
            if provider == 'no_model':
                # no_modelì¸ ê²½ìš° ê¸°ë³¸ fallbackë§Œ ì‹œë„
                try:
                    return await self._extract_text_from_docx_fallback(file_path)
                except:
                    return "[DOCX íŒŒì¼: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ]"
            else:
                # ë‹¤ë¥¸ í”„ë¡œë°”ì´ë”ì¸ ê²½ìš° fallbackìœ¼ë¡œ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
                try:
                    logger.warning("DOCX OCR failed, trying basic text extraction")
                    return await self._extract_text_from_docx_fallback(file_path)
                except:
                    return "[DOCX íŒŒì¼: OCR ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ ëª¨ë‘ ì‹¤íŒ¨]"

    # PPT ê´€ë ¨ ë©”ì„œë“œë“¤
    async def _convert_ppt_to_images(self, file_path: str) -> List[str]:
        """PPTë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ì—¬ ì„ì‹œ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        import tempfile
        import os
        
        temp_files = []
        
        try:
            # ë°©ë²• 1: LibreOffice ì»¤ë§¨ë“œë¼ì¸ ì‚¬ìš© (ê°€ì¥ ê¶Œì¥)
            if PDF2IMAGE_AVAILABLE:
                logger.info("Converting PPT to PDF, then to images using LibreOffice + pdf2image")
                import subprocess
                
                with tempfile.TemporaryDirectory() as temp_dir:
                    try:
                        # LibreOfficeë¡œ PPTë¥¼ PDFë¡œ ë³€í™˜
                        subprocess.run([
                            'libreoffice', '--headless', '--convert-to', 'pdf',
                            '--outdir', temp_dir, file_path
                        ], check=True, capture_output=True)
                        
                        # ë³€í™˜ëœ PDF íŒŒì¼ ì°¾ê¸°
                        pdf_name = Path(file_path).stem + '.pdf'
                        pdf_path = os.path.join(temp_dir, pdf_name)
                        
                        if os.path.exists(pdf_path):
                            # PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
                            images = convert_from_path(pdf_path, dpi=300)
                            
                            for i, image in enumerate(images):
                                with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as temp_img:
                                    image.save(temp_img.name, 'PNG')
                                    temp_files.append(temp_img.name)
                            
                            return temp_files
                        
                    except (subprocess.CalledProcessError, FileNotFoundError) as e:
                        logger.warning(f"LibreOffice PPT conversion failed: {e}")
            
            # ë°©ë²• 2: python-pptx + PILì„ ì´ìš©í•œ í…ìŠ¤íŠ¸ ë Œë”ë§ (fallback, í’ˆì§ˆ ë‚®ìŒ)
            if PIL_AVAILABLE and PYTHON_PPTX_AVAILABLE:
                logger.warning("Using fallback PIL text rendering for PPT (low quality)")
                return await self._render_ppt_text_to_images(file_path)
            
            # ëª¨ë“  ë°©ë²•ì´ ì‹¤íŒ¨í•œ ê²½ìš°
            logger.error("No available method to convert PPT to images")
            return []
            
        except Exception as e:
            logger.error(f"Error converting PPT to images: {e}")
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            for temp_file in temp_files:
                try:
                    os.unlink(temp_file)
                except:
                    pass
            return []

    async def _render_ppt_text_to_images(self, file_path: str) -> List[str]:
        """PPT í…ìŠ¤íŠ¸ë¥¼ PILë¡œ ì´ë¯¸ì§€ë¡œ ë Œë”ë§ (fallback ë°©ë²•)"""
        import tempfile
        
        try:
            # PPTì—ì„œ ìŠ¬ë¼ì´ë“œë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            prs = Presentation(file_path)
            temp_files = []
            
            for slide_num, slide in enumerate(prs.slides):
                # ìŠ¬ë¼ì´ë“œì—ì„œ ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ
                slide_text = ""
                
                for shape in slide.shapes:
                    if hasattr(shape, "text") and shape.text.strip():
                        slide_text += shape.text + "\n"
                
                if not slide_text.strip():  # ë¹ˆ ìŠ¬ë¼ì´ë“œ ìŠ¤í‚µ
                    continue
                
                # ì´ë¯¸ì§€ ìƒì„±
                img_width, img_height = 1200, 900  # ìŠ¬ë¼ì´ë“œ ë¹„ìœ¨ (4:3)
                img = Image.new('RGB', (img_width, img_height), color='white')
                draw = ImageDraw.Draw(img)
                
                try:
                    # ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©
                    font = ImageFont.load_default()
                except:
                    font = None
                
                y_offset = 50
                line_height = 25
                
                # ìŠ¬ë¼ì´ë“œ ì œëª© ì¶”ê°€
                draw.text((50, y_offset), f"=== ìŠ¬ë¼ì´ë“œ {slide_num + 1} ===", 
                            fill='black', font=font)
                y_offset += line_height * 2
                
                # í…ìŠ¤íŠ¸ ë Œë”ë§
                lines = slide_text.split('\n')
                for line in lines:
                    if line.strip() and y_offset < img_height - 50:
                        # ê¸´ ì¤„ì€ ì—¬ëŸ¬ ì¤„ë¡œ ë¶„í• 
                        if len(line) > 80:
                            words = line.split()
                            current_line = ""
                            for word in words:
                                if len(current_line + word) < 80:
                                    current_line += word + " "
                                else:
                                    if current_line.strip():
                                        draw.text((50, y_offset), current_line.strip(), 
                                                fill='black', font=font)
                                        y_offset += line_height
                                    current_line = word + " "
                            if current_line.strip():
                                draw.text((50, y_offset), current_line.strip(), 
                                        fill='black', font=font)
                                y_offset += line_height
                        else:
                            draw.text((50, y_offset), line, fill='black', font=font)
                            y_offset += line_height
                
                # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as temp_file:
                    img.save(temp_file.name, 'PNG')
                    temp_files.append(temp_file.name)
            
            return temp_files
            
        except Exception as e:
            logger.error(f"Error rendering PPT text to images: {e}")
            return []

    async def _extract_text_from_ppt_via_ocr(self, file_path: str) -> str:
        """PPTë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ í›„ ë°°ì¹˜ OCR ì²˜ë¦¬"""
        try:
            current_config = self._get_current_image_text_config()
            if not self._is_image_text_enabled(current_config):
                logger.warning("OCR is disabled for PPT, falling back to text extraction")
                return await self._extract_text_from_ppt_fallback(file_path)
            
            logger.info(f"Converting PPT to images for OCR: {file_path}")
            
            image_files = await self._convert_ppt_to_images(file_path)
            
            if not image_files:
                logger.warning("Failed to convert PPT to images, falling back to text extraction")
                return await self._extract_text_from_ppt_fallback(file_path)
            
            try:
                # ğŸ”¥ ë°°ì¹˜ ì²˜ë¦¬ë¡œ OCR ìˆ˜í–‰
                batch_size = current_config.get('batch_size', 1)
                logger.info(f"Processing {len(image_files)} PPT slides with batch size: {batch_size}")
                
                slide_texts = await self._convert_images_to_text_batch(image_files, batch_size)
                
                # ê²°ê³¼ ì¡°í•©
                all_text = ""
                for i, slide_text in enumerate(slide_texts):
                    if not slide_text.startswith("[ì´ë¯¸ì§€ íŒŒì¼:"):  # ì˜¤ë¥˜ ë©”ì‹œì§€ê°€ ì•„ë‹Œ ê²½ìš°
                        all_text += f"\n=== ìŠ¬ë¼ì´ë“œ {i+1} (OCR) ===\n"
                        all_text += slide_text + "\n"
                    else:
                        logger.warning(f"OCR failed for PPT slide {i+1}: {slide_text}")
                        
            finally:
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                for temp_file in image_files:
                    try:
                        import os
                        os.unlink(temp_file)
                    except:
                        pass
            
            if all_text.strip():
                logger.info(f"Successfully extracted PPT text via batch OCR: {len(all_text)} chars")
                return self.clean_text(all_text)
            else:
                logger.warning("PPT OCR failed, falling back to text extraction")
                return await self._extract_text_from_ppt_fallback(file_path)
                
        except Exception as e:
            logger.error(f"PPT OCR processing failed for {file_path}: {e}")
            logger.warning("PPT OCR failed, falling back to text extraction")
            return await self._extract_text_from_ppt_fallback(file_path)

    async def _extract_text_from_ppt_fallback(self, file_path: str) -> str:
        """PPT íŒŒì¼ì—ì„œ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (python-pptx ì‚¬ìš©)"""
        if not PYTHON_PPTX_AVAILABLE:
            raise Exception("python-pptx is required for PPT file processing but is not available")
        
        try:
            prs = Presentation(file_path)
            text = ""
            
            for slide_num, slide in enumerate(prs.slides):
                logger.info(f"Processing slide {slide_num + 1}/{len(prs.slides)}")
                
                # ìŠ¬ë¼ì´ë“œ ì œëª© ì¶”ê°€
                text += f"\n=== ìŠ¬ë¼ì´ë“œ {slide_num + 1} ===\n"
                
                # ìŠ¬ë¼ì´ë“œì˜ ëª¨ë“  ë„í˜•ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                slide_content = ""
                tables_found = 0
                
                for shape in slide.shapes:
                    # í…ìŠ¤íŠ¸ ë„í˜• ì²˜ë¦¬
                    if hasattr(shape, "text") and shape.text.strip():
                        slide_content += shape.text + "\n"
                    
                    # í‘œ ì²˜ë¦¬
                    elif hasattr(shape, "table"):
                        tables_found += 1
                        slide_content += f"\n--- í‘œ {tables_found} ---\n"
                        
                        table = shape.table
                        for row in table.rows:
                            row_text = []
                            for cell in row.cells:
                                cell_text = cell.text.strip()
                                row_text.append(cell_text)
                            
                            if any(cell.strip() for cell in row_text):  # ë¹ˆ í–‰ì´ ì•„ë‹Œ ê²½ìš°ë§Œ
                                slide_content += " | ".join(row_text) + "\n"
                        
                        slide_content += f"--- í‘œ {tables_found} ë ---\n\n"
                    
                    # ì°¨íŠ¸ë‚˜ ë‹¤ë¥¸ ê°ì²´ì˜ ê²½ìš° íƒ€ì… ì •ë³´ë§Œ ì¶”ê°€
                    elif hasattr(shape, "chart"):
                        slide_content += "[ì°¨íŠ¸ ê°ì²´]\n"
                    elif hasattr(shape, "picture"):
                        slide_content += "[ì´ë¯¸ì§€ ê°ì²´]\n"
                
                # ìŠ¬ë¼ì´ë“œ ë…¸íŠ¸ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
                if hasattr(slide, "notes_slide") and slide.notes_slide.notes_text_frame:
                    notes_text = slide.notes_slide.notes_text_frame.text.strip()
                    if notes_text:
                        slide_content += f"\n[ìŠ¬ë¼ì´ë“œ ë…¸íŠ¸]\n{notes_text}\n"
                
                # ë¹ˆ ìŠ¬ë¼ì´ë“œê°€ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
                if slide_content.strip():
                    text += slide_content + "\n"
                else:
                    text += "[ë¹ˆ ìŠ¬ë¼ì´ë“œ]\n\n"
            
            logger.info(f"Extracted text from {len(prs.slides)} slides, found {tables_found} tables total")
            return self.clean_text(text)
            
        except Exception as e:
            logger.error(f"Error extracting text from PPT {file_path}: {e}")
            raise

    async def _extract_text_from_ppt(self, file_path: str) -> str:
        """PPT íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì‹¤ì‹œê°„ ì„¤ì •ì— ë”°ë¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë˜ëŠ” OCR)"""
        try:
            # ğŸ”¥ ì‹¤ì‹œê°„ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            current_config = self._get_current_image_text_config()
            provider = current_config.get('provider', 'no_model')
            
            logger.info(f"ğŸ”„ Real-time PPT processing with provider: {provider}")
            
            # no_modelì¸ ê²½ìš°ì—ë§Œ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if provider == 'no_model':
                logger.info("Using PPT text extraction mode (no_model)")
                return await self._extract_text_from_ppt_fallback(file_path)
            
            else:
                # openai, vllm ë“± ë‹¤ë¥¸ í”„ë¡œë°”ì´ë”ì¸ ê²½ìš° OCR ì‹œë„
                logger.info(f"Using PPT OCR mode with provider: {provider}")
                return await self._extract_text_from_ppt_via_ocr(file_path)
                
        except Exception as e:
            logger.error(f"PPT processing failed: {e}")
            
            # ì—ëŸ¬ ë°œìƒì‹œì—ë„ ì‹¤ì‹œê°„ ì„¤ì •ì— ë”°ë¼ ì²˜ë¦¬
            current_config = self._get_current_image_text_config()
            provider = current_config.get('provider', 'no_model')
            
            if provider == 'no_model':
                # no_modelì¸ ê²½ìš° ê¸°ë³¸ fallbackë§Œ ì‹œë„
                try:
                    return await self._extract_text_from_ppt_fallback(file_path)
                except:
                    return "[PPT íŒŒì¼: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ]"
            else:
                # ë‹¤ë¥¸ í”„ë¡œë°”ì´ë”ì¸ ê²½ìš° fallbackìœ¼ë¡œ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
                try:
                    logger.warning("PPT OCR failed, trying basic text extraction")
                    return await self._extract_text_from_ppt_fallback(file_path)
                except:
                    return "[PPT íŒŒì¼: OCR ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ ëª¨ë‘ ì‹¤íŒ¨]"

    # ê¸°ì¡´ í—¬í¼ ë©”ì„œë“œë“¤
    def _extract_paragraph_text(self, para_element, doc) -> str:
        """ë‹¨ë½ì—ì„œ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ"""
        try:
            text = ""
            nsmap = doc.element.nsmap if hasattr(doc.element, 'nsmap') else {}
            
            for run in para_element.findall('.//w:r', nsmap):
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                for text_elem in run.findall('.//w:t', nsmap):
                    if text_elem.text:
                        text += text_elem.text
                
                # ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ
                for drawing in run.findall('.//w:drawing', nsmap):
                    # ì´ë¯¸ì§€ê°€ ìˆë‹¤ëŠ” í‘œì‹œ ì¶”ê°€
                    text += " [ì´ë¯¸ì§€] "
                    
                    # ì´ë¯¸ì§€ ì„¤ëª… í…ìŠ¤íŠ¸ ì°¾ê¸° (ê°€ëŠ¥í•œ ê²½ìš°)
                    try:
                        for desc in drawing.findall('.//wp:docPr', nsmap):
                            if desc.get('descr'):
                                text += f"[ì„¤ëª…: {desc.get('descr')}] "
                            elif desc.get('name'):
                                text += f"[ì´ë¦„: {desc.get('name')}] "
                    except:
                        pass  # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨í•´ë„ ê³„ì†
            
            return text
        except Exception as e:
            logger.debug(f"Error extracting paragraph text: {e}")
            # Fallback 1: ë” ê°„ë‹¨í•œ ë°©ë²•
            try:
                text_elements = para_element.findall('.//w:t', {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'})
                return ''.join([elem.text or '' for elem in text_elements])
            except:
                # Fallback 2: ìµœì¢… ë°©ë²•
                try:
                    return para_element.text or ""
                except:
                    return ""
    
    def _extract_table_text(self, table_element) -> str:
        """í‘œ ìš”ì†Œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            text = ""
            # namespace ì •ì˜
            ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}
            
            # XMLì—ì„œ ì§ì ‘ í‘œ ë°ì´í„° ì¶”ì¶œ
            for row in table_element.findall('.//w:tr', ns):
                row_text = []
                for cell in row.findall('.//w:tc', ns):
                    cell_text = ""
                    for text_elem in cell.findall('.//w:t', ns):
                        if text_elem.text:
                            cell_text += text_elem.text
                    row_text.append(cell_text.strip())
                
                if any(cell.strip() for cell in row_text):  # ë¹ˆ í–‰ì´ ì•„ë‹Œ ê²½ìš°ë§Œ
                    text += " | ".join(row_text) + "\n"
            
            return text
        except Exception as e:
            logger.debug(f"Error extracting table text: {e}")
            return ""
    
    def _extract_simple_table_text(self, table) -> str:
        """python-docx Table ê°ì²´ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            text = ""
            for row in table.rows:
                row_text = []
                for cell in row.cells:
                    cell_text = cell.text.strip()
                    row_text.append(cell_text)
                
                if any(cell.strip() for cell in row_text):  # ë¹ˆ í–‰ì´ ì•„ë‹Œ ê²½ìš°ë§Œ
                    text += " | ".join(row_text) + "\n"
            
            return text
        except Exception as e:
            logger.debug(f"Error extracting simple table text: {e}")
            return ""
    
    def _has_page_break(self, element) -> bool:
        """XML ìš”ì†Œì—ì„œ í˜ì´ì§€ ë¸Œë ˆì´í¬ í™•ì¸"""
        try:
            # Word XMLì—ì„œ í˜ì´ì§€ ë¸Œë ˆì´í¬ í™•ì¸
            # w:br ìš”ì†Œì˜ w:typeì´ "page"ì¸ ê²½ìš°
            nsmap = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}
            
            # í˜ì´ì§€ ë¸Œë ˆì´í¬ ì°¾ê¸°
            page_breaks = element.findall('.//w:br[@w:type="page"]', nsmap)
            if page_breaks:
                return True
                
            # lastRenderedPageBreakë„ í™•ì¸
            last_page_breaks = element.findall('.//w:lastRenderedPageBreak', nsmap)
            if last_page_breaks:
                return True
                
            return False
        except Exception as e:
            logger.debug(f"Error checking page break: {e}")
            return False
    
    def _paragraph_has_page_break(self, paragraph) -> bool:
        """python-docx Paragraph ê°ì²´ì—ì„œ í˜ì´ì§€ ë¸Œë ˆì´í¬ í™•ì¸"""
        try:
            # paragraphì˜ runsì„ ê²€ì‚¬í•˜ì—¬ í˜ì´ì§€ ë¸Œë ˆì´í¬ ì°¾ê¸°
            for run in paragraph.runs:
                if run.element.findall('.//w:br[@w:type="page"]', 
                                     {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}):
                    return True
                    
            # paragraph._elementë¥¼ ì§ì ‘ ê²€ì‚¬
            if hasattr(paragraph, '_element'):
                return self._has_page_break(paragraph._element)
                
            return False
        except Exception as e:
            logger.debug(f"Error checking paragraph page break: {e}")
            return False
    
    def _extract_page_mapping(self, text: str, file_extension: str) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ì—ì„œ í˜ì´ì§€ ì •ë³´ ì¶”ì¶œ
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            file_extension: íŒŒì¼ í™•ì¥ì
            
        Returns:
            í˜ì´ì§€ ì •ë³´ ë¦¬ìŠ¤íŠ¸ [{"page_num": 1, "start_pos": 0, "end_pos": 100}, ...]
        """
        try:
            page_mapping = []
            
            # PDF, PPT, DOCXì—ì„œ í˜ì´ì§€ êµ¬ë¶„ì íŒ¨í„´ ì°¾ê¸°
            if file_extension in ['pdf', 'ppt', 'pptx', 'docx', 'doc']:
                import re
                
                # í˜ì´ì§€ êµ¬ë¶„ì íŒ¨í„´ë“¤
                patterns = [
                    r'=== í˜ì´ì§€ (\d+) ===',  # PDF PyPDF2 fallback, DOCX ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    r'=== í˜ì´ì§€ (\d+) \(OCR\) ===',  # PDF OCR
                    r'=== ìŠ¬ë¼ì´ë“œ (\d+) ===',  # PPT ê¸°ë³¸
                    r'=== ìŠ¬ë¼ì´ë“œ (\d+) \(OCR\) ===',  # PPT OCR
                ]
                
                found_pages = False
                for pattern in patterns:
                    matches = list(re.finditer(pattern, text))
                    if matches:
                        logger.info(f"Found {len(matches)} page markers with pattern: {pattern}")
                        
                        for i, match in enumerate(matches):
                            page_num = int(match.group(1))
                            start_pos = match.end()  # í˜ì´ì§€ ì œëª© ë‹¤ìŒë¶€í„°
                            
                            # ë‹¤ìŒ í˜ì´ì§€ì˜ ì‹œì‘ ìœ„ì¹˜ ë˜ëŠ” í…ìŠ¤íŠ¸ ë
                            if i + 1 < len(matches):
                                end_pos = matches[i + 1].start()
                            else:
                                end_pos = len(text)
                            
                            page_mapping.append({
                                "page_num": page_num,
                                "start_pos": start_pos,
                                "end_pos": end_pos
                            })
                        
                        # í˜ì´ì§€ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬
                        page_mapping.sort(key=lambda x: x["page_num"])
                        found_pages = True
                        break
                
                if not found_pages and file_extension in ['docx', 'doc']:
                    # OCR í˜ì´ì§€ êµ¬ë¶„ìë„ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ ê°€ìƒ í˜ì´ì§€ ìƒì„±
                    # DOCXì˜ ê²½ìš° ëŒ€ëµ 1500ìë‹¹ 1í˜ì´ì§€ë¡œ ì¶”ì •
                    chars_per_page = 1500
                    text_length = len(text)
                    
                    if text_length > chars_per_page:
                        estimated_pages = (text_length + chars_per_page - 1) // chars_per_page
                        logger.info(f"Creating {estimated_pages} virtual pages for DOCX based on text length ({text_length} chars)")
                        
                        for page_num in range(1, estimated_pages + 1):
                            start_pos = (page_num - 1) * chars_per_page
                            end_pos = min(page_num * chars_per_page, text_length)
                            
                            page_mapping.append({
                                "page_num": page_num,
                                "start_pos": start_pos,
                                "end_pos": end_pos
                            })
                        found_pages = True
                
                if not found_pages:
                    logger.info("No page markers found, treating as single page document")
                    page_mapping = [{"page_num": 1, "start_pos": 0, "end_pos": len(text)}]
            
            elif file_extension in ['xlsx', 'xls']:
                # Excel: ì‹œíŠ¸ë³„ë¡œ í˜ì´ì§€ êµ¬ë¶„
                import re
                sheet_pattern = r'=== ì‹œíŠ¸: ([^=]+) ==='
                matches = list(re.finditer(sheet_pattern, text))
                
                if matches:
                    for i, match in enumerate(matches):
                        sheet_name = match.group(1).strip()
                        start_pos = match.end()
                        
                        if i + 1 < len(matches):
                            end_pos = matches[i + 1].start()
                        else:
                            end_pos = len(text)
                        
                        page_mapping.append({
                            "page_num": i + 1,
                            "start_pos": start_pos,
                            "end_pos": end_pos,
                            "sheet_name": sheet_name
                        })
                else:
                    page_mapping = [{"page_num": 1, "start_pos": 0, "end_pos": len(text)}]
            
            else:
                # ê¸°íƒ€ íŒŒì¼: 1000ì¤„ë‹¹ 1í˜ì´ì§€ë¡œ ê°€ìƒ í˜ì´ì§€ ìƒì„±
                lines = text.split('\n')
                lines_per_page = 1000
                
                if len(lines) > lines_per_page:
                    page_count = (len(lines) + lines_per_page - 1) // lines_per_page
                    current_pos = 0
                    
                    for page_num in range(1, page_count + 1):
                        start_line = (page_num - 1) * lines_per_page
                        end_line = min(page_num * lines_per_page, len(lines))
                        
                        page_text = '\n'.join(lines[start_line:end_line])
                        start_pos = current_pos
                        end_pos = current_pos + len(page_text)
                        
                        page_mapping.append({
                            "page_num": page_num,
                            "start_pos": start_pos,
                            "end_pos": end_pos
                        })
                        
                        current_pos = end_pos + 1  # +1 for the newline
                else:
                    page_mapping = [{"page_num": 1, "start_pos": 0, "end_pos": len(text)}]
            
            return page_mapping
            
        except Exception as e:
            logger.error(f"Failed to extract page mapping: {e}")
            return [{"page_num": 1, "start_pos": 0, "end_pos": len(text)}]
    
    def _find_chunk_position(self, chunk: str, full_text: str, start_pos: int = 0) -> int:
        """ì²­í¬ì˜ ì „ì²´ í…ìŠ¤íŠ¸ ë‚´ ìœ„ì¹˜ ì°¾ê¸°"""
        try:
            # 1ì°¨: ì „ì²´ ì²­í¬ í…ìŠ¤íŠ¸ë¡œ ê²€ìƒ‰
            pos = full_text.find(chunk, start_pos)
            if pos != -1:
                return pos
            
            # 2ì°¨: ì²­í¬ì˜ ì²« ì¤„ë¡œ ê²€ìƒ‰ (10ì ì´ìƒì¸ ê²½ìš°ë§Œ)
            chunk_lines = chunk.strip().split('\n')
            if chunk_lines and len(chunk_lines[0]) >= 10:
                first_line = chunk_lines[0].strip()
                pos = full_text.find(first_line, start_pos)
                if pos != -1:
                    # ì‹¤ì œ ì²­í¬ ì‹œì‘ ìœ„ì¹˜ ì¬íƒìƒ‰
                    chunk_start = full_text.find(chunk[:50] if len(chunk) > 50 else chunk, pos)
                    if chunk_start != -1:
                        return chunk_start
                    return pos
            
            # 3ì°¨: ì²­í¬ì˜ ì²« 50ìë¡œ ê²€ìƒ‰ (10ì ì´ìƒì¸ ê²½ìš°ë§Œ)
            if len(chunk.strip()) >= 10:
                chunk_start = chunk.strip()[:50]
                pos = full_text.find(chunk_start, start_pos)
                if pos != -1:
                    return pos
            
            return -1
            
        except Exception as e:
            logger.debug(f"Error finding chunk position: {e}")
            return -1
    
    def _build_line_starts(self, text: str) -> List[int]:
        """í…ìŠ¤íŠ¸ì—ì„œ ê° ë¼ì¸ì˜ ì‹œì‘ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ ìƒì„±"""
        try:
            starts = [0]
            for idx, ch in enumerate(text):
                if ch == '\n':
                    # ë‹¤ìŒ ë¬¸ì ì¸ë±ìŠ¤ê°€ ë¼ì¸ ì‹œì‘
                    if idx + 1 < len(text):
                        starts.append(idx + 1)
            return starts
        except Exception as e:
            logger.debug(f"Error building line starts: {e}")
            return [0]

    def _build_line_offset_table(self, text: str, file_extension: str) -> List[Dict[str, int]]:
        """í…ìŠ¤íŠ¸ì—ì„œ ê° ë¼ì¸ì˜ ê¸€ë¡œë²Œ ì˜¤í”„ì…‹(start/end)ê³¼ í˜ì´ì§€ ì •ë³´ë¥¼ í¬í•¨í•œ í…Œì´ë¸” ìƒì„±

        ë°˜í™˜ê°’: [{"line_num": 1, "start": 0, "end": 10, "page": 1}, ...]
        """
        try:
            lines = text.split('\n')
            table: List[Dict[str, int]] = []
            pos = 0

            # í˜ì´ì§€ ë§¤í•‘ì„ ë¯¸ë¦¬ ìƒì„±í•´ë‘ë©´ ë¼ì¸ë³„ë¡œ í˜ì´ì§€ë¥¼ ë¹ ë¥´ê²Œ ì°¾ì„ ìˆ˜ ìˆìŒ
            page_mapping = self._extract_page_mapping(text, file_extension)

            def _page_for_pos(p: int) -> int:
                try:
                    for pinfo in page_mapping:
                        if pinfo["start_pos"] <= p < pinfo["end_pos"]:
                            return pinfo["page_num"]
                except Exception:
                    pass
                return 1

            for i, line in enumerate(lines):
                start = pos
                end = pos + len(line)
                mid = start + max(0, (end - start) // 2)
                page = _page_for_pos(mid)

                table.append({
                    "line_num": i + 1,
                    "start": start,
                    "end": end,
                    "page": page,
                })

                # +1 for the '\n' character that was removed by split (except maybe last line)
                pos = end + 1

            return table
        except Exception as e:
            logger.debug(f"Error building line offset table: {e}")
            return [{"line_num": 1, "start": 0, "end": len(text), "page": 1}]

    def _find_line_index_by_pos(self, pos: int, line_table: List[Dict[str, int]]) -> int:
        """ì´ë¶„íƒìƒ‰ìœ¼ë¡œ ìœ„ì¹˜ì— í•´ë‹¹í•˜ëŠ” ë¼ì¸ ì¸ë±ìŠ¤(0-based)ë¥¼ ë°˜í™˜"""
        try:
            if not line_table:
                return 0
            starts = [l["start"] for l in line_table]
            idx = bisect.bisect_right(starts, pos) - 1
            if idx < 0:
                return 0
            return min(idx, len(line_table) - 1)
        except Exception as e:
            logger.debug(f"Error finding line index by pos: {e}")
            return 0

    def _build_sentence_starts(self, text: str) -> List[int]:
        """ë¬¸ì¥ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„ë¦¬í•´ ê° ë¬¸ì¥ì˜ ì‹œì‘ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±

        í•œêµ­ì–´ì˜ ê²½ìš° ë¬¸ì¥ ì¢…ê²°ì–´ë¯¸(ì˜ˆ: 'ë‹¤.')ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬
        ë¬¸ì¥ ë‹¨ìœ„ì˜ ë¼ì¸ ë²ˆí˜¸ë¥¼ ì œê³µí•¨ìœ¼ë¡œì¨ PDFì—ì„œì˜ ì‹œê°ì  ë¼ì¸ê³¼
        ìœ ì‚¬í•œ êµ¬ë¶„ì„ ì–»ìŠµë‹ˆë‹¤.
        """
        try:
            starts: List[int] = []
            if not text:
                return [0]

            # ì •ê·œì‹: ê°€ëŠ¥í•œ ë¬¸ì¥ ì¢…ê²° íŒ¨í„´(ë‹¤. ë˜ëŠ” ì˜ì–´ê¶Œ . ? !)ì„ í¬ê´„
            # non-greedyë¡œ ë¬¸ì¥ ë‹¨ìœ„ë¥¼ ìº¡ì³
            pattern = re.compile(r'.*?(?:ë‹¤\.|[.?!])(?=\s+|$)', re.DOTALL)
            pos = 0
            for m in pattern.finditer(text):
                start = m.start()
                # ì²« ë¬¸ì¥ì˜ ì‹œì‘ì´ 0ì´ ì•„ë‹ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë³´ì¥
                if not starts or start != starts[-1]:
                    starts.append(start)
                pos = m.end()

            # ë‚¨ì€ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ë§ˆì§€ë§‰ ì‹œì‘ ì¶”ê°€
            if not starts:
                return [0]
            # Ensure first is 0
            if starts[0] != 0:
                starts.insert(0, 0)
            return starts
        except Exception as e:
            logger.debug(f"Error building sentence starts: {e}")
            return [0]

    def _pos_to_line(self, pos: int, line_starts: List[int]) -> int:
        """ë¬¸ìì—´ì˜ í¬ì§€ì…˜ì„ ë¼ì¸ ë²ˆí˜¸(1-based)ë¡œ ë³€í™˜"""
        try:
            if pos < 0:
                return 1
            # bisectë¡œ ê°€ì¥ í° start <= posë¥¼ ì°¾ìŒ
            idx = bisect.bisect_right(line_starts, pos) - 1
            return max(1, idx + 1)
        except Exception as e:
            logger.debug(f"Error mapping pos to line: {e}")
            return 1
    
    def _get_page_number_for_chunk(self, chunk: str, chunk_pos: int, page_mapping: List[Dict[str, Any]]) -> int:
        """ì²­í¬ì˜ í˜ì´ì§€ ë²ˆí˜¸ ê³„ì‚°"""
        try:
            if not page_mapping:
                return 1
            
            if chunk_pos == -1:
                # ìœ„ì¹˜ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ì²« ë²ˆì§¸ í˜ì´ì§€ë¡œ ì„¤ì •
                return 1
            
            # ì²­í¬ì˜ ì¤‘ê°„ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í˜ì´ì§€ ê²°ì •
            chunk_mid_pos = chunk_pos + len(chunk) // 2
            
            for page_info in page_mapping:
                if page_info["start_pos"] <= chunk_mid_pos < page_info["end_pos"]:
                    return page_info["page_num"]
            
            # ë§¤í•‘ì—ì„œ ì°¾ì§€ ëª»í•œ ê²½ìš° ê°€ì¥ ê°€ê¹Œìš´ í˜ì´ì§€ ì„ íƒ
            closest_page = min(page_mapping, key=lambda p: abs(p["start_pos"] - chunk_pos))
            logger.debug(f"Chunk position {chunk_pos} not in any page range, using closest page {closest_page['page_num']}")
            return closest_page["page_num"]
            
        except Exception as e:
            logger.error(f"Error calculating page number for chunk: {e}")
            return 1
    
    async def extract_text_from_file(self, file_path: str, file_extension: str) -> str:
        """íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        category = self.get_file_category(file_extension)
        logger.info(f"Extracting text from {file_extension} file ({category} category): {file_path}")

        if file_extension == 'pdf':
            return await self._extract_text_from_pdf(file_path)
        elif file_extension in ['docx', 'doc']:
            return await self._extract_text_from_docx(file_path)
        elif file_extension in ['pptx', 'ppt']:
            return await self._extract_text_from_ppt(file_path)
        elif file_extension in ['xlsx', 'xls']:
            return await self._extract_text_from_excel(file_path)
        elif file_extension in ['csv', 'tsv']:
            return await self._extract_text_from_csv(file_path)
        elif file_extension in self.image_types:
            return await self._convert_image_to_text(file_path)
        elif file_extension in (self.text_types + self.code_types + self.config_types + 
                                self.script_types + self.log_types + self.web_types):
            return await self._extract_text_from_text_file(file_path, file_extension)
        else:
            raise ValueError(f"Unsupported file type: {file_extension}")

    def _is_similar_table_text(self, text1: str, text2: str, threshold: float = 0.8) -> bool:
        """ë‘ í‘œ í…ìŠ¤íŠ¸ê°€ ìœ ì‚¬í•œì§€ í™•ì¸ (ì¤‘ë³µ ì œê±°ìš©)"""
        try:
            if not text1 or not text2:
                return False
            
            # ê³µë°± ì •ê·œí™”
            text1_clean = re.sub(r'\s+', ' ', text1.strip())
            text2_clean = re.sub(r'\s+', ' ', text2.strip())
            
            # ì™„ì „íˆ ë™ì¼í•œ ê²½ìš°
            if text1_clean == text2_clean:
                return True
            
            # ê¸¸ì´ê°€ ë§¤ìš° ë‹¤ë¥¸ ê²½ìš°
            len1, len2 = len(text1_clean), len(text2_clean)
            if min(len1, len2) / max(len1, len2) < 0.5:
                return False
            
            # ê°„ë‹¨í•œ ìœ ì‚¬ë„ ê²€ì‚¬ (ê³µí†µ ë¶€ë¶„ ë¹„ìœ¨)
            shorter = text1_clean if len1 < len2 else text2_clean
            longer = text2_clean if len1 < len2 else text1_clean
            
            common_ratio = len(set(shorter.split()) & set(longer.split())) / len(set(shorter.split()))
            return common_ratio >= threshold
            
        except Exception:
            return False
    
    async def _extract_text_from_excel(self, file_path: str) -> str:
        """Excel(xlsx/xls) íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (pandas ì—†ì´)"""
        ext = os.path.splitext(file_path)[1].lower()
        text = ""

        try:
            if ext == ".xlsx":
                if not OPENPYXL_AVAILABLE:
                    raise Exception("openpyxlì´ ì„¤ì¹˜ë˜ì–´ì•¼ .xlsx íŒŒì¼ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                wb = load_workbook(file_path, data_only=True)
                for sheet_name in wb.sheetnames:
                    ws = wb[sheet_name]
                    text += f"\n=== ì‹œíŠ¸: {sheet_name} ===\n"
                    for row in ws.iter_rows(values_only=True):
                        row_text = " | ".join(str(v) for v in row if v is not None)
                        if row_text.strip():
                            text += row_text + "\n"

            elif ext == ".xls":
                if not XLRD_AVAILABLE:
                    raise Exception("xlrdê°€ ì„¤ì¹˜ë˜ì–´ì•¼ .xls íŒŒì¼ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                wb = xlrd.open_workbook(file_path)
                for sheet in wb.sheets():
                    text += f"\n=== ì‹œíŠ¸: {sheet.name} ===\n"
                    for row_idx in range(sheet.nrows):
                        row_values = sheet.row_values(row_idx)
                        row_text = " | ".join(str(v) for v in row_values if v)
                        if row_text.strip():
                            text += row_text + "\n"
            else:
                raise Exception(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” Excel í˜•ì‹ì…ë‹ˆë‹¤: {ext}")

            return self.clean_text(text)
        except Exception as e:
            logger.error(f"Error extracting text from Excel {file_path}: {e}")
            raise

    async def _extract_text_from_csv(self, file_path: str, encoding: str = "utf-8") -> str:
        """CSV íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (pandas ì—†ì´)"""
        text = ""
        try:
            async with aiofiles.open(file_path, mode="r", encoding=encoding) as f:
                reader = csv.reader((await f.read()).splitlines())
                for row in reader:
                    row_text = " | ".join(row)
                    if row_text.strip():
                        text += row_text + "\n"
            return self.clean_text(text)
        except Exception as e:
            logger.error(f"Error extracting text from CSV {file_path}: {e}")
            raise
    
    async def _extract_text_from_text_file(self, file_path: str, file_type: str) -> str:
        """í…ìŠ¤íŠ¸ ê¸°ë°˜ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë‹¤ì–‘í•œ ì¸ì½”ë”© ì‹œë„)"""
        category = self.get_file_category(file_type)
        
        for encoding in self.encodings:
            try:
                async with aiofiles.open(file_path, 'r', encoding=encoding) as file:
                    text = await file.read()
                
                logger.info(f"Successfully read {file_path} with {encoding} encoding")
                
                # íŒŒì¼ ì¹´í…Œê³ ë¦¬ì— ë”°ë¼ ë‹¤ë¥¸ ì •ë¦¬ ë°©ì‹ ì ìš©
                if category == 'code':
                    return self.clean_code_text(text, file_type)
                else:
                    return self.clean_text(text)
                    
            except UnicodeDecodeError:
                logger.debug(f"Failed to read {file_path} with {encoding} encoding, trying next...")
                continue
            except Exception as e:
                logger.error(f"Error reading file {file_path} with {encoding} encoding: {e}")
                continue
        
        # ëª¨ë“  ì¸ì½”ë”©ì´ ì‹¤íŒ¨í•œ ê²½ìš°
        raise Exception(f"Could not read file {file_path} with any supported encoding")
    
    def chunk_text(self, text: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        
        Args:
            text: ë¶„í• í•  í…ìŠ¤íŠ¸
            chunk_size: ì²­í¬ í¬ê¸°
            chunk_overlap: ì²­í¬ ê°„ ì¤‘ë³µ í¬ê¸°
            
        Returns:
            ë¶„í• ëœ í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸
            
        Raises:
            Exception: í…ìŠ¤íŠ¸ ë¶„í•  ì˜¤ë¥˜
        """
        try:
            if not text or not text.strip():
                logger.warning("Empty text provided for chunking")
                return [""]
            
            # \n\n\nì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ë¨¼ì € ì´ê²ƒìœ¼ë¡œ ë¶„í• 
            if "\n\n\n" in text:
                logger.info("Found \\n\\n\\n in text, splitting by major sections first")
                major_sections = text.split("\n\n\n")
                all_chunks = []
                
                for i, section in enumerate(major_sections):
                    if not section.strip():
                        continue
                        
                    logger.info(f"Processing major section {i+1}/{len(major_sections)}")
                    
                    # ê° ì„¹ì…˜ì´ chunk_sizeë³´ë‹¤ ì‘ìœ¼ë©´ ì›ë³¸(ê³µë°± í¬í•¨) ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    if len(section) <= chunk_size:
                        all_chunks.append(section)
                    else:
                        # í° ì„¹ì…˜ì€ ì¶”ê°€ë¡œ ì²­í‚¹
                        text_splitter = RecursiveCharacterTextSplitter(
                            chunk_size=chunk_size,
                            chunk_overlap=chunk_overlap,
                            length_function=len,
                            separators=["\n\n", "\n", " ", ""]
                        )
                        section_chunks = text_splitter.split_text(section)
                        all_chunks.extend(section_chunks)
                
                logger.info(f"Text split into {len(all_chunks)} chunks using major sections (\\n\\n\\n)")
                return all_chunks
            
            else:
                # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì²­í‚¹
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap,
                    length_function=len,
                    separators=["\n\n", "\n", " ", ""]
                )
                chunks = text_splitter.split_text(text)
                logger.info(f"Text split into {len(chunks)} chunks (size: {chunk_size}, overlap: {chunk_overlap})")
                return chunks
                
        except Exception as e:
            logger.error(f"Error chunking text: {e}")
            raise
    
    def chunk_text_with_metadata(self, text: str, file_extension: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ê³  ê° ì²­í¬ì˜ ë©”íƒ€ë°ì´í„°(í˜ì´ì§€, ë¼ì¸ ì •ë³´) í¬í•¨
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            file_extension: íŒŒì¼ í™•ì¥ì
            chunk_size: ì²­í¬ í¬ê¸°
            chunk_overlap: ì²­í¬ ê°„ ê²¹ì¹¨ í¬ê¸°
            
        Returns:
            ì²­í¬ë³„ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸ [{"text": str, "page_number": int, "line_start": int, "line_end": int}, ...]
        """
        try:
            # 1. ì˜¤í”„ì…‹ ê¸°ë°˜ìœ¼ë¡œ ì²­í¬ ìƒì„± (ë¬¸ì ì˜¤í”„ì…‹ ê¸°ì¤€)
            text_len = len(text)
            if chunk_size <= 0:
                raise ValueError("chunk_size must be > 0")

            # ì•ˆì „í•œ overlap
            if chunk_overlap >= chunk_size:
                chunk_overlap = max(1, chunk_size - 1)

            chunks_with_metadata = []
            line_table = self._build_line_offset_table(text, file_extension)
            page_mapping = self._extract_page_mapping(text, file_extension)

            start = 0
            idx = 0
            step = chunk_size - chunk_overlap
            while start < text_len:
                end = min(start + chunk_size, text_len)
                chunk_text = text[start:end]

                # ê¸€ë¡œë²Œ ì˜¤í”„ì…‹
                global_start = start
                global_end = end - 1 if end > 0 else 0

                # ë¼ì¸ ì¸ë±ìŠ¤ ë§¤í•‘
                start_line_idx = self._find_line_index_by_pos(global_start, line_table)
                end_line_idx = self._find_line_index_by_pos(global_end, line_table)

                line_start = line_table[start_line_idx]["line_num"]
                line_end = line_table[end_line_idx]["line_num"]

                # í˜ì´ì§€ëŠ” ì‹œì‘ ìœ„ì¹˜ ê¸°ì¤€
                page_number = line_table[start_line_idx].get("page", 1)

                chunks_with_metadata.append({
                    "text": chunk_text,
                    "page_number": page_number,
                    "line_start": line_start,
                    "line_end": line_end,
                    "global_start": global_start,
                    "global_end": global_end,
                    "chunk_index": idx
                })

                idx += 1
                start += step
            
            logger.info(f"Created {len(chunks_with_metadata)} chunks with metadata for {file_extension} file")
            return chunks_with_metadata
            
        except Exception as e:
            logger.error(f"Failed to chunk text with metadata: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì²­í¬ë§Œ ë°˜í™˜
            chunks = self.chunk_text(text, chunk_size, chunk_overlap)
            return [{"text": chunk, "page_number": 1, "line_start": i+1, "line_end": i+1, "chunk_index": i} 
                   for i, chunk in enumerate(chunks)]
    
    def chunk_code_text(self, text: str, file_type: str, chunk_size: int = 1500, chunk_overlap: int = 300) -> List[str]:
        """ì½”ë“œ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (ì–¸ì–´ë³„ êµ¬ë¬¸ êµ¬ì¡°ë¥¼ ê³ ë ¤í•œ ë¶„í• )
            
        Args:
            text: ë¶„í• í•  ì½”ë“œ í…ìŠ¤íŠ¸
            file_type: íŒŒì¼ í˜•ì‹
            chunk_size: ì²­í¬ í¬ê¸° (ì½”ë“œëŠ” ì¢€ ë” í° ì²­í¬ ì‚¬ìš©)
            chunk_overlap: ì²­í¬ ê°„ ì¤‘ë³µ í¬ê¸°
            
        Returns:
            ë¶„í• ëœ í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        try:
            if not text or not text.strip():
                logger.warning("Empty code text provided for chunking")
                return [""]
            
            lang = LANGCHAIN_CODE_LANGUAGE_MAP.get(file_type.lower())

            if lang:
                text_splitter = RecursiveCharacterTextSplitter.from_language(
                    language=lang,
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap
                )
                logger.info(f"Using language-specific splitter for {file_type} ({lang})")
            else:
                # fallback: ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                logger.info(f"No language-specific splitter for {file_type}, using fallback.")
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap,
                    length_function=len,
                    separators=["\n\n", "\n", " ", ""]
                )

            chunks = text_splitter.split_text(text)
            logger.info(f"Code text split into {len(chunks)} chunks (size: {chunk_size}, overlap: {chunk_overlap})")
            return chunks
        except Exception as e:
            logger.error(f"Error chunking code text: {e}")
            raise
    
    def validate_file_format(self, file_path: str) -> tuple[bool, str]:
        """íŒŒì¼ í˜•ì‹ ìœ íš¨ì„± ê²€ì‚¬
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            
        Returns:
            (ìœ íš¨ì„±, íŒŒì¼í˜•ì‹) íŠœí”Œ
        """
        try:
            file_extension = Path(file_path).suffix[1:].lower()
            is_valid = file_extension in self.supported_types
            return is_valid, file_extension
        except Exception:
            return False, ""
    
    def estimate_chunks_count(self, text: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> int:
        """í…ìŠ¤íŠ¸ì—ì„œ ìƒì„±ë  ì²­í¬ ìˆ˜ ì¶”ì •
        
        Args:
            text: ëŒ€ìƒ í…ìŠ¤íŠ¸
            chunk_size: ì²­í¬ í¬ê¸°
            chunk_overlap: ì²­í¬ ê°„ ì¤‘ë³µ í¬ê¸°
            
        Returns:
            ì˜ˆìƒ ì²­í¬ ìˆ˜
        """
        if not text:
            return 0
        
        text_length = len(text)
        if text_length <= chunk_size:
            return 1
        
        # ê°„ë‹¨í•œ ì¶”ì • ê³µì‹
        effective_chunk_size = chunk_size - chunk_overlap
        estimated_chunks = (text_length - chunk_overlap) // effective_chunk_size + 1
        return max(1, estimated_chunks)
    
    def get_file_info(self, file_path: str) -> Dict[str, str]:
        """íŒŒì¼ ì •ë³´ ë°˜í™˜
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            
        Returns:
            íŒŒì¼ ì •ë³´ ë”•ì…”ë„ˆë¦¬ (extension, category, supported)
        """
        try:
            file_extension = Path(file_path).suffix[1:].lower()
            category = self.get_file_category(file_extension)
            is_supported = file_extension in self.supported_types
            
            return {
                'extension': file_extension,
                'category': category,
                'supported': str(is_supported)
            }
        except Exception:
            return {
                'extension': 'unknown',
                'category': 'unknown', 
                'supported': 'false'
            }

    def get_current_config_status(self) -> Dict[str, Any]:
        """í˜„ì¬ ì„¤ì • ìƒíƒœ ë°˜í™˜ (ë””ë²„ê¹…ìš©)"""
        try:
            current_config = self._get_current_image_text_config()
            return {
                "provider": current_config.get('provider', 'unknown'),
                "ocr_enabled": self._is_image_text_enabled(current_config),
                "base_url": current_config.get('base_url', 'unknown'),
                "model": current_config.get('model', 'unknown'),
                "temperature": current_config.get('temperature', 'unknown'),
                "batch_size": current_config.get('batch_size', 1),
                "langchain_available": LANGCHAIN_OPENAI_AVAILABLE,
                "pdf2image_available": PDF2IMAGE_AVAILABLE,
                "docx2pdf_available": DOCX2PDF_AVAILABLE,
                "python_pptx_available": PYTHON_PPTX_AVAILABLE,
                "pil_available": PIL_AVAILABLE
            }
        except Exception as e:
            return {"error": str(e)}

    def test(self):
        """ì„¤ì • í…ŒìŠ¤íŠ¸ ë©”ì„œë“œ (ë””ë²„ê¹…ìš©)"""
        try:
            current_config = self._get_current_image_text_config()
            provider = current_config.get('provider', 'no_model')
            
            logger.info(f"ğŸ” Test - Current provider: {provider}")
            logger.info(f"ğŸ” Test - Current config: {current_config}")
            logger.info(f"ğŸ” Test - OCR enabled: {self._is_image_text_enabled(current_config)}")
            
        except Exception as e:
            logger.error(f"Error in test method: {e}")
            raise