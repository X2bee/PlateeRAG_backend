# PlateERAG Backend Service ì‹œìŠ¤í…œ ê°€ì´ë“œ

## ğŸ“– ê°œìš”

PlateERAG Backendì˜ Service ì‹œìŠ¤í…œì€ **ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ê³¼ ë°ì´í„° ì²˜ë¦¬**ë¥¼ ë‹´ë‹¹í•˜ëŠ” í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤. ê° ì„œë¹„ìŠ¤ëŠ” íŠ¹ì • ë„ë©”ì¸ì˜ ê¸°ëŠ¥ì„ ë‹´ë‹¹í•˜ë©°, ì»¨íŠ¸ë¡¤ëŸ¬ì™€ ëª¨ë¸ ì‚¬ì´ì˜ ì¤‘ê°„ ê³„ì¸µ ì—­í• ì„ í•©ë‹ˆë‹¤.

### ğŸ¯ í•µì‹¬ íŠ¹ì§•
- **ëª¨ë“ˆí™”ëœ ì„¤ê³„**: ê° ì„œë¹„ìŠ¤ëŠ” ë…ë¦½ì ì¸ ê¸°ëŠ¥ ì˜ì—­ì„ ë‹´ë‹¹
- **í™•ì¥ì„±**: ìƒˆë¡œìš´ ì„œë¹„ìŠ¤ë¥¼ ì‰½ê²Œ ì¶”ê°€í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°
- **ì¬ì‚¬ìš©ì„±**: ì„œë¹„ìŠ¤ ê°„ ê³µí†µ ê¸°ëŠ¥ ê³µìœ 
- **ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ë¶„ë¦¬**: ë°ì´í„° ì•¡ì„¸ìŠ¤ì™€ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì˜ ëª…í™•í•œ ë¶„ë¦¬
- **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: ì‹¤ì‹œê°„ ì„±ëŠ¥ ì¶”ì  ë° ë¡œê¹…

## ğŸ—ï¸ Service ì•„í‚¤í…ì²˜

### í´ë” êµ¬ì¡°
```
service/
â”œâ”€â”€ README.md                    # ğŸ“– ì´ ë¬¸ì„œ
â”œâ”€â”€ database/                    # ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ connection.py            # ğŸ“¡ AppDatabaseManager
â”‚   â”œâ”€â”€ execution_meta_service.py # ğŸ”„ ì‹¤í–‰ ë©”íƒ€ë°ì´í„° ì„œë¹„ìŠ¤
â”‚   â””â”€â”€ models/                  # ğŸ“‹ ë°ì´í„° ëª¨ë¸
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ base_model.py        # ğŸ—ï¸ ê¸°ë³¸ ëª¨ë¸ í´ë˜ìŠ¤
â”‚       â”œâ”€â”€ chat.py              # ğŸ’¬ ì±„íŒ… ëª¨ë¸
â”‚       â”œâ”€â”€ executor.py          # ğŸ”„ ì‹¤í–‰ ëª¨ë¸
â”‚       â”œâ”€â”€ performance.py       # ğŸ“Š ì„±ëŠ¥ ëª¨ë¸
â”‚       â”œâ”€â”€ persistent_config_model.py # âš™ï¸ ì„¤ì • ëª¨ë¸
â”‚       â””â”€â”€ user.py              # ğŸ‘¤ ì‚¬ìš©ì ëª¨ë¸
â”œâ”€â”€ embedding/                   # ğŸ”¤ ì„ë² ë”© ì„œë¹„ìŠ¤
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_embedding.py        # ğŸ—ï¸ ê¸°ë³¸ ì„ë² ë”© í´ë˜ìŠ¤
â”‚   â”œâ”€â”€ custom_http_embedding.py # ğŸŒ HTTP ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸
â”‚   â”œâ”€â”€ embedding_factory.py     # ğŸ­ ì„ë² ë”© íŒ©í† ë¦¬
â”‚   â”œâ”€â”€ huggingface_embedding.py # ğŸ¤— HuggingFace ì„ë² ë”©
â”‚   â””â”€â”€ openai_embedding.py      # ğŸ¤– OpenAI ì„ë² ë”©
â”œâ”€â”€ general_function/            # ğŸ”§ ì¼ë°˜ ê¸°ëŠ¥
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ chat.py                  # ğŸ’¬ ì±„íŒ… ê¸°ëŠ¥
â”œâ”€â”€ monitoring/                  # ğŸ“Š ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ performance_logger.py    # ğŸ“ˆ ì„±ëŠ¥ ë¡œê¹…
â”œâ”€â”€ retrieval/                   # ğŸ” ê²€ìƒ‰ ì„œë¹„ìŠ¤
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ document_processor.py    # ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬
â”‚   â””â”€â”€ rag_service.py           # ğŸ§  RAG ì„œë¹„ìŠ¤
â””â”€â”€ vector_db/                   # ğŸ—‚ï¸ ë²¡í„° DB ì„œë¹„ìŠ¤
    â”œâ”€â”€ __init__.py
    â””â”€â”€ vector_manager.py        # ğŸ“Š ë²¡í„° DB ê´€ë¦¬
```

## ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì„œë¹„ìŠ¤ (Database)

### ğŸ“¡ AppDatabaseManager

`AppDatabaseManager`ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ ë°ì´í„°ë² ì´ìŠ¤ì˜ **í•µì‹¬ ê´€ë¦¬ ì»´í¬ë„ŒíŠ¸**ì…ë‹ˆë‹¤. ORM-like ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•˜ì—¬ ëª¨ë¸ ê¸°ë°˜ ë°ì´í„°ë² ì´ìŠ¤ ì‘ì—…ì„ ë‹¨ìˆœí™”í•©ë‹ˆë‹¤.

#### ğŸ¯ ì£¼ìš” ê¸°ëŠ¥
- **ëª¨ë¸ ë“±ë¡ ì‹œìŠ¤í…œ**: ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸ í´ë˜ìŠ¤ ìë™ ë“±ë¡
- **í…Œì´ë¸” ìë™ ìƒì„±**: ë“±ë¡ëœ ëª¨ë¸ ê¸°ë°˜ í…Œì´ë¸” ìƒì„±
- **CRUD ì‘ì—…**: Create, Read, Update, Delete ê¸°ë³¸ ì‘ì—…
- **ë‹¤ì¤‘ DB ì§€ì›**: SQLite, PostgreSQL ì§€ì›
- **íŠ¸ëœì­ì…˜ ê´€ë¦¬**: ì•ˆì „í•œ ë°ì´í„°ë² ì´ìŠ¤ ì‘ì—…

#### ğŸ”§ ì´ˆê¸°í™” ë° ì„¤ì •
```python
from service.database.connection import AppDatabaseManager
from service.database.models.user import User
from service.database.models.chat import Chat

# ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € ì´ˆê¸°í™”
db_manager = AppDatabaseManager(database_config)

# ëª¨ë¸ ë“±ë¡
db_manager.register_model(User)
db_manager.register_model(Chat)

# ë˜ëŠ” ì—¬ëŸ¬ ëª¨ë¸ í•œ ë²ˆì— ë“±ë¡
db_manager.register_models([User, Chat])

# ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” (ì—°ê²° + í…Œì´ë¸” ìƒì„±)
if db_manager.initialize_database():
    print("ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
else:
    print("ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨")
```

#### ğŸ“‹ CRUD ì‘ì—… ì˜ˆì‹œ

##### 1. **Create (ìƒì„±)**
```python
from service.database.models.user import User

# ìƒˆ ì‚¬ìš©ì ìƒì„±
user = User(
    username="john_doe",
    email="john@example.com",
    full_name="John Doe",
    created_at=datetime.now()
)

# ë°ì´í„°ë² ì´ìŠ¤ì— ì‚½ì…
user_id = db_manager.insert(user)
if user_id:
    print(f"ì‚¬ìš©ì ìƒì„± ì™„ë£Œ: ID {user_id}")
else:
    print("ì‚¬ìš©ì ìƒì„± ì‹¤íŒ¨")
```

##### 2. **Read (ì¡°íšŒ)**
```python
from service.database.models.user import User

# IDë¡œ ì‚¬ìš©ì ì¡°íšŒ
user = db_manager.find_by_id(User, user_id)
if user:
    print(f"ì‚¬ìš©ì ì¡°íšŒ: {user.username}")

# ëª¨ë“  ì‚¬ìš©ì ì¡°íšŒ (í˜ì´ì§•)
users = db_manager.find_all(User, limit=10, offset=0)
for user in users:
    print(f"ì‚¬ìš©ì: {user.username}")

# ì¡°ê±´ìœ¼ë¡œ ì‚¬ìš©ì ì¡°íšŒ
active_users = db_manager.find_by_condition(
    User, 
    conditions={"is_active": True}, 
    limit=50
)
```

##### 3. **Update (ìˆ˜ì •)**
```python
# ì‚¬ìš©ì ì •ë³´ ìˆ˜ì •
user = db_manager.find_by_id(User, user_id)
if user:
    user.full_name = "John Smith"
    user.updated_at = datetime.now()
    
    if db_manager.update(user):
        print("ì‚¬ìš©ì ì •ë³´ ìˆ˜ì • ì™„ë£Œ")
    else:
        print("ì‚¬ìš©ì ì •ë³´ ìˆ˜ì • ì‹¤íŒ¨")
```

##### 4. **Delete (ì‚­ì œ)**
```python
# ì‚¬ìš©ì ì‚­ì œ
if db_manager.delete(User, user_id):
    print("ì‚¬ìš©ì ì‚­ì œ ì™„ë£Œ")
else:
    print("ì‚¬ìš©ì ì‚­ì œ ì‹¤íŒ¨")
```

#### ğŸ” ê³ ê¸‰ ê¸°ëŠ¥

##### 1. **ë³µì¡í•œ ì¡°ê±´ ê²€ìƒ‰**
```python
# ì—¬ëŸ¬ ì¡°ê±´ìœ¼ë¡œ ê²€ìƒ‰
users = db_manager.find_by_condition(
    User, 
    conditions={
        "is_active": True,
        "created_at": datetime(2024, 1, 1)  # íŠ¹ì • ë‚ ì§œ ì´í›„
    },
    limit=100,
    offset=0
)
```

##### 2. **í…Œì´ë¸” ìƒì„± í™•ì¸**
```python
# ë“±ë¡ëœ ëª¨ë“  ëª¨ë¸ì˜ í…Œì´ë¸” ìƒì„±
if db_manager.create_tables():
    print("ëª¨ë“  í…Œì´ë¸” ìƒì„± ì™„ë£Œ")
else:
    print("í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨")
```

##### 3. **ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰**
```python
# ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ë§ˆì´ê·¸ë ˆì´ì…˜
if db_manager.run_migrations():
    print("ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ")
else:
    print("ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨")
```

#### ğŸ› ï¸ ì„¤ì • ë° ìµœì í™”

##### 1. **ì—°ê²° ì„¤ì •**
```python
# ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • í™•ì¸
db_type = db_manager.config_db_manager.db_type
print(f"ë°ì´í„°ë² ì´ìŠ¤ íƒ€ì…: {db_type}")

# ì—°ê²° ìƒíƒœ í™•ì¸
if db_manager.config_db_manager.connection:
    print("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™œì„±")
else:
    print("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë¹„í™œì„±")
```

##### 2. **ì¸ë±ìŠ¤ ìµœì í™”**
```python
# íŠ¹ì • ëª¨ë¸ì— ëŒ€í•œ ì¸ë±ìŠ¤ ìƒì„± (PersistentConfigModel ì˜ˆì‹œ)
# ìë™ìœ¼ë¡œ ì²˜ë¦¬ë˜ì§€ë§Œ ìˆ˜ë™ìœ¼ë¡œë„ ê°€ëŠ¥
index_query = "CREATE INDEX IF NOT EXISTS idx_user_email ON users(email)"
db_manager.config_db_manager.execute_query(index_query)
```

##### 3. **ì—°ê²° ì¢…ë£Œ**
```python
# ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ ì‹œ ì—°ê²° ì •ë¦¬
db_manager.close()
```

## ğŸ”¤ ì„ë² ë”© ì„œë¹„ìŠ¤ (Embedding)

### ğŸ­ EmbeddingFactory

ì„ë² ë”© ì„œë¹„ìŠ¤ëŠ” **í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜**í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì„ë² ë”© ì œê³µìë¥¼ ì§€ì›í•˜ëŠ” íŒ©í† ë¦¬ íŒ¨í„´ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

#### ğŸ¯ ì§€ì› ì„ë² ë”© ì œê³µì
- **OpenAI**: OpenAIì˜ ì„ë² ë”© ëª¨ë¸ (`text-embedding-ada-002`, `text-embedding-3-small`, `text-embedding-3-large`)
- **HuggingFace**: HuggingFaceì˜ ì˜¤í”ˆì†ŒìŠ¤ ì„ë² ë”© ëª¨ë¸
- **Custom HTTP**: ì‚¬ìš©ì ì •ì˜ HTTP API ì„ë² ë”© ì„œë¹„ìŠ¤

#### ğŸ”§ ì‚¬ìš© ì˜ˆì‹œ
```python
from service.embedding import EmbeddingFactory

# ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ ìƒì„±
embedding_client = EmbeddingFactory.create_embedding_client(vectordb_config)

# í…ìŠ¤íŠ¸ ì„ë² ë”©
text = "ì•ˆë…•í•˜ì„¸ìš”, ì„¸ê³„!"
embedding_vector = embedding_client.embed_text(text)
print(f"ì„ë² ë”© ì°¨ì›: {len(embedding_vector)}")

# ë°°ì¹˜ ì„ë² ë”©
texts = ["ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸", "ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸", "ì„¸ ë²ˆì§¸ í…ìŠ¤íŠ¸"]
embedding_vectors = embedding_client.embed_batch(texts)
print(f"ë°°ì¹˜ ì„ë² ë”© ì™„ë£Œ: {len(embedding_vectors)}ê°œ")
```

#### ğŸ“Š ì œê³µìë³„ íŠ¹ì§•
| ì œê³µì | ëª¨ë¸ ì˜ˆì‹œ | ì°¨ì› | íŠ¹ì§• |
|--------|-----------|------|------|
| OpenAI | text-embedding-ada-002 | 1536 | ê³ í’ˆì§ˆ, ìƒìš© ì„œë¹„ìŠ¤ |
| HuggingFace | sentence-transformers/all-MiniLM-L6-v2 | 384 | ì˜¤í”ˆì†ŒìŠ¤, ë‹¤ì–‘í•œ ëª¨ë¸ |
| Custom HTTP | ì‚¬ìš©ì ì •ì˜ | ê°€ë³€ | ì‚¬ìš©ì ë§ì¶¤í˜• API |

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤ (Monitoring)

### ğŸ“ˆ PerformanceLogger

`PerformanceLogger`ëŠ” **ì›Œí¬í”Œë¡œìš° ë…¸ë“œì˜ ì„±ëŠ¥ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¸¡ì •**í•˜ê³  ë¡œê¹…í•˜ëŠ” ì»´í¬ë„ŒíŠ¸ì…ë‹ˆë‹¤.

#### ğŸ¯ ì¸¡ì • í•­ëª©
- **ì²˜ë¦¬ ì‹œê°„**: ë…¸ë“œ ì‹¤í–‰ ì‹œê°„
- **CPU ì‚¬ìš©ë¥ **: í”„ë¡œì„¸ìŠ¤ ê¸°ì¤€ CPU ì‚¬ìš©ëŸ‰
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: RAM ì‚¬ìš©ëŸ‰
- **GPU ì‚¬ìš©ë¥ **: NVIDIA GPU ì‚¬ìš©ëŸ‰ (ì„ íƒì‚¬í•­)

#### ğŸ”§ ì‚¬ìš© ì˜ˆì‹œ
```python
from service.monitoring.performance_logger import PerformanceLogger

# ì„±ëŠ¥ ë¡œê±° ìƒì„±
logger = PerformanceLogger(
    workflow_name="chat_workflow",
    workflow_id="workflow_001",
    node_id="chat_node_1",
    node_name="OpenAI Chat",
    user_id="user_123",
    db_manager=db_manager
)

# ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ë¡œ ì‚¬ìš©
with logger:
    # ëª¨ë‹ˆí„°ë§í•  ì‘ì—… ì‹¤í–‰
    result = some_expensive_operation()
    
# ì„±ëŠ¥ ë°ì´í„°ê°€ ìë™ìœ¼ë¡œ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ë¨
```

## ğŸ” ê²€ìƒ‰ ì„œë¹„ìŠ¤ (Retrieval)

### ğŸ“„ DocumentProcessor

`DocumentProcessor`ëŠ” **ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë¬¸ì„œë¥¼ ì²˜ë¦¬**í•˜ì—¬ RAG ì‹œìŠ¤í…œì— ì í•©í•œ í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ì»´í¬ë„ŒíŠ¸ì…ë‹ˆë‹¤.

#### ğŸ¯ ì£¼ìš” ê¸°ëŠ¥
- **ë‹¤ì–‘í•œ íŒŒì¼ í˜•ì‹ ì§€ì›**: PDF, DOCX, DOC, TXT
- **í…ìŠ¤íŠ¸ ì¶”ì¶œ**: ë¬¸ì„œì—ì„œ ìˆœìˆ˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
- **í…ìŠ¤íŠ¸ ì •ë¦¬**: ë¶ˆí•„ìš”í•œ ê³µë°±, ì¤„ë°”ê¿ˆ ì •ë¦¬
- **ì²­í¬ ë¶„í• **: ê¸´ ë¬¸ì„œë¥¼ ê²€ìƒ‰ì— ì í•©í•œ í¬ê¸°ë¡œ ë¶„í• 

#### ğŸ“‹ ì§€ì› íŒŒì¼ í˜•ì‹
```python
document_processor = DocumentProcessor()

# ì§€ì› í˜•ì‹ í™•ì¸
supported_types = document_processor.get_supported_types()
print(f"ì§€ì› í˜•ì‹: {supported_types}")
# ì¶œë ¥: ['pdf', 'docx', 'doc', 'txt']
```

#### ğŸ”§ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜ˆì‹œ
```python
import asyncio
from service.retrieval.document_processor import DocumentProcessor

async def process_document():
    processor = DocumentProcessor()
    
    # PDF íŒŒì¼ ì²˜ë¦¬
    pdf_text = await processor.extract_text_from_file("document.pdf", "pdf")
    print(f"ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(pdf_text)}")
    
    # DOCX íŒŒì¼ ì²˜ë¦¬
    docx_text = await processor.extract_text_from_file("document.docx", "docx")
    print(f"ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(docx_text)}")
    
    # í…ìŠ¤íŠ¸ ì •ë¦¬
    cleaned_text = processor.clean_text(pdf_text)
    print(f"ì •ë¦¬ëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(cleaned_text)}")

# ë¹„ë™ê¸° ì‹¤í–‰
asyncio.run(process_document())
```

#### ğŸ“Š í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í• 
```python
from service.retrieval.document_processor import DocumentProcessor

processor = DocumentProcessor()

# ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
long_text = "ë§¤ìš° ê¸´ ë¬¸ì„œ ë‚´ìš©..." * 1000
chunks = processor.split_text_into_chunks(
    text=long_text,
    chunk_size=1000,      # ì²­í¬ í¬ê¸°
    chunk_overlap=100     # ì²­í¬ ê°„ ì¤‘ë³µ
)

print(f"ì´ ì²­í¬ ìˆ˜: {len(chunks)}")
for i, chunk in enumerate(chunks[:3]):  # ì²˜ìŒ 3ê°œ ì²­í¬ë§Œ ì¶œë ¥
    print(f"ì²­í¬ {i+1}: {len(chunk)} ë¬¸ì")
```

#### ğŸ” ê³ ê¸‰ ê¸°ëŠ¥

##### 1. **ë©”íƒ€ë°ì´í„° ì¶”ì¶œ**
```python
# íŒŒì¼ ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
file_path = "document.pdf"
metadata = await processor.extract_metadata(file_path)
text = await processor.extract_text_from_file(file_path, "pdf")

print(f"íŒŒì¼ í¬ê¸°: {metadata.get('size', 'Unknown')}")
print(f"ìƒì„±ì¼: {metadata.get('created_at', 'Unknown')}")
print(f"í˜ì´ì§€ ìˆ˜: {metadata.get('page_count', 'Unknown')}")
```

##### 2. **ë°°ì¹˜ ì²˜ë¦¬**
```python
# ì—¬ëŸ¬ íŒŒì¼ ì¼ê´„ ì²˜ë¦¬
file_paths = ["doc1.pdf", "doc2.docx", "doc3.txt"]
results = await processor.batch_process(file_paths)

for file_path, result in results.items():
    if result['success']:
        print(f"{file_path}: ì„±ê³µ - {len(result['text'])} ë¬¸ì")
    else:
        print(f"{file_path}: ì‹¤íŒ¨ - {result['error']}")
```

## ğŸ§  RAG ì„œë¹„ìŠ¤ (RAG Service)

### ğŸ¯ RAGService

`RAGService`ëŠ” **RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œì˜ í•µì‹¬**ìœ¼ë¡œ, ë¬¸ì„œ ì²˜ë¦¬, ì„ë² ë”©, ë²¡í„° ê²€ìƒ‰ì„ í†µí•©í•˜ì—¬ ì™„ì „í•œ RAG ì›Œí¬í”Œë¡œìš°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

#### ğŸ—ï¸ ì•„í‚¤í…ì²˜ êµ¬ì„±ìš”ì†Œ
- **DocumentProcessor**: ë¬¸ì„œ ì²˜ë¦¬ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
- **EmbeddingFactory**: í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
- **VectorManager**: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬
- **ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§**: RAG ì›Œí¬í”Œë¡œìš° ì¡°ì •

#### ğŸ”§ ì´ˆê¸°í™” ë° ì„¤ì •
```python
from service.retrieval.rag_service import RAGService

# RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
rag_service = RAGService(
    vectordb_config=vectordb_config,
    openai_config=openai_config  # ì„ íƒì‚¬í•­
)

# ì—°ê²° ìƒíƒœ í™•ì¸
if rag_service.is_connected():
    print("RAG ì„œë¹„ìŠ¤ ì—°ê²° ì™„ë£Œ")
else:
    print("RAG ì„œë¹„ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
```

#### ğŸ“š ì»¬ë ‰ì…˜ ê´€ë¦¬

##### 1. **ì»¬ë ‰ì…˜ ìƒì„±**
```python
# ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
result = rag_service.create_collection(
    collection_name="knowledge_base",
    description="íšŒì‚¬ ì§€ì‹ ë² ì´ìŠ¤",
    metadata={
        "category": "internal_docs",
        "department": "engineering",
        "version": "1.0"
    }
)

print(f"ì»¬ë ‰ì…˜ ìƒì„± ê²°ê³¼: {result['status']}")
```

##### 2. **ì»¬ë ‰ì…˜ ëª©ë¡ ì¡°íšŒ**
```python
# ëª¨ë“  ì»¬ë ‰ì…˜ ì¡°íšŒ
collections = rag_service.list_collections()
print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ë ‰ì…˜: {collections['collections']}")

# ì»¬ë ‰ì…˜ ìƒì„¸ ì •ë³´ ì¡°íšŒ
collection_info = rag_service.get_collection_info("knowledge_base")
print(f"ë²¡í„° ìˆ˜: {collection_info['vectors_count']}")
print(f"ë²¡í„° ì°¨ì›: {collection_info['config']['vector_size']}")
```

##### 3. **ì»¬ë ‰ì…˜ ì‚­ì œ**
```python
# ì»¬ë ‰ì…˜ ì‚­ì œ
result = rag_service.delete_collection("old_collection")
print(f"ì‚­ì œ ê²°ê³¼: {result['message']}")
```

#### ğŸ“„ ë¬¸ì„œ ì—…ë¡œë“œ ë° ì²˜ë¦¬

##### 1. **ë‹¨ì¼ ë¬¸ì„œ ì—…ë¡œë“œ**
```python
# íŒŒì¼ ì—…ë¡œë“œ
with open("document.pdf", "rb") as file:
    result = await rag_service.upload_document(
        file=file,
        filename="document.pdf",
        collection_name="knowledge_base",
        metadata={
            "author": "John Doe",
            "department": "engineering",
            "tags": ["technical", "guide"]
        }
    )

print(f"ë¬¸ì„œ ì—…ë¡œë“œ ì™„ë£Œ: {result['document_id']}")
print(f"ìƒì„±ëœ ì²­í¬ ìˆ˜: {result['chunks_count']}")
```

##### 2. **ë°°ì¹˜ ë¬¸ì„œ ì—…ë¡œë“œ**
```python
# ì—¬ëŸ¬ ë¬¸ì„œ ì¼ê´„ ì—…ë¡œë“œ
file_paths = ["doc1.pdf", "doc2.docx", "doc3.txt"]
results = await rag_service.batch_upload_documents(
    file_paths=file_paths,
    collection_name="knowledge_base",
    chunk_size=1000,
    chunk_overlap=100
)

for file_path, result in results.items():
    if result['success']:
        print(f"{file_path}: ì„±ê³µ - {result['chunks_count']} ì²­í¬")
    else:
        print(f"{file_path}: ì‹¤íŒ¨ - {result['error']}")
```

#### ğŸ” ë²¡í„° ê²€ìƒ‰ ë° RAG ì¿¼ë¦¬

##### 1. **ê¸°ë³¸ ë²¡í„° ê²€ìƒ‰**
```python
# ë²¡í„° ê²€ìƒ‰
search_results = rag_service.search_vectors(
    collection_name="knowledge_base",
    query="Python í”„ë¡œê·¸ë˜ë° ê°€ì´ë“œ",
    top_k=5,
    score_threshold=0.7
)

print(f"ê²€ìƒ‰ ê²°ê³¼: {len(search_results['results'])}ê°œ")
for i, result in enumerate(search_results['results']):
    print(f"{i+1}. ì ìˆ˜: {result['score']:.3f}")
    print(f"   ë‚´ìš©: {result['payload']['content'][:100]}...")
```

##### 2. **í•„í„°ë§ëœ ê²€ìƒ‰**
```python
# ì¡°ê±´ë¶€ ê²€ìƒ‰
filtered_results = rag_service.search_vectors(
    collection_name="knowledge_base",
    query="API ë¬¸ì„œ",
    top_k=10,
    filters={
        "department": "engineering",
        "tags": "technical"
    }
)

print(f"í•„í„°ë§ëœ ê²€ìƒ‰ ê²°ê³¼: {len(filtered_results['results'])}ê°œ")
```

##### 3. **RAG ì¿¼ë¦¬ ì‹¤í–‰**
```python
# ì™„ì „í•œ RAG ì¿¼ë¦¬
rag_result = await rag_service.query(
    query="Pythonì—ì„œ ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì„ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?",
    collection_name="knowledge_base",
    top_k=5,
    include_sources=True,
    temperature=0.7
)

print(f"RAG ì‘ë‹µ: {rag_result['answer']}")
print(f"ì°¸ì¡° ë¬¸ì„œ: {len(rag_result['sources'])}ê°œ")
for source in rag_result['sources']:
    print(f"- {source['filename']} (ì ìˆ˜: {source['score']:.3f})")
```

#### ğŸ”§ ê³ ê¸‰ ê¸°ëŠ¥

##### 1. **ì‚¬ìš©ì ì •ì˜ ì²­í¬ ì„¤ì •**
```python
# ì‚¬ìš©ì ì •ì˜ ì²­í¬ ì„¤ì •ìœ¼ë¡œ ë¬¸ì„œ ì²˜ë¦¬
custom_result = await rag_service.upload_document(
    file=file,
    filename="large_document.pdf",
    collection_name="knowledge_base",
    chunk_size=1500,      # ë” í° ì²­í¬ í¬ê¸°
    chunk_overlap=200,    # ë” í° ì¤‘ë³µ
    metadata={
        "processing_config": "large_chunks"
    }
)
```

##### 2. **ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸**
```python
# ë¬¸ì„œ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ë¬¸ì„œ ë®ì–´ì“°ê¸°)
update_result = await rag_service.update_document(
    document_id="doc_12345",
    file=updated_file,
    filename="updated_document.pdf",
    collection_name="knowledge_base"
)

print(f"ì—…ë°ì´íŠ¸ ì™„ë£Œ: {update_result['message']}")
```

##### 3. **ë¬¸ì„œ ì‚­ì œ**
```python
# íŠ¹ì • ë¬¸ì„œ ì‚­ì œ
delete_result = rag_service.delete_document(
    document_id="doc_12345",
    collection_name="knowledge_base"
)

print(f"ì‚­ì œ ì™„ë£Œ: {delete_result['message']}")
```

##### 4. **í†µê³„ ë° ë¶„ì„**
```python
# ì»¬ë ‰ì…˜ í†µê³„
stats = rag_service.get_collection_stats("knowledge_base")
print(f"ì´ ë¬¸ì„œ ìˆ˜: {stats['document_count']}")
print(f"ì´ ì²­í¬ ìˆ˜: {stats['chunk_count']}")
print(f"í‰ê·  ì²­í¬ í¬ê¸°: {stats['avg_chunk_size']}")
print(f"ìµœê·¼ ì—…ë¡œë“œ: {stats['last_upload_date']}")
```

##### 5. **ë°±ì—… ë° ë³µì›**
```python
# ì»¬ë ‰ì…˜ ë°±ì—…
backup_result = rag_service.backup_collection(
    collection_name="knowledge_base",
    backup_path="/backups/knowledge_base_backup.json"
)

# ì»¬ë ‰ì…˜ ë³µì›
restore_result = rag_service.restore_collection(
    collection_name="knowledge_base_restored",
    backup_path="/backups/knowledge_base_backup.json"
)
```

## ğŸ—‚ï¸ ë²¡í„° DB ì„œë¹„ìŠ¤ (Vector Database)

### ğŸ“Š VectorManager

`VectorManager`ëŠ” **Qdrant ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì˜ ì €ìˆ˜ì¤€ ê´€ë¦¬**ë¥¼ ë‹´ë‹¹í•˜ëŠ” í•µì‹¬ ì»´í¬ë„ŒíŠ¸ì…ë‹ˆë‹¤. ë²¡í„° ì €ì¥, ê²€ìƒ‰, ê´€ë¦¬ì˜ ëª¨ë“  ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

#### ğŸ¯ ì£¼ìš” ê¸°ëŠ¥
- **ì»¬ë ‰ì…˜ ê´€ë¦¬**: ìƒì„±, ì‚­ì œ, ì¡°íšŒ
- **ë²¡í„° ì €ì¥**: ëŒ€ìš©ëŸ‰ ë²¡í„° ë°ì´í„° ì €ì¥
- **ìœ ì‚¬ë„ ê²€ìƒ‰**: ê³ ì„±ëŠ¥ ë²¡í„° ê²€ìƒ‰
- **ë©”íƒ€ë°ì´í„° ê´€ë¦¬**: ë²¡í„°ì™€ ì—°ê²°ëœ ë©”íƒ€ë°ì´í„° ê´€ë¦¬
- **í•„í„°ë§**: ë³µì¡í•œ ì¡°ê±´ ê¸°ë°˜ ê²€ìƒ‰

#### ğŸ”§ ì´ˆê¸°í™” ë° ì—°ê²°
```python
from service.vector_db.vector_manager import VectorManager

# ë²¡í„° ë§¤ë‹ˆì € ì´ˆê¸°í™”
vector_manager = VectorManager(vectordb_config)

# ì—°ê²° ìƒíƒœ í™•ì¸
if vector_manager.is_connected():
    print("Qdrant ì—°ê²° ì„±ê³µ")
else:
    print("Qdrant ì—°ê²° ì‹¤íŒ¨")
    
# ì—°ê²° ì¬ì‹œë„
vector_manager.ensure_connected()
```

#### ğŸ“Š ì»¬ë ‰ì…˜ ê´€ë¦¬

##### 1. **ì»¬ë ‰ì…˜ ìƒì„±**
```python
# ê¸°ë³¸ ì»¬ë ‰ì…˜ ìƒì„±
result = vector_manager.create_collection(
    collection_name="documents",
    vector_size=1536,
    distance="Cosine"
)
print(f"ì»¬ë ‰ì…˜ ìƒì„±: {result['status']}")

# ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ì»¬ë ‰ì…˜ ìƒì„±
result = vector_manager.create_collection(
    collection_name="advanced_docs",
    vector_size=1536,
    distance="Cosine",
    description="ê³ ê¸‰ ë¬¸ì„œ ì»¬ë ‰ì…˜",
    metadata={
        "version": "2.0",
        "category": "premium",
        "max_documents": 10000
    }
)
```

##### 2. **ì»¬ë ‰ì…˜ ëª©ë¡ ë° ì •ë³´ ì¡°íšŒ**
```python
# ëª¨ë“  ì»¬ë ‰ì…˜ ëª©ë¡
collections = vector_manager.list_collections()
print(f"ì»¬ë ‰ì…˜ ëª©ë¡: {collections['collections']}")

# íŠ¹ì • ì»¬ë ‰ì…˜ ì •ë³´
info = vector_manager.get_collection_info("documents")
print(f"ë²¡í„° ìˆ˜: {info['vectors_count']}")
print(f"ë²¡í„° ì°¨ì›: {info['config']['vector_size']}")
print(f"ê±°ë¦¬ ë©”íŠ¸ë¦­: {info['config']['distance']}")
```

##### 3. **ì»¬ë ‰ì…˜ ì‚­ì œ**
```python
# ì»¬ë ‰ì…˜ ì‚­ì œ
result = vector_manager.delete_collection("old_collection")
print(f"ì‚­ì œ ê²°ê³¼: {result['message']}")
```

#### ğŸ“ ë²¡í„° í¬ì¸íŠ¸ ì‘ì—…

##### 1. **í¬ì¸íŠ¸ ì‚½ì…**
```python
# ë‹¨ì¼ í¬ì¸íŠ¸ ì‚½ì…
points = [
    {
        "vector": [0.1, 0.2, 0.3, ...],  # 1536ì°¨ì› ë²¡í„°
        "payload": {
            "text": "ì´ê²ƒì€ ìƒ˜í”Œ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.",
            "category": "sample",
            "timestamp": "2024-01-01T00:00:00Z"
        }
    }
]

result = vector_manager.insert_points("documents", points)
print(f"ì‚½ì… ì™„ë£Œ: {result['message']}")

# ë°°ì¹˜ í¬ì¸íŠ¸ ì‚½ì…
batch_points = []
for i in range(100):
    batch_points.append({
        "vector": [random.random() for _ in range(1536)],
        "payload": {
            "text": f"ë¬¸ì„œ {i}",
            "category": "batch",
            "index": i
        }
    })

result = vector_manager.insert_points("documents", batch_points)
print(f"ë°°ì¹˜ ì‚½ì… ì™„ë£Œ: {result['message']}")
```

##### 2. **í¬ì¸íŠ¸ ê²€ìƒ‰**
```python
# ê¸°ë³¸ ë²¡í„° ê²€ìƒ‰
query_vector = [0.1, 0.2, 0.3, ...]  # ê²€ìƒ‰ ë²¡í„°
results = vector_manager.search_points(
    collection_name="documents",
    query_vector=query_vector,
    limit=10,
    score_threshold=0.5
)

print(f"ê²€ìƒ‰ ê²°ê³¼: {results['total']}ê°œ")
for result in results['results']:
    print(f"ID: {result['id']}, ì ìˆ˜: {result['score']:.3f}")
    print(f"ë‚´ìš©: {result['payload']['text']}")
```

##### 3. **í•„í„°ë§ëœ ê²€ìƒ‰**
```python
# ì¡°ê±´ë¶€ ê²€ìƒ‰
filtered_results = vector_manager.search_points(
    collection_name="documents",
    query_vector=query_vector,
    limit=10,
    filter_criteria={
        "category": "technical",
        "timestamp": {
            "range": {
                "gte": "2024-01-01T00:00:00Z",
                "lte": "2024-12-31T23:59:59Z"
            }
        }
    }
)

print(f"í•„í„°ë§ëœ ê²€ìƒ‰ ê²°ê³¼: {len(filtered_results['results'])}ê°œ")
```

##### 4. **í¬ì¸íŠ¸ ì‚­ì œ**
```python
# íŠ¹ì • í¬ì¸íŠ¸ ì‚­ì œ
point_ids = ["point_1", "point_2", "point_3"]
result = vector_manager.delete_points("documents", point_ids)
print(f"ì‚­ì œ ì™„ë£Œ: {result['message']}")
```

#### ğŸ” ê³ ê¸‰ ê²€ìƒ‰ ê¸°ëŠ¥

##### 1. **ìŠ¤í¬ë¡¤ ê²€ìƒ‰**
```python
# ëŒ€ìš©ëŸ‰ ë°ì´í„° ìŠ¤í¬ë¡¤ ê²€ìƒ‰
points, next_offset = vector_manager.scroll_points(
    collection_name="documents",
    limit=1000,
    with_payload=True,
    with_vectors=False
)

print(f"ìŠ¤í¬ë¡¤ ê²°ê³¼: {len(points)}ê°œ í¬ì¸íŠ¸")
print(f"ë‹¤ìŒ ì˜¤í”„ì…‹: {next_offset}")

# ë‹¤ìŒ ë°°ì¹˜ ì¡°íšŒ
if next_offset:
    next_points, final_offset = vector_manager.scroll_points(
        collection_name="documents",
        limit=1000,
        offset=next_offset,
        with_payload=True,
        with_vectors=False
    )
```

##### 2. **ì¡°ê±´ë¶€ ìŠ¤í¬ë¡¤**
```python
# í•„í„° ì¡°ê±´ìœ¼ë¡œ ìŠ¤í¬ë¡¤
filtered_points, offset = vector_manager.scroll_points(
    collection_name="documents",
    filter_criteria={
        "category": "important",
        "score": {
            "range": {"gte": 0.8}
        }
    },
    limit=500
)
```

#### ğŸ› ï¸ ê´€ë¦¬ ë° ìœ ì§€ë³´ìˆ˜

##### 1. **ì»¬ë ‰ì…˜ ë¬¸ì„œ ìˆ˜ ì—…ë°ì´íŠ¸**
```python
# ë¬¸ì„œ ì¶”ê°€ ì‹œ ì¹´ìš´íŠ¸ ì¦ê°€
vector_manager.update_collection_document_count("documents", 1)

# ë¬¸ì„œ ì‚­ì œ ì‹œ ì¹´ìš´íŠ¸ ê°ì†Œ
vector_manager.update_collection_document_count("documents", -1)
```

##### 2. **ì—°ê²° ê´€ë¦¬**
```python
# ì—°ê²° ìƒíƒœ í™•ì¸ ë° ì¬ì—°ê²°
try:
    vector_manager.ensure_connected()
    print("ì—°ê²° ìƒíƒœ ì •ìƒ")
except Exception as e:
    print(f"ì—°ê²° ì˜¤ë¥˜: {e}")

# ë¦¬ì†ŒìŠ¤ ì •ë¦¬
vector_manager.cleanup()
```

##### 3. **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**
```python
# ì»¬ë ‰ì…˜ í†µê³„ ì¡°íšŒ
info = vector_manager.get_collection_info("documents")
print(f"ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰: {info.get('disk_data_size', 'N/A')}")
print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {info.get('ram_data_size', 'N/A')}")
print(f"ì¸ë±ìŠ¤ëœ ë²¡í„° ìˆ˜: {info.get('indexed_vectors_count', 'N/A')}")
```

#### ğŸ”§ ê³ ê¸‰ í™œìš© íŒ¨í„´

##### 1. **ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”**
```python
def optimized_batch_insert(vector_manager, collection_name, large_dataset):
    """ëŒ€ìš©ëŸ‰ ë°ì´í„° ìµœì í™”ëœ ë°°ì¹˜ ì‚½ì…"""
    batch_size = 1000
    
    for i in range(0, len(large_dataset), batch_size):
        batch = large_dataset[i:i + batch_size]
        
        try:
            result = vector_manager.insert_points(collection_name, batch)
            print(f"ë°°ì¹˜ {i//batch_size + 1} ì™„ë£Œ: {len(batch)}ê°œ")
        except Exception as e:
            print(f"ë°°ì¹˜ {i//batch_size + 1} ì‹¤íŒ¨: {e}")
```

##### 2. **ìœ ì‚¬ë„ ê²€ìƒ‰ ìµœì í™”**
```python
def semantic_search_with_fallback(vector_manager, collection_name, query_vector):
    """ìœ ì‚¬ë„ ê²€ìƒ‰ + í´ë°± ë¡œì§"""
    
    # 1ì°¨ ê²€ìƒ‰ (ë†’ì€ ì„ê³„ê°’)
    results = vector_manager.search_points(
        collection_name=collection_name,
        query_vector=query_vector,
        limit=5,
        score_threshold=0.8
    )
    
    # ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ 2ì°¨ ê²€ìƒ‰ (ë‚®ì€ ì„ê³„ê°’)
    if len(results['results']) < 3:
        results = vector_manager.search_points(
            collection_name=collection_name,
            query_vector=query_vector,
            limit=10,
            score_threshold=0.5
        )
    
    return results
```

##### 3. **ë©€í‹° ì»¬ë ‰ì…˜ ê²€ìƒ‰**
```python
def multi_collection_search(vector_manager, collections, query_vector):
    """ì—¬ëŸ¬ ì»¬ë ‰ì…˜ì—ì„œ ë™ì‹œ ê²€ìƒ‰"""
    all_results = []
    
    for collection_name in collections:
        try:
            results = vector_manager.search_points(
                collection_name=collection_name,
                query_vector=query_vector,
                limit=5
            )
            
            # ì»¬ë ‰ì…˜ ì •ë³´ ì¶”ê°€
            for result in results['results']:
                result['collection'] = collection_name
                all_results.append(result)
                
        except Exception as e:
            print(f"ì»¬ë ‰ì…˜ {collection_name} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
    
    # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
    all_results.sort(key=lambda x: x['score'], reverse=True)
    return all_results[:10]  # ìƒìœ„ 10ê°œ ê²°ê³¼
```

## ğŸ”§ ì„œë¹„ìŠ¤ í†µí•© ë° í™œìš©

### ğŸ¯ í†µí•© ì„œë¹„ìŠ¤ í™œìš© ì˜ˆì‹œ

```python
# ì™„ì „í•œ RAG ì‹œìŠ¤í…œ êµ¬ì¶•
class IntegratedRAGSystem:
    def __init__(self, configs):
        self.db_manager = AppDatabaseManager(configs.database)
        self.vector_manager = VectorManager(configs.vectordb)
        self.rag_service = RAGService(configs.vectordb, configs.openai)
        self.document_processor = DocumentProcessor()
        
    async def process_and_store_document(self, file_path, collection_name):
        """ë¬¸ì„œ ì²˜ë¦¬ ë° ì €ì¥ í†µí•© ì›Œí¬í”Œë¡œìš°"""
        
        # 1. ë¬¸ì„œ ì²˜ë¦¬
        text = await self.document_processor.extract_text_from_file(file_path)
        chunks = self.document_processor.split_text_into_chunks(text)
        
        # 2. ì„ë² ë”© ìƒì„±
        embedding_client = EmbeddingFactory.create_embedding_client(configs.vectordb)
        embeddings = embedding_client.embed_batch(chunks)
        
        # 3. ë²¡í„° ì €ì¥
        points = []
        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            points.append({
                "vector": embedding,
                "payload": {
                    "text": chunk,
                    "source": file_path,
                    "chunk_index": i
                }
            })
        
        self.vector_manager.insert_points(collection_name, points)
        
        # 4. ë©”íƒ€ë°ì´í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        # (í•„ìš”ì— ë”°ë¼ êµ¬í˜„)
        
        return {"chunks": len(chunks), "embeddings": len(embeddings)}
    
    async def intelligent_search(self, query, collection_name):
        """ì§€ëŠ¥í˜• ê²€ìƒ‰ ì‹œìŠ¤í…œ"""
        
        # 1. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        with PerformanceLogger("search", "search_001", "search_node", "Intelligent Search"):
            
            # 2. RAG ê²€ìƒ‰ ì‹¤í–‰
            results = await self.rag_service.query(
                query=query,
                collection_name=collection_name,
                top_k=5,
                include_sources=True
            )
            
            # 3. ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
            # (í•„ìš”ì— ë”°ë¼ êµ¬í˜„)
            
            return results
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ê´€ë ¨ íŒŒì¼ë“¤
- **[database/connection.py](database/connection.py)**: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê´€ë¦¬
- **[embedding/embedding_factory.py](embedding/embedding_factory.py)**: ì„ë² ë”© íŒ©í† ë¦¬
- **[monitoring/performance_logger.py](monitoring/performance_logger.py)**: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- **[retrieval/document_processor.py](retrieval/document_processor.py)**: ë¬¸ì„œ ì²˜ë¦¬
- **[retrieval/rag_service.py](retrieval/rag_service.py)**: RAG ì„œë¹„ìŠ¤
- **[vector_db/vector_manager.py](vector_db/vector_manager.py)**: ë²¡í„° DB ê´€ë¦¬

### ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¬¸ì„œ
- **[Qdrant](https://qdrant.tech/documentation/)**: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
- **[OpenAI](https://platform.openai.com/docs)**: OpenAI API
- **[LangChain](https://python.langchain.com/)**: LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬
- **[HuggingFace](https://huggingface.co/docs)**: HuggingFace ìƒíƒœê³„

### ëª¨ë²” ì‚¬ë¡€
- **[RAG ì‹œìŠ¤í…œ ì„¤ê³„](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html)**: RAG ì•„í‚¤í…ì²˜ ê°€ì´ë“œ
- **[ë²¡í„° ê²€ìƒ‰ ìµœì í™”](https://qdrant.tech/documentation/guides/optimization/)**: ë²¡í„° ê²€ìƒ‰ ì„±ëŠ¥ ìµœì í™”

---

**PlateERAG Backend Service System**  
*ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ â€¢ ğŸ”¤ ì„ë² ë”© ì„œë¹„ìŠ¤ â€¢ ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ â€¢ ğŸ” ë¬¸ì„œ ì²˜ë¦¬ â€¢ ğŸ§  RAG ì‹œìŠ¤í…œ â€¢ ğŸ—‚ï¸ ë²¡í„° DB*

**ì´ì œ ì„œë¹„ìŠ¤ ê³„ì¸µì„ ì™„ì „íˆ ì´í•´í•˜ê³  íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!**
