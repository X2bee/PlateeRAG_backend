# /service/data_manager/data_manager.py
import uuid
import os
import threading
import time
import sys
import io
import hashlib
from typing import Dict, Any, Optional, List
from datetime import datetime
import gc
from huggingface_hub import hf_hub_download, list_repo_files
import pyarrow.parquet as pq
import pyarrow as pa
import pyarrow.compute as pc
import pyarrow.csv as csv
from .data_manager_helper import (
    save_and_load_files,
    classify_dataset_files,
    download_and_read_file,
    process_multiple_files,
    combine_tables,
    determine_file_type_from_filename,
    create_result_info,
    generate_dataset_statistics,
    drop_columns_from_table,
    replace_column_values,
    apply_column_operation,
    remove_null_rows,
    upload_dataset_to_hf,
    copy_column,
    rename_column,
    format_columns_string,
    calculate_columns_operation,
    execute_safe_callback
)
import json
import logging

logger = logging.getLogger(__name__)

# ë‹¤ìš´ë¡œë“œ ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
downloads_path = os.path.join(os.getcwd(), "downloads")

class DataManager:
    """
    Data Manager Instance Class (Dataset-Centric êµ¬ì¡°)
    - Dataset IDë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë°ì´í„° ê´€ë¦¬
    - Manager IDëŠ” ì„¸ì…˜ ê´€ë¦¬ìš©
    - MinIO + Redis ê¸°ë°˜ ë²„ì „ ê´€ë¦¬
    """

    def __init__(self, user_id: str, user_name: str = "Unknown",
                 minio_storage=None, redis_manager=None,
                 manager_id: str = None):  # âœ… manager_id ë§¤ê°œë³€ìˆ˜ ì¶”ê°€
        """
        DataManager ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™”

        Args:
            user_id (str): ì‚¬ìš©ì ID
            user_name (str): ì‚¬ìš©ì ì´ë¦„
            minio_storage: MinIO ìŠ¤í† ë¦¬ì§€ í´ë¼ì´ì–¸íŠ¸
            redis_manager: Redis ë²„ì „ ê´€ë¦¬ì
            manager_id (str, optional): ë³µì› ì‹œ ì‚¬ìš©í•  Manager ID
        """
        # ========== ê¸°ë³¸ ì‹ë³„ì ==========
        if manager_id:
            self.manager_id = manager_id  # âœ… ë³µì› ì‹œ ê¸°ì¡´ ID ì‚¬ìš©
            self._is_restored = True
            logger.info(f"â™»ï¸ ê¸°ì¡´ Manager IDë¡œ ë³µì›: {manager_id}")
        else:
            self.manager_id = f"mgr_{uuid.uuid4().hex[:12]}"  # ìƒˆ ìƒì„± ì‹œì—ë§Œ UUID
            self._is_restored = False
            logger.info(f"âœ¨ ìƒˆ Manager ID ìƒì„±: {self.manager_id}")
        
        self.user_id = str(user_id)
        self.user_name = user_name
        self.created_at = datetime.now()
        self.is_active = True
        
        # ========== ë°ì´í„°ì…‹ ê´€ë¦¬ (Dataset-Centric) ==========
        self.dataset_id = None  # ì‹¤ì œ ë°ì´í„°ì˜ ID (Primary Key)
        self.dataset = None  # PyArrow Table
        
        # ========== ìŠ¤í† ë¦¬ì§€ í´ë¼ì´ì–¸íŠ¸ ==========
        self.minio_storage = minio_storage
        self.redis_manager = redis_manager
        
        # ========== ë²„ì „ ê´€ë¦¬ ==========
        self.current_version = 0
        self.viewing_version = 0  # í˜„ì¬ ë³´ê³  ìˆëŠ” ë²„ì „
        
        self.dataset_load_count = 0  # ë°ì´í„°ì…‹ ë¡œë“œ íšŸìˆ˜

        # ========== ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ==========
        gc.collect()
        self.initial_memory = self._get_object_memory_size()
        self._monitoring = True
        self._resource_stats = {
            'instance_memory_usage': [],
            'dataset_memory': [],
            'peak_instance_memory': self.initial_memory,
            'current_instance_memory': self.initial_memory
        }
        
        # ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ ì‹œì‘
        self._monitor_thread = threading.Thread(
            target=self._monitor_instance_memory, 
            daemon=True
        )
        self._monitor_thread.start()
        
        # ========== Manager ì„¸ì…˜ ë“±ë¡ ==========
        # âœ… ë³µì›ëœ ê²½ìš° Redis ì¬ë“±ë¡ í•˜ì§€ ì•ŠìŒ (ì´ë¯¸ ì—°ê²°ë˜ì–´ ìˆìŒ)
        if self.redis_manager and not self._is_restored:
            try:
                # ì•„ì§ datasetì´ ì—†ìœ¼ë¯€ë¡œ Noneìœ¼ë¡œ ë“±ë¡
                self.redis_manager.link_manager_to_dataset(
                    self.manager_id, 
                    None,  # dataset_idëŠ” ì²« ë¡œë“œ ì‹œ ì„¤ì •
                    self.user_id
                )
                logger.info(f"âœ… Manager ì„¸ì…˜ ë“±ë¡: {self.manager_id}")
            except Exception as e:
                logger.warning(f"Manager ì„¸ì…˜ ë“±ë¡ ì‹¤íŒ¨: {e}")
        
        logger.info(f"DataManager {'ë³µì›' if self._is_restored else 'ìƒì„±'}: {self.manager_id} (user: {self.user_id})")

    # ========== ë©”ëª¨ë¦¬ ê´€ë¦¬ ë©”ì„œë“œ ==========

    def _get_object_memory_size(self) -> int:
        """DataManager ì¸ìŠ¤í„´ìŠ¤ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°"""
        total_size = 0

        # ì¸ìŠ¤í„´ìŠ¤ ìì²´ì˜ ê¸°ë³¸ í¬ê¸°
        total_size += sys.getsizeof(self)

        # ì£¼ìš” ì†ì„±ë“¤ë§Œ ì„ ë³„ì ìœ¼ë¡œ ê³„ì‚°
        safe_attrs = [
            'manager_id', 'user_id', 'user_name', 'created_at',
            'is_active', 'initial_memory', 'dataset', 'current_version', 'dataset_id'
        ]

        for attr_name in safe_attrs:
            if hasattr(self, attr_name):
                try:
                    attr_value = getattr(self, attr_name)
                    if attr_name == 'dataset' and attr_value is not None:
                        total_size += self._calculate_dataset_memory_size(attr_value)
                    else:
                        total_size += self._calculate_deep_size(attr_value)
                except Exception as e:
                    logger.warning(f"ë©”ëª¨ë¦¬ ê³„ì‚° ì‹¤íŒ¨ ({attr_name}): {e}")
                    continue

        # _resource_statsëŠ” ë³„ë„ë¡œ ê°„ë‹¨í•˜ê²Œ ê³„ì‚°
        try:
            stats_size = sys.getsizeof(self._resource_stats)
            for key, value in self._resource_stats.items():
                stats_size += sys.getsizeof(key) + sys.getsizeof(value)
                if isinstance(value, list):
                    stats_size += sum(sys.getsizeof(item) for item in value[-10:])
            total_size += stats_size
        except:
            pass

        return total_size

    def _calculate_dataset_memory_size(self, dataset) -> int:
        """ë°ì´í„°ì…‹ ì „ìš© ë©”ëª¨ë¦¬ í¬ê¸° ê³„ì‚°"""
        if dataset is None:
            return 0

        try:
            if hasattr(dataset, 'nbytes'):
                return dataset.nbytes
            elif hasattr(dataset, 'get_total_buffer_size'):
                return dataset.get_total_buffer_size()
            else:
                return sys.getsizeof(dataset)
        except Exception as e:
            logger.warning(f"ë°ì´í„°ì…‹ ë©”ëª¨ë¦¬ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return sys.getsizeof(dataset)

    def _calculate_deep_size(self, obj, seen=None) -> int:
        """ê°ì²´ì˜ ì „ì²´ ë©”ëª¨ë¦¬ í¬ê¸°ë¥¼ ì¬ê·€ì ìœ¼ë¡œ ê³„ì‚°"""
        if seen is None:
            seen = set()

        obj_id = id(obj)
        if obj_id in seen:
            return 0

        seen.add(obj_id)
        size = sys.getsizeof(obj)

        if isinstance(obj, dict):
            for key, value in obj.items():
                size += self._calculate_deep_size(key, seen)
                size += self._calculate_deep_size(value, seen)
        elif isinstance(obj, (list, tuple, set, frozenset)):
            for item in obj:
                size += self._calculate_deep_size(item, seen)
        elif hasattr(obj, '__dict__'):
            for attr_value in obj.__dict__.values():
                size += self._calculate_deep_size(attr_value, seen)
        elif hasattr(obj, '__slots__'):
            for slot in obj.__slots__:
                if hasattr(obj, slot):
                    size += self._calculate_deep_size(getattr(obj, slot), seen)

        return size

    def _monitor_instance_memory(self):
        """DataManager ì¸ìŠ¤í„´ìŠ¤ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ"""
        consecutive_errors = 0
        max_errors = 5

        while self._monitoring and self.is_active:
            try:
                if consecutive_errors == 0:
                    gc.collect()

                current_instance_memory = self._get_object_memory_size()
                dataset_memory = self._calculate_dataset_memory_size(self.dataset)

                try:
                    self._resource_stats['instance_memory_usage'].append(current_instance_memory)
                    self._resource_stats['dataset_memory'].append(dataset_memory)
                    self._resource_stats['current_instance_memory'] = current_instance_memory

                    if current_instance_memory > self._resource_stats['peak_instance_memory']:
                        self._resource_stats['peak_instance_memory'] = current_instance_memory

                    for key in ['instance_memory_usage', 'dataset_memory']:
                        if len(self._resource_stats[key]) > 50:
                            self._resource_stats[key] = self._resource_stats[key][-50:]

                except Exception as e:
                    logger.warning(f"í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

                consecutive_errors = 0

                if dataset_memory > 100 * 1024 * 1024:
                    time.sleep(5)
                else:
                    time.sleep(10)

            except Exception as e:
                consecutive_errors += 1
                logger.warning(f"ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜ ({consecutive_errors}/{max_errors}): {e}")

                if consecutive_errors >= max_errors:
                    logger.error("ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì—°ì† ì˜¤ë¥˜ë¡œ ìŠ¤ë ˆë“œ ì¢…ë£Œ")
                    break

                time.sleep(15)

    def _calculate_checksum(self, table: pa.Table) -> str:
        """ë°ì´í„° ì²´í¬ì„¬ ê³„ì‚°"""
        try:
            buffer = io.BytesIO()
            pq.write_table(table, buffer)
            data_bytes = buffer.getvalue()
            return hashlib.sha256(data_bytes).hexdigest()
        except Exception as e:
            logger.error(f"ì²´í¬ì„¬ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return ""

    # ========== ë²„ì „ ê´€ë¦¬ ë©”ì„œë“œ ==========

    def _save_version(self, operation_name: str, metadata: Dict[str, Any] = None):
        """ë²„ì „ ì €ì¥ (Dataset ID ê¸°ì¤€) - Redis ë©”íƒ€ë¥¼ ë¨¼ì € ì €ì¥í•˜ê³  MinIOëŠ” í›„ìˆœìœ„ë¡œ ì²˜ë¦¬"""
        if self.dataset is None or self.dataset_id is None:
            logger.warning("âš ï¸  ë°ì´í„°ì…‹ ë˜ëŠ” dataset_idê°€ ì—†ì–´ ë²„ì „ ì €ì¥ ê±´ë„ˆëœ€")
            return

        try:
            logger.info(f"ğŸ“ ë²„ì „ ì €ì¥ ì‹œì‘: dataset={self.dataset_id}, version={self.current_version}, operation={operation_name}")

            minio_path = None

            # 1) Redisì— ë©”íƒ€ë°ì´í„° ì¤€ë¹„/ì €ì¥ (í•­ìƒ ì‹œë„)
            version_info = {
                "version": self.current_version,
                "operation": operation_name,
                "timestamp": datetime.now().isoformat(),
                "minio_path": None,
                "checksum": self._calculate_checksum(self.dataset),
                "num_rows": self.dataset.num_rows,
                "num_columns": self.dataset.num_columns,
                "columns": self.dataset.column_names,
                "manager_id": self.manager_id,
                "metadata": metadata or {}
            }

            if self.redis_manager:
                try:
                    self.redis_manager.save_version_metadata(self.dataset_id, self.current_version, version_info)
                    logger.info(f"  âœ… Redis ë©”íƒ€ë°ì´í„° ì €ì¥: {self.dataset_id} v{self.current_version}")
                except Exception as e:
                    logger.warning(f"  âš ï¸ Redis ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨(ê³„ì† ì§„í–‰): {e}")

            # 2) MinIOì— ìŠ¤ëƒ…ìƒ· ì €ì¥ (ì‹¤íŒ¨í•´ë„ ë©”íƒ€ëŠ” ìœ ì§€)
            if self.minio_storage:
                try:
                    minio_path = self.minio_storage.save_version_snapshot(
                        self.dataset_id,
                        self.current_version,
                        self.dataset,
                        operation_name,
                        metadata or {}
                    )
                    logger.info(f"  âœ… MinIO ìŠ¤ëƒ…ìƒ· ì €ì¥: {minio_path}")
                except Exception as e:
                    logger.warning(f"  âš ï¸ MinIO ìŠ¤ëƒ…ìƒ· ì €ì¥ ì‹¤íŒ¨(ë©”íƒ€ëŠ” ìœ ì§€ë¨): {e}")

            # 3) Redis ë©”íƒ€ì— minio_pathê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸ ì‹œë„
            if self.redis_manager and minio_path:
                try:
                    existing = self.redis_manager.get_version_metadata(self.dataset_id, self.current_version)
                    if existing:
                        existing['minio_path'] = minio_path
                        key = f"{self.redis_manager.dataset_prefix}:{self.dataset_id}:version:{self.current_version}"
                        self.redis_manager.redis_client.set(key, json.dumps(existing))
                        logger.info(f"  âœ… Redis ë²„ì „ ë©”íƒ€ì— minio_path ì—…ë°ì´íŠ¸")
                except Exception as e:
                    logger.warning(f"  âš ï¸ Redis minio_path ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

            # 4) ë¡œì»¬ current_version ì¦ê°€
            old_version = self.current_version
            self.current_version += 1
            logger.info(f"âœ… ë²„ì „ ì €ì¥ ì™„ë£Œ: {self.dataset_id} v{old_version} ({operation_name}) â†’ ë‹¤ìŒ ë²„ì „: v{self.current_version}")

        except Exception as e:
            logger.error(f"âŒ ë²„ì „ ì €ì¥ ì‹¤íŒ¨: {e}", exc_info=True)

    def get_version_history(self) -> List[Dict[str, Any]]:
        """ë²„ì „ ì´ë ¥ ì¡°íšŒ (Dataset ID ê¸°ì¤€)"""
        if self.redis_manager and self.dataset_id:
            return self.redis_manager.get_all_versions(self.dataset_id)
        return []

    def rollback_to_version(self, version: int) -> Dict[str, Any]:
        """íŠ¹ì • ë²„ì „ìœ¼ë¡œ ë¡¤ë°± (Dataset ID ê¸°ì¤€)"""
        if not self.redis_manager or not self.minio_storage:
            raise RuntimeError("ë²„ì „ ê´€ë¦¬ ê¸°ëŠ¥ì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        if not self.dataset_id:
            raise RuntimeError("dataset_idê°€ ì—†ìŠµë‹ˆë‹¤")

        try:
            # ë²„ì „ ë©”íƒ€ë°ì´í„° ì¡°íšŒ
            version_info = self.redis_manager.get_version_metadata(self.dataset_id, version)

            if not version_info:
                raise ValueError(f"ë²„ì „ {version}ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            operation_name = version_info["operation"]
            
            # MinIOì—ì„œ í•´ë‹¹ ë²„ì „ ë¡œë“œ
            self.dataset = self.minio_storage.load_version_snapshot(
                self.dataset_id,
                version,
                operation_name
            )

            self.current_version = version + 1
            self.viewing_version = version

            logger.info(f"âœ… ë²„ì „ {version}ìœ¼ë¡œ ë¡¤ë°± ì™„ë£Œ: {self.dataset_id}")

            return {
                "success": True,
                "dataset_id": self.dataset_id,
                "rolled_back_to_version": version,
                "operation": operation_name,
                "num_rows": self.dataset.num_rows,
                "num_columns": self.dataset.num_columns
            }

        except Exception as e:
            logger.error(f"ë¡¤ë°± ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"ë¡¤ë°± ì‹¤íŒ¨: {str(e)}")

    # ========== ë¦¬ì†ŒìŠ¤ í†µê³„ ==========

    def get_resource_stats(self) -> Dict[str, Any]:
        """DataManager ì¸ìŠ¤í„´ìŠ¤ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í†µê³„ ë°˜í™˜"""
        try:
            current_instance_memory = self._resource_stats.get('current_instance_memory', 0)
            dataset_memory = self._calculate_dataset_memory_size(self.dataset)

            memory_history = self._resource_stats.get('instance_memory_usage', [])
            dataset_history = self._resource_stats.get('dataset_memory', [])

            recent_memory = memory_history[-10:] if memory_history else [current_instance_memory]
            recent_dataset = dataset_history[-10:] if dataset_history else [dataset_memory]

            avg_memory = sum(recent_memory) / len(recent_memory) if recent_memory else 0
            avg_dataset = sum(recent_dataset) / len(recent_dataset) if recent_dataset else 0

            return {
                'manager_id': self.manager_id,
                'dataset_id': self.dataset_id,
                'user_id': self.user_id,
                'user_name': self.user_name,
                'created_at': self.created_at.isoformat(),
                'is_active': self.is_active,

                # ë©”ëª¨ë¦¬ ì •ë³´
                'current_instance_memory_mb': current_instance_memory / (1024 * 1024),
                'initial_instance_memory_mb': self.initial_memory / (1024 * 1024),
                'peak_instance_memory_mb': self._resource_stats.get('peak_instance_memory', 0) / (1024 * 1024),
                'memory_growth_mb': (current_instance_memory - self.initial_memory) / (1024 * 1024),
                'dataset_memory_mb': dataset_memory / (1024 * 1024),
                'average_memory_mb': avg_memory / (1024 * 1024),
                'average_dataset_mb': avg_dataset / (1024 * 1024),

                # ë°ì´í„° ìƒíƒœ
                'has_dataset': self.dataset is not None,
                'dataset_rows': self.dataset.num_rows if self.dataset is not None else 0,
                'dataset_columns': self.dataset.num_columns if self.dataset is not None else 0,

                # ë²„ì „ ì •ë³´
                'current_version': self.current_version,
                'viewing_version': self.viewing_version,
                'version_management_enabled': self.minio_storage is not None and self.redis_manager is not None,

                # ë©”ëª¨ë¦¬ ë¶„í¬
                'memory_distribution': {
                    'dataset_percent': (dataset_memory / current_instance_memory * 100) if current_instance_memory > 0 else 0,
                    'other_percent': ((current_instance_memory - dataset_memory) / current_instance_memory * 100) if current_instance_memory > 0 else 0
                },

                # ëª¨ë‹ˆí„°ë§ ìƒíƒœ
                'monitoring_active': self._monitoring,
                'memory_samples_count': len(memory_history)
            }

        except Exception as e:
            logger.error(f"ë¦¬ì†ŒìŠ¤ í†µê³„ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                'manager_id': self.manager_id,
                'dataset_id': self.dataset_id,
                'user_id': self.user_id,
                'is_active': self.is_active,
                'error': f"í†µê³„ ìƒì„± ì‹¤íŒ¨: {str(e)}",
                'has_dataset': self.dataset is not None,
                'current_version': self.current_version
            }

    # ========== ë°ì´í„°ì…‹ ê¸°ë³¸ ì¡°ì‘ ==========

    def get_dataset(self) -> Any:
        """ë°ì´í„°ì…‹ ë°˜í™˜"""
        if not self.is_active:
            raise RuntimeError("DataManager is not active")
        return self.dataset

    def get_dataset_sample(self, num_samples: int = 10) -> Dict[str, Any]:
        """ë°ì´í„°ì…‹ ìƒ˜í”Œ ì¡°íšŒ"""
        if not self.is_active:
            raise RuntimeError("DataManager is not active")

        if self.dataset is None:
            return {
                "success": True,
                "sample_data": [],
                "sample_count": 0,
                "total_rows": 0,
                "total_columns": 0,
                "columns": [],
                "column_info": "",
                "sampled_at": datetime.now().isoformat()
            }

        try:
            actual_samples = min(num_samples, self.dataset.num_rows)
            sample_table = self.dataset.slice(0, actual_samples)

            column_info = {}
            for i, col_name in enumerate(sample_table.column_names):
                col_type = str(sample_table.schema.field(i).type)
                column_info[col_name] = {
                    "type": col_type,
                    "nullable": sample_table.schema.field(i).nullable
                }

            sample_records = []
            for row_idx in range(sample_table.num_rows):
                record = {}
                for col_idx, col_name in enumerate(sample_table.column_names):
                    value = sample_table.column(col_idx)[row_idx].as_py()
                    record[col_name] = None if value is None else value
                sample_records.append(record)

            return {
                "success": True,
                "sample_data": sample_records,
                "sample_count": len(sample_records),
                "total_rows": self.dataset.num_rows,
                "total_columns": self.dataset.num_columns,
                "columns": sample_table.column_names,
                "column_info": column_info,
                "sampled_at": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"Failed to get dataset sample: {e}")
            raise RuntimeError(f"Dataset sample retrieval failed: {str(e)}")

    def set_dataset(self, dataset: Any) -> None:
        """ë°ì´í„°ì…‹ ì„¤ì •"""
        if not self.is_active:
            raise RuntimeError("DataManager is not active")
        self.dataset = dataset
        logger.info(f"Dataset set for manager {self.manager_id}")

    def remove_dataset(self) -> bool:
        """ë°ì´í„°ì…‹ ì œê±°"""
        if self.dataset is not None:
            self.dataset = None
            gc.collect()
            logger.info(f"Dataset removed from manager {self.manager_id}")
            return True
        return False

    # ========== ë°ì´í„°ì…‹ ë¡œë“œ ë©”ì„œë“œ ==========

    def hf_download_and_load_dataset(self, repo_id: str, filename: str = None, 
                                split: str = None) -> Dict[str, Any]:
        if not self.is_active:
            raise RuntimeError("DataManager is not active")

        try:
            # ë‹¤ìš´ë¡œë“œ ì¤€ë¹„...
            cache_dir = os.path.join(downloads_path, "huggingface_cache")
            os.makedirs(cache_dir, exist_ok=True)
            logger.info("HuggingFace ë‹¤ìš´ë¡œë“œ ì‹œì‘: repo=%s, user=%s", repo_id, self.user_id)

            # íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° í…Œì´ë¸” ì¡°í•©
            if filename is None:
                repo_files = list_repo_files(repo_id, repo_type='dataset')
                target_files, file_type = classify_dataset_files(repo_files, split)
                tables, downloaded_paths = process_multiple_files(repo_id, target_files, file_type, cache_dir)
                combined_table = combine_tables(tables, target_files)
                result_info = create_result_info(repo_id=repo_id, file_type=file_type, combined_table=combined_table, files_processed=target_files, local_paths=downloaded_paths)
            else:
                file_type = determine_file_type_from_filename(filename)
                combined_table, downloaded_path = download_and_read_file(repo_id, filename, file_type, cache_dir)
                result_info = create_result_info(repo_id=repo_id, file_type=file_type, combined_table=combined_table, filename=filename, local_path=downloaded_path)

            # ë°ì´í„°ì…‹ ì„¤ì •
            self.dataset = combined_table
            logger.info("í…Œì´ë¸” ë¡œë“œ ì™„ë£Œ: %dí–‰, %dì—´", combined_table.num_rows, combined_table.num_columns)

            # load_count ì¦ê°€ (í•­ìƒ)
            self.dataset_load_count += 1
            is_first_load = (self.dataset_id is None)

            # Dataset ID ìƒì„± ë° Redis ë“±ë¡ (ì›ìì  ë³´ì¥)
            if is_first_load:
                repo_slug = repo_id.replace('/', '_')
                unique_id = uuid.uuid4().hex[:8]
                self.dataset_id = f"ds_hf_{repo_slug}_{unique_id}"
                logger.info(f"âœ¨ ìƒˆ Dataset ID ìƒì„±: {self.dataset_id}")

                if self.redis_manager:
                    try:
                        dataset_metadata = {
                            "source_type": "huggingface",
                            "repo_id": repo_id,
                            "filename": filename,
                            "split": split,
                            "created_at": datetime.now().isoformat(),
                            "created_by": self.user_id,
                            "original_rows": combined_table.num_rows,
                            "original_columns": combined_table.num_columns
                        }
                        self.redis_manager.register_dataset(self.dataset_id, self.user_id, dataset_metadata)
                        self.redis_manager.link_manager_to_dataset(self.manager_id, self.dataset_id, self.user_id)
                        logger.info("âœ… Redis ë°ì´í„°ì…‹ ë“±ë¡ ë° Manager-Dataset ë§í¬ ì™„ë£Œ")
                    except Exception as e:
                        logger.warning(f"Redis ë“±ë¡ ì‹¤íŒ¨: {e}")

                # --- ì¶”ê°€: ì›ë³¸(raw-datasets) ì €ì¥ ---
                if self.minio_storage:
                    try:
                        metadata_for_minio = dataset_metadata if 'dataset_metadata' in locals() else {}
                        self.minio_storage.save_original_dataset(self.user_id, self.dataset_id, self.dataset, metadata_for_minio)
                        logger.info(f"âœ… MinIO ì›ë³¸ ì €ì¥ ì™„ë£Œ: raw-datasets/{self.user_id}/{self.dataset_id}/original.parquet")
                    except Exception as e:
                        logger.warning(f"MinIO ì›ë³¸ ì €ì¥ ì‹¤íŒ¨(ê³„ì†): {e}")

            else:
                logger.info(f"â™»ï¸ ê¸°ì¡´ Dataset ì¬ë¡œë“œ: {self.dataset_id} (ë¡œë“œ {self.dataset_load_count}íšŒì°¨)")

            # ì†ŒìŠ¤ ì •ë³´ ìƒì„±
            source_info = {
                "type": "huggingface",
                "repo_id": repo_id,
                "filename": filename,
                "split": split,
                "file_type": file_type,
                "loaded_at": datetime.now().isoformat(),
                "checksum": self._calculate_checksum(self.dataset),
                "num_rows": combined_table.num_rows,
                "num_columns": combined_table.num_columns,
                "columns": combined_table.column_names,
                "load_count": self.dataset_load_count,
                "is_reload": not is_first_load
            }

            # Redisì— ì†ŒìŠ¤ ì •ë³´ ì €ì¥ (í•­ìƒ ì‹œë„)
            if self.redis_manager:
                try:
                    self.redis_manager.save_source_info(self.dataset_id, source_info)
                    logger.info("âœ… Redis ì†ŒìŠ¤ ì •ë³´ ì €ì¥")
                except Exception as e:
                    logger.warning(f"Redis ì†ŒìŠ¤ ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {e}")

            # operation ì´ë¦„ ê²°ì •
            operation_name = "initial_load" if is_first_load else f"reload_{self.dataset_load_count}"
            logger.info(f"ğŸ’¾ ë²„ì „ ì €ì¥: operation={operation_name}, load_count={self.dataset_load_count}")

            # ì•ˆì „í•œ ë²„ì „ ì €ì¥ (Redis ìš°ì„ , MinIO í›„ìˆœìœ„)
            self._save_version(operation_name, source_info)

            # ê²°ê³¼ ë³´ê°•
            result_info.update({
                "dataset_id": self.dataset_id,
                "manager_id": self.manager_id,
                "user_id": self.user_id,
                "is_new_dataset": is_first_load,
                "current_version": self.current_version - 1,
                "load_count": self.dataset_load_count,
                "is_new_version": not is_first_load,
                "source_info": source_info
            })

            logger.info(f"âœ… HuggingFace ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: dataset={self.dataset_id}, version={self.current_version - 1}, load_count={self.dataset_load_count}")
            return result_info

        except Exception as e:
            logger.error(f"HuggingFace ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: repo={repo_id}, error={e}", exc_info=True)
            raise RuntimeError(f"Dataset download/load failed: {str(e)}")
            
    def local_upload_and_load_dataset(self, uploaded_files, filenames: List[str]) -> Dict[str, Any]:
        if not self.is_active:
            raise RuntimeError("DataManager is not active")

        try:
            if not isinstance(uploaded_files, list):
                uploaded_files = [uploaded_files]
            if not isinstance(filenames, list):
                filenames = [filenames]

            logger.info("ë¡œì»¬ íŒŒì¼ ì—…ë¡œë“œ ì‹œì‘: %dê°œ íŒŒì¼", len(uploaded_files))

            # íŒŒì¼ ì €ì¥ ë° ë¡œë“œ
            combined_table, base_dataset_id = save_and_load_files(uploaded_files, filenames, self.manager_id)

            # load count ì¦ê°€
            self.dataset_load_count += 1
            is_first_load = (self.dataset_id is None)

            # Dataset ID ìƒì„± ë° Redis ë“±ë¡
            if is_first_load:
                unique_id = uuid.uuid4().hex[:8]
                self.dataset_id = f"ds_local_{base_dataset_id}_{unique_id}"
                if self.redis_manager:
                    try:
                        dataset_metadata = {
                            "source_type": "local",
                            "filenames": filenames,
                            "created_at": datetime.now().isoformat(),
                            "created_by": self.user_id,
                            "original_rows": combined_table.num_rows,
                            "original_columns": combined_table.num_columns
                        }
                        self.redis_manager.register_dataset(self.dataset_id, self.user_id, dataset_metadata)
                        self.redis_manager.link_manager_to_dataset(self.manager_id, self.dataset_id, self.user_id)
                        logger.info("âœ… Redis ë°ì´í„°ì…‹ ë“±ë¡ ë° Manager-Dataset ë§í¬ ì™„ë£Œ")
                    except Exception as e:
                        logger.warning(f"Redis ë“±ë¡ ì‹¤íŒ¨: {e}")

                # --- ì¶”ê°€: ì›ë³¸(raw-datasets) ì €ì¥ ---
                if self.minio_storage:
                    try:
                        metadata_for_minio = dataset_metadata if 'dataset_metadata' in locals() else {}
                        self.minio_storage.save_original_dataset(self.user_id, self.dataset_id, combined_table, metadata_for_minio)
                        logger.info(f"âœ… MinIO ì›ë³¸ ì €ì¥ ì™„ë£Œ: raw-datasets/{self.user_id}/{self.dataset_id}/original.parquet")
                    except Exception as e:
                        logger.warning(f"MinIO ì›ë³¸ ì €ì¥ ì‹¤íŒ¨(ê³„ì†): {e}")

            else:
                logger.info(f"â™»ï¸  ê¸°ì¡´ Dataset ì¬ë¡œë“œ: {self.dataset_id} (ë¡œë“œ {self.dataset_load_count}íšŒì°¨)")

            # ë°ì´í„°ì…‹ ì„¤ì •
            self.dataset = combined_table

            # source_info
            source_info = {
                "type": "local",
                "filenames": filenames,
                "loaded_at": datetime.now().isoformat(),
                "checksum": self._calculate_checksum(self.dataset),
                "num_rows": self.dataset.num_rows,
                "num_columns": self.dataset.num_columns,
                "columns": self.dataset.column_names,
                "load_count": self.dataset_load_count,
                "is_reload": not is_first_load
            }

            # Redisì— ì†ŒìŠ¤ ì •ë³´ ì €ì¥
            if self.redis_manager:
                try:
                    self.redis_manager.save_source_info(self.dataset_id, source_info)
                    logger.info("âœ… Redis ì†ŒìŠ¤ ì •ë³´ ì €ì¥")
                except Exception as e:
                    logger.warning("Redis ì†ŒìŠ¤ ì •ë³´ ì €ì¥ ì‹¤íŒ¨: %s", e)

            # ë²„ì „ ì €ì¥
            operation_name = "initial_load" if is_first_load else f"reload_{self.dataset_load_count}"
            self._save_version(operation_name, source_info)

            result_info = {
                "success": True,
                "dataset_id": self.dataset_id,
                "manager_id": self.manager_id,
                "base_dataset_id": base_dataset_id,
                "num_files": len(filenames),
                "filenames": filenames,
                "num_rows": combined_table.num_rows,
                "num_columns": combined_table.num_columns,
                "columns": combined_table.column_names,
                "loaded_at": datetime.now().isoformat(),
                "load_count": self.dataset_load_count,
                "is_new_version": not is_first_load,
                "current_version": self.current_version - 1
            }

            logger.info(f"âœ… ë¡œì»¬ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {self.dataset_id}, version={self.current_version - 1}, load_count={self.dataset_load_count}")
            return result_info

        except Exception as e:
            logger.error(f"ë¡œì»¬ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}", exc_info=True)
            raise RuntimeError(f"ë¡œì»¬ ë°ì´í„°ì…‹ ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

    # ========== ë°ì´í„°ì…‹ ë‚´ë³´ë‚´ê¸° ==========

    def download_dataset_as_csv(self, output_path: str = None) -> str:
        """í˜„ì¬ ë¡œë“œëœ ë°ì´í„°ì…‹ì„ CSV íŒŒì¼ë¡œ ì €ì¥"""
        if not self.is_active:
            raise RuntimeError("DataManager is not active")

        if self.dataset is None:
            raise RuntimeError("No dataset loaded")

        try:
            if output_path is None:
                download_dir = os.path.join(downloads_path, "tmp", "dataset_downloads")
                os.makedirs(download_dir, exist_ok=True)
                output_path = os.path.join(download_dir, f"dataset_{self.manager_id}.csv")

            csv.write_csv(self.dataset, output_path)

            logger.info("Dataset exported to CSV: %s (rows: %d, columns: %d)",
                       output_path, self.dataset.num_rows, self.dataset.num_columns)

            return output_path

        except Exception as e:
            logger.error("Failed to export dataset to CSV: %s", e)
            raise RuntimeError(f"CSV export failed: {str(e)}")

    def download_dataset_as_parquet(self, output_path: str = None) -> str:
        """í˜„ì¬ ë¡œë“œëœ ë°ì´í„°ì…‹ì„ Parquet íŒŒì¼ë¡œ ì €ì¥"""
        if not self.is_active:
            raise RuntimeError("DataManager is not active")

        if self.dataset is None:
            raise RuntimeError("No dataset loaded")

        try:
            if output_path is None:
                download_dir = os.path.join(downloads_path, "tmp", "dataset_downloads")
                os.makedirs(download_dir, exist_ok=True)
                output_path = os.path.join(download_dir, f"dataset_{self.manager_id}.parquet")

            pq.write_table(self.dataset, output_path)

            logger.info("Dataset exported to Parquet: %s (rows: %d, columns: %d)",
                       output_path, self.dataset.num_rows, self.dataset.num_columns)

            return output_path

        except Exception as e:
            logger.error("Failed to export dataset to Parquet: %s", e)
            raise RuntimeError(f"Parquet export failed: {str(e)}")

    # ========== ë°ì´í„°ì…‹ í†µê³„ ==========

    def get_dataset_statistics(self) -> Dict[str, Any]:
        """í˜„ì¬ ë¡œë“œëœ ë°ì´í„°ì…‹ì˜ ê¸°ìˆ í†µê³„ì •ë³´ ë°˜í™˜"""
        if not self.is_active:
            raise RuntimeError("DataManager is not active")

        if self.dataset is None:
            raise RuntimeError("No dataset loaded")

        try:
            statistics = generate_dataset_statistics(self.dataset)
            logger.info("Dataset statistics generated for manager %s", self.manager_id)
            return statistics

        except Exception as e:
            logger.error("Failed to generate dataset statistics: %s", e)
            raise RuntimeError(f"Statistics generation failed: {str(e)}")

    # ========== ë°ì´í„°ì…‹ ë³€í™˜ ë©”ì„œë“œ (ë²„ì „ ì €ì¥ í¬í•¨) ==========

    def drop_dataset_columns(self, columns_to_drop: List[str]) -> Dict[str, Any]:
        """ì»¬ëŸ¼ ì‚­ì œ (ë²„ì „ ì €ì¥ í¬í•¨)"""
        if not self.is_active:
            raise RuntimeError("DataManager is not active")

        if self.dataset is None:
            raise RuntimeError("No dataset loaded")

        initial_memory = self._calculate_dataset_memory_size(self.dataset)

        try:
            old_table = self.dataset
            new_table, result_info = drop_columns_from_table(self.dataset, columns_to_drop)
            self.dataset = new_table

            del old_table
            gc.collect()

            final_memory = self._calculate_dataset_memory_size(self.dataset)
            memory_reduced = initial_memory - final_memory

            result_info["memory_info"] = {
                "initial_memory_mb": initial_memory / (1024 * 1024),
                "final_memory_mb": final_memory / (1024 * 1024),
                "memory_reduced_mb": memory_reduced / (1024 * 1024)
            }

            # ë²„ì „ ì €ì¥
            self._save_version("drop_columns", {
                "dropped_columns": columns_to_drop
            })

            logger.info("Dataset columns dropped for manager %s: %s (ë©”ëª¨ë¦¬ ì ˆì•½: %.2f MB)",
                       self.manager_id, columns_to_drop, memory_reduced / (1024 * 1024))

            return result_info

        except Exception as e:
            gc.collect()
            logger.error("Failed to drop dataset columns: %s", e)
            raise RuntimeError(f"Column drop failed: {str(e)}")

    def replace_dataset_column_values(self, column_name: str, old_value: str, 
                                     new_value: str) -> Dict[str, Any]:
        """ê°’ êµì²´ (ë²„ì „ ì €ì¥ í¬í•¨)"""
        if not self.is_active:
            raise RuntimeError("DataManager is not active")

        if self.dataset is None:
            raise RuntimeError("No dataset loaded")

        try:
            new_table, result_info = replace_column_values(self.dataset, column_name, old_value, new_value)
            self.dataset = new_table

            # ë²„ì „ ì €ì¥
            self._save_version("replace_values", {
                "column": column_name,
                "old_value": old_value,
                "new_value": new_value
            })

            logger.info("Column values replaced for manager %s: %s", self.manager_id, result_info)

            return result_info

        except Exception as e:
            logger.error("Failed to replace column values: %s", e)
            raise RuntimeError(f"Value replacement failed: {str(e)}")

    def apply_dataset_column_operation(self, column_name: str, operation: str) -> Dict[str, Any]:
        """ì»¬ëŸ¼ ì—°ì‚° ì ìš© (ë²„ì „ ì €ì¥ í¬í•¨)"""
        if not self.is_active:
            raise RuntimeError("DataManager is not active")

        if self.dataset is None:
            raise RuntimeError("No dataset loaded")

        try:
            old_table = self.dataset
            new_table, result_info = apply_column_operation(self.dataset, column_name, operation)
            self.dataset = new_table

            del old_table
            gc.collect()

            # ë²„ì „ ì €ì¥
            self._save_version("apply_operation", {
                "column": column_name,
                "operation": operation
            })

            return result_info

        except Exception as e:
            logger.error("ì»¬ëŸ¼ ì—°ì‚° ì ìš© ì‹¤íŒ¨: %s", e)
            raise RuntimeError(f"ì»¬ëŸ¼ ì—°ì‚° ì ìš© ì‹¤íŒ¨: {str(e)}")

    def remove_null_rows_from_dataset(self, column_name: str = None) -> Dict[str, Any]:
        """NULL row ì œê±° (ë²„ì „ ì €ì¥ í¬í•¨)"""
        if not self.is_active:
            raise RuntimeError("DataManager is not active")

        if self.dataset is None:
            raise RuntimeError("No dataset loaded")

        initial_memory = self._calculate_dataset_memory_size(self.dataset)

        try:
            old_table = self.dataset
            new_table, result_info = remove_null_rows(self.dataset, column_name)
            self.dataset = new_table

            del old_table
            gc.collect()

            final_memory = self._calculate_dataset_memory_size(self.dataset)
            memory_saved = initial_memory - final_memory

            result_info["memory_info"] = {
                "initial_memory_mb": initial_memory / (1024 * 1024),
                "final_memory_mb": final_memory / (1024 * 1024),
                "memory_saved_mb": memory_saved / (1024 * 1024)
            }

            # ë²„ì „ ì €ì¥
            self._save_version("remove_null_rows", {
                "column": column_name,
                "rows_removed": result_info["removed_rows"]
            })

            logger.info("NULL row ì œê±° ì™„ë£Œ: ë§¤ë‹ˆì € %sì—ì„œ %dê°œ í–‰ ì œê±°",
                       self.manager_id, result_info["removed_rows"])

            return result_info

        except Exception as e:
            logger.error("NULL row ì œê±° ì‹¤íŒ¨: %s", e)
            raise RuntimeError(f"NULL row ì œê±° ì‹¤íŒ¨: {str(e)}")

    def upload_dataset_to_hf_repo(self, repo_id: str, hf_user_id: str, hub_token: str,
                                 filename: str = None, private: bool = False) -> Dict[str, Any]:
        """HuggingFace Hub ì—…ë¡œë“œ"""
        if not self.is_active:
            raise RuntimeError("DataManager is not active")

        if self.dataset is None:
            raise RuntimeError("No dataset loaded")

        try:
            result_info = upload_dataset_to_hf(
                self.dataset, repo_id, hf_user_id, hub_token, filename, private
            )

            logger.info("HuggingFace ì—…ë¡œë“œ ì™„ë£Œ: ë§¤ë‹ˆì € %s â†’ %s",
                       self.manager_id, result_info["repo_id"])

            return result_info

        except Exception as e:
            logger.error("HuggingFace ì—…ë¡œë“œ ì‹¤íŒ¨: %s", e)
            raise RuntimeError(f"HuggingFace ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

    def copy_dataset_column(self, source_column: str, new_column: str) -> Dict[str, Any]:
        """ì»¬ëŸ¼ ë³µì‚¬ (ë²„ì „ ì €ì¥ í¬í•¨)"""
        if not self.is_active:
            raise RuntimeError("DataManager is not active")

        if self.dataset is None:
            raise RuntimeError("No dataset loaded")

        try:
            old_table = self.dataset
            new_table, result_info = copy_column(self.dataset, source_column, new_column)
            self.dataset = new_table

            del old_table
            gc.collect()

            # ë²„ì „ ì €ì¥
            self._save_version("copy_column", {
                "source_column": source_column,
                "new_column": new_column
            })

            logger.info("ì»¬ëŸ¼ ë³µì‚¬ ì™„ë£Œ: ë§¤ë‹ˆì € %sì—ì„œ '%s' â†’ '%s'",
                       self.manager_id, source_column, new_column)

            return result_info

        except Exception as e:
            logger.error("ì»¬ëŸ¼ ë³µì‚¬ ì‹¤íŒ¨: %s", e)
            raise RuntimeError(f"ì»¬ëŸ¼ ë³µì‚¬ ì‹¤íŒ¨: {str(e)}")

    def rename_dataset_column(self, old_name: str, new_name: str) -> Dict[str, Any]:
        """ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½ (ë²„ì „ ì €ì¥ í¬í•¨)"""
        if not self.is_active:
            raise RuntimeError("DataManager is not active")

        if self.dataset is None:
            raise RuntimeError("No dataset loaded")

        try:
            old_table = self.dataset
            new_table, result_info = rename_column(self.dataset, old_name, new_name)
            self.dataset = new_table

            del old_table
            gc.collect()

            # ë²„ì „ ì €ì¥
            self._save_version("rename_column", {
                "old_name": old_name,
                "new_name": new_name
            })

            logger.info("ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½ ì™„ë£Œ: ë§¤ë‹ˆì € %sì—ì„œ '%s' â†’ '%s'",
                       self.manager_id, old_name, new_name)

            return result_info

        except Exception as e:
            logger.error("ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½ ì‹¤íŒ¨: %s", e)
            raise RuntimeError(f"ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½ ì‹¤íŒ¨: {str(e)}")

    def format_columns_to_string(self, column_names: List[str], template: str, 
                                new_column: str) -> Dict[str, Any]:
        """ì»¬ëŸ¼ ë¬¸ìì—´ í¬ë§·íŒ… (ë²„ì „ ì €ì¥ í¬í•¨)"""
        if not self.is_active:
            raise RuntimeError("DataManager is not active")

        if self.dataset is None:
            raise RuntimeError("No dataset loaded")

        try:
            old_table = self.dataset
            new_table, result_info = format_columns_string(
                self.dataset, column_names, template, new_column
            )
            self.dataset = new_table

            del old_table
            gc.collect()

            # ë²„ì „ ì €ì¥
            self._save_version("format_columns", {
                "column_names": column_names,
                "template": template,
                "new_column": new_column
            })

            logger.info("ì»¬ëŸ¼ ë¬¸ìì—´ í¬ë§·íŒ… ì™„ë£Œ: ë§¤ë‹ˆì € %sì—ì„œ %s â†’ '%s'",
                       self.manager_id, column_names, new_column)

            return result_info

        except Exception as e:
            logger.error("ì»¬ëŸ¼ ë¬¸ìì—´ í¬ë§·íŒ… ì‹¤íŒ¨: %s", e)
            raise RuntimeError(f"ì»¬ëŸ¼ ë¬¸ìì—´ í¬ë§·íŒ… ì‹¤íŒ¨: {str(e)}")

    def calculate_columns_to_new(self, col1: str, col2: str, operation: str, 
                                 new_column: str) -> Dict[str, Any]:
        """ì»¬ëŸ¼ ê°„ ì—°ì‚° (ë²„ì „ ì €ì¥ í¬í•¨)"""
        if not self.is_active:
            raise RuntimeError("DataManager is not active")

        if self.dataset is None:
            raise RuntimeError("No dataset loaded")

        try:
            old_table = self.dataset
            new_table, result_info = calculate_columns_operation(
                self.dataset, col1, col2, operation, new_column
            )
            self.dataset = new_table

            del old_table
            gc.collect()

            # ë²„ì „ ì €ì¥
            self._save_version("calculate_columns", {
                "col1": col1,
                "col2": col2,
                "operation": operation,
                "new_column": new_column
            })

            logger.info("ì»¬ëŸ¼ ì—°ì‚° ì™„ë£Œ: ë§¤ë‹ˆì € %sì—ì„œ %s %s %s â†’ '%s'",
                       self.manager_id, col1, operation, col2, new_column)

            return result_info

        except Exception as e:
            logger.error("ì»¬ëŸ¼ ì—°ì‚° ì‹¤íŒ¨: %s", e)
            raise RuntimeError(f"ì»¬ëŸ¼ ì—°ì‚° ì‹¤íŒ¨: {str(e)}")

    def execute_dataset_callback(self, callback_code: str) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì½œë°± ì‹¤í–‰ (ë²„ì „ ì €ì¥ í¬í•¨)"""
        if not self.is_active:
            raise RuntimeError("DataManager is not active")

        if self.dataset is None:
            raise RuntimeError("No dataset loaded")

        try:
            old_table = self.dataset
            new_table, result_info = execute_safe_callback(self.dataset, callback_code)
            self.dataset = new_table

            del old_table
            gc.collect()

            # ë²„ì „ ì €ì¥
            self._save_version("execute_callback", {
                "code_length": len(callback_code),
                "rows_changed": result_info["rows_changed"],
                "columns_changed": result_info["columns_changed"]
            })

            logger.info("ì‚¬ìš©ì ì½œë°± ì‹¤í–‰ ì™„ë£Œ: ë§¤ë‹ˆì € %s, %dí–‰ â†’ %dí–‰, %dì—´ â†’ %dì—´",
                       self.manager_id, result_info["original_rows"],
                       result_info["final_rows"], result_info["original_columns"],
                       result_info["final_columns"])

            return result_info

        except Exception as e:
            logger.error("ì‚¬ìš©ì ì½œë°± ì‹¤í–‰ ì‹¤íŒ¨: %s", e)
            raise RuntimeError(f"ì‚¬ìš©ì ì½œë°± ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")


    # /service/data_manager/data_manager.pyì— ì¶”ê°€

    # ========== ë°ì´í„°ì…‹ ë¡œë“œ ë©”ì„œë“œ ========== ì„¹ì…˜ì— ì¶”ê°€

    def db_load_dataset(self, 
                       db_config: Dict[str, Any],
                       query: str = None,
                       table_name: str = None,
                       chunk_size: int = None) -> Dict[str, Any]:
        """
        ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œ
        
        Args:
            db_config: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •
                {
                    'db_type': 'postgresql' | 'mysql' | 'sqlite',
                    'host': str,
                    'port': int,
                    'database': str,
                    'username': str,
                    'password': str
                }
            query: SQL ì¿¼ë¦¬ (query ë˜ëŠ” table_name ì¤‘ í•˜ë‚˜ í•„ìˆ˜)
            table_name: í…Œì´ë¸”ëª… (query ë˜ëŠ” table_name ì¤‘ í•˜ë‚˜ í•„ìˆ˜)
            chunk_size: ì²­í¬ í¬ê¸° (ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ìš©)
            
        Returns:
            Dict[str, Any]: ë¡œë“œ ê²°ê³¼ ì •ë³´
        """
        if not self.is_active:
            raise RuntimeError("DataManager is not active")

        if not query and not table_name:
            raise RuntimeError("query ë˜ëŠ” table_name ì¤‘ í•˜ë‚˜ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤")

        try:
            import sqlalchemy
            from sqlalchemy import create_engine, text
            import pandas as pd
            
            logger.info("DB ë°ì´í„°ì…‹ ë¡œë“œ ì‹œì‘: db_type=%s, user=%s", 
                       db_config.get('db_type'), self.user_id)
            
            # ========== 1. DB ì—°ê²° ë¬¸ìì—´ ìƒì„± ==========
            db_type = db_config.get('db_type', 'postgresql').lower()
            
            if db_type == 'postgresql':
                connection_string = (
                    f"postgresql://{db_config['username']}:{db_config['password']}"
                    f"@{db_config['host']}:{db_config.get('port', 5432)}"
                    f"/{db_config['database']}"
                )
            elif db_type == 'mysql':
                connection_string = (
                    f"mysql+pymysql://{db_config['username']}:{db_config['password']}"
                    f"@{db_config['host']}:{db_config.get('port', 3306)}"
                    f"/{db_config['database']}"
                )
            elif db_type == 'sqlite':
                connection_string = f"sqlite:///{db_config['database']}"
            else:
                raise RuntimeError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” DB íƒ€ì…: {db_type}")
            
            # ========== 2. DB ì—°ê²° ë° ë°ì´í„° ë¡œë“œ ==========
            engine = create_engine(connection_string)
            
            # SQL ì¿¼ë¦¬ ê²°ì •
            if query:
                sql_query = query
                logger.info(f"  â””â”€ ì‚¬ìš©ì ì •ì˜ ì¿¼ë¦¬ ì‹¤í–‰")
            else:
                sql_query = f"SELECT * FROM {table_name}"
                logger.info(f"  â””â”€ í…Œì´ë¸” ì „ì²´ ì¡°íšŒ: {table_name}")
            
            # ë°ì´í„° ë¡œë“œ
            if chunk_size:
                # ì²­í¬ ë‹¨ìœ„ë¡œ ë¡œë“œ (ëŒ€ìš©ëŸ‰ ë°ì´í„°)
                logger.info(f"  â””â”€ ì²­í¬ í¬ê¸°: {chunk_size}")
                chunks = []
                for chunk_df in pd.read_sql(sql_query, engine, chunksize=chunk_size):
                    chunks.append(pa.Table.from_pandas(chunk_df))
                combined_table = pa.concat_tables(chunks)
                logger.info(f"  â””â”€ {len(chunks)}ê°œ ì²­í¬ ë³‘í•© ì™„ë£Œ")
            else:
                # ì „ì²´ ë¡œë“œ
                df = pd.read_sql(sql_query, engine)
                combined_table = pa.Table.from_pandas(df)
            
            engine.dispose()
            
            logger.info("í…Œì´ë¸” ë¡œë“œ ì™„ë£Œ: %dí–‰, %dì—´", 
                       combined_table.num_rows, combined_table.num_columns)
            
            # ========== 3. ë°ì´í„°ì…‹ ì„¤ì • ==========
            self.dataset = combined_table
            
            # load_count ì¦ê°€
            self.dataset_load_count += 1
            is_first_load = (self.dataset_id is None)
            
            # ========== 4. Dataset ID ìƒì„± ë° Redis ë“±ë¡ ==========
            if is_first_load:
                db_identifier = f"{db_type}_{db_config['database']}"
                if table_name:
                    db_identifier += f"_{table_name}"
                unique_id = uuid.uuid4().hex[:8]
                self.dataset_id = f"ds_db_{db_identifier}_{unique_id}"
                logger.info(f"âœ¨ ìƒˆ Dataset ID ìƒì„±: {self.dataset_id}")
                
                if self.redis_manager:
                    try:
                        dataset_metadata = {
                            "source_type": "database",
                            "db_type": db_type,
                            "database": db_config['database'],
                            "table_name": table_name,
                            "query": query if query else f"SELECT * FROM {table_name}",
                            "created_at": datetime.now().isoformat(),
                            "created_by": self.user_id,
                            "original_rows": combined_table.num_rows,
                            "original_columns": combined_table.num_columns
                        }
                        self.redis_manager.register_dataset(
                            self.dataset_id, self.user_id, dataset_metadata
                        )
                        self.redis_manager.link_manager_to_dataset(
                            self.manager_id, self.dataset_id, self.user_id
                        )
                        logger.info("âœ… Redis ë°ì´í„°ì…‹ ë“±ë¡ ë° Manager-Dataset ë§í¬ ì™„ë£Œ")
                    except Exception as e:
                        logger.warning(f"Redis ë“±ë¡ ì‹¤íŒ¨: {e}")
                
                # MinIO ì›ë³¸ ì €ì¥
                if self.minio_storage:
                    try:
                        metadata_for_minio = dataset_metadata if 'dataset_metadata' in locals() else {}
                        self.minio_storage.save_original_dataset(
                            self.user_id, self.dataset_id, self.dataset, metadata_for_minio
                        )
                        logger.info(f"âœ… MinIO ì›ë³¸ ì €ì¥ ì™„ë£Œ: raw-datasets/{self.user_id}/{self.dataset_id}/original.parquet")
                    except Exception as e:
                        logger.warning(f"MinIO ì›ë³¸ ì €ì¥ ì‹¤íŒ¨(ê³„ì†): {e}")
            else:
                logger.info(f"â™»ï¸ ê¸°ì¡´ Dataset ì¬ë¡œë“œ: {self.dataset_id} (ë¡œë“œ {self.dataset_load_count}íšŒì°¨)")
            
            # ========== 5. ì†ŒìŠ¤ ì •ë³´ ìƒì„± ==========
            source_info = {
                "type": "database",
                "db_type": db_type,
                "database": db_config['database'],
                "table_name": table_name,
                "query": query if query else f"SELECT * FROM {table_name}",
                "loaded_at": datetime.now().isoformat(),
                "checksum": self._calculate_checksum(self.dataset),
                "num_rows": combined_table.num_rows,
                "num_columns": combined_table.num_columns,
                "columns": combined_table.column_names,
                "load_count": self.dataset_load_count,
                "is_reload": not is_first_load
            }
            
            # Redisì— ì†ŒìŠ¤ ì •ë³´ ì €ì¥
            if self.redis_manager:
                try:
                    self.redis_manager.save_source_info(self.dataset_id, source_info)
                    logger.info("âœ… Redis ì†ŒìŠ¤ ì •ë³´ ì €ì¥")
                except Exception as e:
                    logger.warning(f"Redis ì†ŒìŠ¤ ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {e}")
            
            # ========== 6. ë²„ì „ ì €ì¥ ==========
            operation_name = "initial_load" if is_first_load else f"reload_{self.dataset_load_count}"
            logger.info(f"ğŸ’¾ ë²„ì „ ì €ì¥: operation={operation_name}, load_count={self.dataset_load_count}")
            
            self._save_version(operation_name, source_info)
            
            # ========== 7. ê²°ê³¼ ë°˜í™˜ ==========
            result_info = {
                "success": True,
                "dataset_id": self.dataset_id,
                "manager_id": self.manager_id,
                "user_id": self.user_id,
                "db_type": db_type,
                "database": db_config['database'],
                "table_name": table_name,
                "query": query,
                "num_rows": combined_table.num_rows,
                "num_columns": combined_table.num_columns,
                "columns": combined_table.column_names,
                "loaded_at": datetime.now().isoformat(),
                "is_new_dataset": is_first_load,
                "current_version": self.current_version - 1,
                "load_count": self.dataset_load_count,
                "is_new_version": not is_first_load,
                "source_info": source_info
            }
            
            logger.info(f"âœ… DB ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: dataset={self.dataset_id}, version={self.current_version - 1}")
            return result_info
            
        except ImportError as e:
            logger.error(f"í•„ìˆ˜ íŒ¨í‚¤ì§€ ë¯¸ì„¤ì¹˜: {e}")
            raise RuntimeError(f"DB ì—°ê²°ì— í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {str(e)}")
        except Exception as e:
            logger.error(f"DB ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}", exc_info=True)
            raise RuntimeError(f"DB ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

    # ========== ì •ë¦¬ ë° ì†Œë©¸ì ==========

    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ë° ë§¤ë‹ˆì € ì¢…ë£Œ"""
        logger.info(f"Cleaning up DataManager {self.manager_id}")

        self.is_active = False
        self._monitoring = False

        # ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°
        if hasattr(self, '_monitor_thread') and self._monitor_thread.is_alive():
            self._monitor_thread.join(timeout=2)

        # ë°ì´í„° ì •ë¦¬
        self.dataset = None

        # Redis Manager ì„¸ì…˜ í•´ì œ
        if self.redis_manager:
            try:
                self.redis_manager.unlink_manager(self.manager_id, self.user_id)
                logger.info(f"Manager ì„¸ì…˜ í•´ì œ: {self.manager_id}")
            except Exception as e:
                logger.warning(f"Manager ì„¸ì…˜ í•´ì œ ì‹¤íŒ¨: {e}")

        # ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()

        logger.info(f"DataManager {self.manager_id} cleaned up successfully!")

    def __del__(self):
        """ì†Œë©¸ì - ìë™ ì •ë¦¬"""
        if hasattr(self, 'is_active') and self.is_active:
            try:
                self.cleanup()
            except Exception as e:
                logger.error(f"ì†Œë©¸ìì—ì„œ ì •ë¦¬ ì‹¤íŒ¨: {e}")